{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, time, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cPickle as pk\n",
    "np.random.seed(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../2-DRMM/') # use what is contained in the file `../2-DRMM/DRMM.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "PK_FPATH = '../data/processed_data_sidhid.pk'\n",
    "MODEL_FPATH = '../models/1124_model_2embed_2conv1d_2FC.h5' # path of best trained model \n",
    "NOTES_DIR = '/local/XW/DATA/MIMIC/noteevents_by_sid_hid/'\n",
    "TOKENIZER_FPATH = '../data/tokenizer.pk'\n",
    "# constants\n",
    "MAX_NB_WORDS = 20000 # top 20k most freq words\n",
    "MAX_SEQ_LEN = 1000\n",
    "N_LABELS = 50\n",
    "N_SIDHID = 58328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file contains the prepared data for note2vec training, \n",
      "* sidhids:     list of the 58361 unique (sid,hid) pairs\n",
      "* sidhid2icds: mapping from (sid,hid) pair --> set of icd codes\n",
      "* sidhid2khot: mapping from (sid,hid) pair --> khot-encoding correponding to this sidhid pair\n",
      "* sidhid2seq:  mapping from (sid,hid) pair --> fix-length sequences (len=1000) of word ids\n",
      "* word2idx:    mapping from a word to its id used in the sequence\n",
      "* embedding_w2v／embedding_glove: matrices for the embedding layer (used as the weights parameter)\n",
      "* train_sidhids/val_sidhids: list of (sid,hid) pairs used as training/validation set\n",
      "* X_train/Y_train/X_val/Y_val: ndarray generated for training/validation\n",
      "\n",
      "And here are 2 useful functions' source code: \n",
      "\n",
      "def to_khot(sidhid2icds, K=N_LABELS): # generate khot encoding (useful if want to change the K)\n",
      "    icds = zip( *icd_ctr.most_common(N_LABELS-1) )[0] + ('other',)\n",
      "    sidhid2khot = {} # map subject_id to k-hot vector\n",
      "    for sid,hid in sidhid2icds.keys():\n",
      "        _khot = np.zeros(N_LABELS)\n",
      "        for _icd in sidhid2icds[(sid,hid)]:\n",
      "            if _icd in icds: \n",
      "                _khot[icds.index(_icd)] = 1\n",
      "            else: # label 'other icds'\n",
      "                _khot[-1] = 1\n",
      "        sidhid2khot[(sid,hid)] = _khot\n",
      "    return sidhid2khot\n",
      "\n",
      "def getXY(sidhid_lst): # give a list of (sid, hid) pairs, generate the X and Y\n",
      "    data, labels = [], []\n",
      "    for sidhid in sidhid_lst:\n",
      "        data.append(sidhid2seq[sidhid])\n",
      "        labels.append(sidhid2khot[sidhid])\n",
      "    X = np.array(data)\n",
      "    Y = np.array(labels)\n",
      "    return X,Y\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load pickled data\n",
    "pk_data = pk.load(open(PK_FPATH, 'rb'))\n",
    "X_train = pk_data['X_train']\n",
    "print pk_data['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load note2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     4,   415,  3867],\n",
       "       [ 9846,    48,  3477, ...,  1159,   269,  7250],\n",
       "       [ 4008,    11,  5635, ...,   121,    69,  6624],\n",
       "       ..., \n",
       "       [10310,   421,  1747, ...,   680,    57,   475],\n",
       "       [  785,   486,  1517, ...,  3268,   470,   682],\n",
       "       [ 4917,    20,  2463, ...,  1062,    97,   116]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print X_train[:10].shape\n",
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ***NOTE***\n",
    "# To load models from file, we have to modify metrics.py at: \n",
    "# `/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras/` \n",
    "# to add the custom metric function, otherwise `load_model` throws exception ! \n",
    "# cf issue: https://github.com/fchollet/keras/issues/3911\n",
    "from keras.models import load_model\n",
    "model = load_model(MODEL_FPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"main_input:0\", shape=(?, 1000), dtype=int32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 500), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print model.layers[0].input\n",
    "print model.layers[11].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use K.function to construct a model that outputs embedding vector\n",
    "from keras import backend as K\n",
    "get_embedvec = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                  [model.layers[11].output])\n",
    "embedvec = lambda X: get_embedvec([X,0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 500)\n"
     ]
    }
   ],
   "source": [
    "# output in test mode = 0\n",
    "layer_output = embedvec(X_train[:10])\n",
    "print layer_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn a paragraph into 500-dimensional input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sidhids = []\n",
    "# texts = [] # text bodies\n",
    "# for fname in tqdm(os.listdir(NOTES_DIR)): # the data is 3.7G in size, can hold in memory...\n",
    "#     sid,hid = map( int, fname[:-4].split('_') )\n",
    "#     sidhids.append( (sid,hid) )\n",
    "#     fpath = os.path.join(NOTES_DIR, fname)\n",
    "#     df = pd.read_csv(fpath)\n",
    "#     texts.append( '\\n=======\\n\\n\\n'.join(df['text']) )\n",
    "# print('found %d texts' % len(texts))\n",
    "\n",
    "# tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, # filter out numbers, otherwise lots of numbers\n",
    "#                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+'0123456789') \n",
    "# print 'fitting on whole text corpus...',\n",
    "# tokenizer.fit_on_texts(texts) # this might take some time\n",
    "# print 'done. '\n",
    "\n",
    "# pk.dump(tokenizer, open('data/tokenizer.pk', 'wb'), pk.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paragraph2vec(paragraph):\n",
    "    seqs = tokenizer.texts_to_sequences([paragraph.encode('utf-8')])\n",
    "    seqs_padded = pad_sequences(seqs, maxlen=MAX_SEQ_LEN)\n",
    "    return embedvec(seqs_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 500)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_sample = ''' The imaged portions of the abdomen show a few [**Last Name (un) 36399**]-filled loops of bowel\n",
    "   within the left abdomen.  No abnormal soft tissue mass or calcifications.  No\n",
    "   free interperitoneal air.  The imaged bony structures are unremarkable.'''\n",
    "paragraph2vec(paragraph_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "topic_tree = etree.parse('../data/topics2016.xml')\n",
    "pat = re.compile('\\W*\\n\\W*\\n')\n",
    "def get_query_paragraphs(i): # returns the paragraphs in topic i \n",
    "    text = '\\n=====\\n'.join( topic_tree.xpath('//topic[@number=\"%d\"]/*/text()'%i) )\n",
    "    paras = pat.split(text.lower())\n",
    "    return [p.strip() for p in paras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 12, 3, 4, 4, 4, 7, 6, 11, 5, 4, 3, 7, 4, 3, 4, 5, 9, 7, 5, 3, 6, 8, 4, 4, 5, 8, 6, 6, 5]\n"
     ]
    }
   ],
   "source": [
    "QUERIES = [get_query_paragraphs(i) for i in xrange(1,31)]\n",
    "print map(len, QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "MAX_QLEN = max(map(len, QUERIES))\n",
    "print MAX_QLEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# padding queries to the same length N\n",
    "PARA_PLACEHOLDER = '</s>'\n",
    "def pad_query(q, SZ=N):\n",
    "    return q + [PARA_PLACEHOLDER]*(SZ-len(q))\n",
    "QUERIES = map(pad_query, QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "QUERIES = {i+1:QUERIES[i] for i in xrange(30)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to extract histvec from query/article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PMC_PATH = '/local/XW/DATA/TREC/PMCs/'\n",
    "pmcid2fpath = {}\n",
    "\n",
    "for subdir1 in os.listdir(PMC_PATH):\n",
    "    for subdir2 in os.listdir(os.path.join(PMC_PATH, subdir1)):\n",
    "        diry = os.path.join(PMC_PATH, subdir1, subdir2)\n",
    "        for fn in os.listdir(diry):\n",
    "            pmcid = fn[:-5]\n",
    "            fpath = os.path.join(diry, fn)\n",
    "            pmcid2fpath[pmcid] = fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "78 m w/ pmh of cabg in early [**month (only) 3**] at [**hospital6 4406**]\n",
      "   (transferred to nursing home for rehab on [**12-8**] after several falls out\n",
      "   of bed.) he was then readmitted to [**hospital6 1749**] on\n",
      "   [**3120-12-11**] after developing acute pulmonary edema/chf/unresponsiveness?.\n",
      "   there was a question whether he had a small mi; he reportedly had a\n",
      "   small nqwmi. he improved with diuresis and was not intubated\n",
      "---\n",
      "yesterday, he was noted to have a melanotic stool earlier this evening\n",
      "   and then approximately 9 loose bm w/ some melena and some frank blood\n",
      "   just prior to transfer, unclear quantity\n",
      "---\n",
      "78 m transferred to nursing home for rehab after cabg. reportedly readmitted with a small nqwmi. yesterday, he was noted to have a melanotic stool and then today he had approximately 9 loose bm w/ some melena and some frank blood just prior to transfer, unclear quantity\n",
      "---\n",
      "a 78 year old male presents with frequent stools and melena.\n"
     ]
    }
   ],
   "source": [
    "for p in get_query_paragraphs(1):\n",
    "    print '---'\n",
    "    print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_paragraphs(pmcid):\n",
    "    'returns a list of texts, each as a paragraph'\n",
    "    fpath = pmcid2fpath[pmcid]\n",
    "    tree = etree.parse(fpath)\n",
    "    ret = []\n",
    "    body = tree.xpath('//body')[0]\n",
    "    for p in body.xpath('.//p'):\n",
    "        ret.append( p.xpath('string(.)').strip() )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_article_paragraphs('107838')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARA_PLACEHOLDER = '</s>'\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_histvec(query_para, pmcid):\n",
    "    if query_para == PARA_PLACEHOLDER: \n",
    "        return np.zeros(30)\n",
    "    qvec = paragraph2vec(query_para)\n",
    "    dvecs = np.vstack( [ paragraph2vec(p.encode('ascii','ignore')) for p in get_article_paragraphs(pmcid)] )\n",
    "    cossims = np.dot(dvecs, qvec.T) / norm(qvec) / norm(dvecs, axis=1)\n",
    "    hist, _ = np.histogram( cossims, bins=30, range=(0,1) )\n",
    "    ret = np.log(hist+1)\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  3.04452244,\n",
       "        4.4308168 ,  4.33073334,  3.52636052,  3.61091791,  4.21950771,\n",
       "        4.26267988,  4.76217393,  4.49980967,  5.01727984,  5.27811466])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_histvec(get_query_paragraphs(1)[1], '107838')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_para = get_query_paragraphs(1)[1] \n",
    "pmcid = '107838'\n",
    "qvec = paragraph2vec(query_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_query_doc_feature(qid, pmcid): # query: list of paragraphs\n",
    "    query = QUERIES[qid]\n",
    "    return np.array([ get_histvec(p, pmcid) for p in query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 30)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_query_doc_feature(1, '107838').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data: padded queries, positive and negative histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37707/37707 [01:36<00:00, 390.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "candidates = defaultdict(list) # dict[int, list<str>] mapping qid to list of its candidate docids (that appeared in the qrel)\n",
    "n_pos = defaultdict(int) # dict[int, int] mapping qid to the number of positive documents in qrel\n",
    "relevance = {} # dict[(int,str), int] mapping (qid,docid) pairs to its relevance (0,1,2)\n",
    "with open('../data/qrels.txt') as f:\n",
    "    for line in tqdm(f, total=37707): \n",
    "        qid, _, pmcid, rel = line.split()\n",
    "        qid = int(qid); rel = int(rel)\n",
    "        try: \n",
    "            if len( get_article_paragraphs(pmcid) ) <= 3: \n",
    "                continue\n",
    "            relevance[(qid,pmcid)] =rel\n",
    "            candidates[qid].append(pmcid)\n",
    "            if rel>0: n_pos[qid] += 1\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf(para):\n",
    "    return -10 if para==PARA_PLACEHOLDER else 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IDFs = {}\n",
    "for qid in QUERIES.keys(): \n",
    "    IDFs[qid] = np.array([idf(para) for para in QUERIES[qid]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(22, 8), (27, 12), (4, 18), (10, 19), (2, 34), (30, 39), (18, 67), (15, 69), (7, 71), (21, 73), (5, 95), (23, 106), (12, 108), (26, 112), (14, 117), (29, 117), (9, 121), (1, 128), (6, 141), (13, 148), (3, 150), (17, 175), (16, 182), (28, 211), (25, 216), (19, 218), (11, 364), (20, 631), (24, 757), (8, 831)]\n",
      "[8, 12, 18, 19, 34, 39, 67, 69, 71, 73, 95, 106, 108, 112, 117, 117, 121, 128, 141, 148, 150, 175, 182, 211, 216, 218, 364, 631, 757, 831]\n"
     ]
    }
   ],
   "source": [
    "print sorted(n_pos.items(), key=lambda (k,v): v)\n",
    "all_pos = sorted( n_pos.values() ) \n",
    "print all_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 112 364\n"
     ]
    }
   ],
   "source": [
    "avg_pos_80 = all_pos[len(all_pos) * 9 / 10 - 1] # x1.5\n",
    "avg_pos_50 = all_pos[len(all_pos) * 5 / 10 - 2] # x3\n",
    "avg_pos_10 = all_pos[len(all_pos) * 5 / 30] # x10\n",
    "print avg_pos_10, avg_pos_50, avg_pos_80 # quantiles of posid numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores = [2, 1, 0] total= 163263 got 3840 instances for query 1\n",
      "scores = [2, 1, 0] total= 42372 got 6800 instances for query 2\n",
      "scores = [2, 1, 0] total= 192356 got 4500 instances for query 3\n",
      "scores = [2, 1, 0] total= 24345 got 3600 instances for query 4\n",
      "scores = [2, 1, 0] total= 126691 got 5700 instances for query 5\n",
      "scores = [2, 1, 0] total= 97817 got 4230 instances for query 6\n",
      "scores = [2, 1, 0] total= 72479 got 4260 instances for query 7\n",
      "scores = [2, 1, 0] total= 435854 got 8000 instances for query 8\n",
      "scores = [2, 1, 0] total= 126334 got 3630 instances for query 9\n",
      "scores = [2, 0] total= 21318 got 3800 instances for query 10\n",
      "scores = [2, 1, 0] total= 255176 got 10920 instances for query 11\n",
      "scores = [2, 1, 0] total= 117927 got 6480 instances for query 12\n",
      "scores = [2, 1, 0] total= 189428 got 4440 instances for query 13\n",
      "scores = [2, 1, 0] total= 124466 got 3510 instances for query 14\n",
      "scores = [2, 1, 0] total= 68651 got 4140 instances for query 15\n",
      "scores = [2, 1, 0] total= 250340 got 5460 instances for query 16\n",
      "scores = [2, 1, 0] total= 161175 got 5250 instances for query 17\n",
      "scores = [2, 1, 0] total= 71502 got 4020 instances for query 18\n",
      "scores = [2, 1, 0] total= 219438 got 6540 instances for query 19\n",
      "scores = [2, 1, 0] total= 356988 got 8000 instances for query 20\n",
      "scores = [2, 1, 0] total= 64822 got 4380 instances for query 21\n",
      "scores = [2, 0] total= 9296 got 1600 instances for query 22\n",
      "scores = [2, 1, 0] total= 161688 got 6360 instances for query 23\n",
      "scores = [2, 1, 0] total= 615971 got 8000 instances for query 24\n",
      "scores = [2, 1, 0] total= 205871 got 6480 instances for query 25\n",
      "scores = [2, 1, 0] total= 103127 got 6720 instances for query 26\n",
      "scores = [2, 0] total= 11400 got 2400 instances for query 27\n",
      "scores = [2, 1, 0] total= 307502 got 6330 instances for query 28\n",
      "scores = [2, 1, 0] total= 76023 got 3510 instances for query 29\n",
      "scores = [2, 1, 0] total= 49700 got 7800 instances for query 30\n"
     ]
    }
   ],
   "source": [
    "instances = {} # mapping qid to list, instances[qid] = list (pos_docid, neg_docid) pairs for qid, \n",
    "# use pairs in instances for training\n",
    "np.random.seed(1)\n",
    "for qid in QUERIES.keys():\n",
    "    \n",
    "    pernegative = 20 # number of limited pairs per positive sample\n",
    "    num_of_instances = 8000 # number limit of pairs per query\n",
    "    \n",
    "    num_pos_currquery = n_pos[qid]\n",
    "    curr_pernegative = pernegative\n",
    "    curr_num_of_instance = num_of_instances # -- their trick: gen less pairs for queries with more pos docs\n",
    "    if(num_pos_currquery <= avg_pos_10): \n",
    "        curr_pernegative *= 10; curr_num_of_instance *= 10\n",
    "    elif(num_pos_currquery <= avg_pos_50): \n",
    "        curr_pernegative *= 3; curr_num_of_instance *= 3; \n",
    "    elif(num_pos_currquery <= avg_pos_80): \n",
    "        curr_pernegative *= 1.5; curr_num_of_instance *= 1.5; \n",
    "    \n",
    "    rel_scores = defaultdict(list) # mapping a rel score to list of docids\n",
    "    for docid in candidates[qid]:\n",
    "        rel = relevance[(qid,docid)]\n",
    "        rel_scores[rel].append(docid)\n",
    "    scores = sorted( rel_scores.keys(), reverse=True ) # scores are sorted in desc order\n",
    "    print 'scores =',scores, \n",
    "    total_instance = 0\n",
    "    for i in xrange(len(scores)): # scores[i] = pos score\n",
    "        for j in xrange(i+1, len(scores)): # scores[j] = neg score\n",
    "            total_instance += len(rel_scores[scores[i]]) * len(rel_scores[scores[j]])\n",
    "    print 'total=', total_instance, \n",
    "    total_instance = min(total_instance, curr_num_of_instance)\n",
    "    from numpy.random import choice \n",
    "    instances_for_q = []\n",
    "    for i in xrange(len(scores)):# scores are sorted in desc order\n",
    "        pos_score = scores[i]\n",
    "        cur_pos_ids = rel_scores[pos_score] # mapping a rel score to list of docids\n",
    "        cur_neg_ids = []\n",
    "        for j in xrange(i+1, len(scores)):\n",
    "            neg_score = scores[j]\n",
    "            cur_neg_ids += rel_scores[neg_score]# FOUND A BUG HERE\n",
    "        if len(cur_neg_ids)==0: break\n",
    "        for posid in cur_pos_ids:\n",
    "            for negid in choice(cur_neg_ids, min(len(cur_neg_ids),int(curr_pernegative)), replace=False):\n",
    "                instances_for_q.append( (posid,negid) )\n",
    "            if len(instances_for_q)>=total_instance: break\n",
    "        if len(instances_for_q)>=total_instance: break\n",
    "    print 'got %d instances for query %d' % (len(instances_for_q), qid)\n",
    "    instances[qid] = instances_for_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1380, 1272, 1404, 1368, 1414, 810, 1080, 1175, 1135, 1141, 986, 1177, 1397, 1167, 1062, 1512, 1054, 1125, 1185, 1039, 943, 1170, 1606, 1400, 1119, 1005, 962, 1623, 746, 1307]\n"
     ]
    }
   ],
   "source": [
    "print map(len, candidates.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 986/986 [36:11<00:00,  1.46s/it]\n",
      "100%|██████████| 1177/1177 [1:13:16<00:00,  1.49s/it]\n",
      "100%|██████████| 1397/1397 [2:57:28<00:00,  2.54s/it]\n",
      "100%|██████████| 1167/1167 [1:24:52<00:00,  1.72s/it]\n",
      "100%|██████████| 1062/1062 [1:03:48<00:00,  1.31s/it]\n",
      "100%|██████████| 1512/1512 [1:51:10<00:00,  2.32s/it]\n",
      "100%|██████████| 1054/1054 [1:55:07<00:00,  1.73s/it]\n",
      "100%|██████████| 1125/1125 [3:08:06<00:00,  3.75s/it]\n",
      "100%|██████████| 1185/1185 [2:49:27<00:00,  2.69s/it]\n",
      "100%|██████████| 1039/1039 [1:55:01<00:00,  1.68s/it]\n",
      "100%|██████████| 943/943 [1:01:51<00:00,  1.24it/s]\n",
      "100%|██████████| 1170/1170 [2:09:17<00:00,  2.26s/it]\n",
      "100%|██████████| 1606/1606 [3:27:05<00:00,  5.68s/it]\n",
      "100%|██████████| 1400/1400 [1:40:39<00:00,  2.07s/it]\n",
      "100%|██████████| 1119/1119 [1:17:22<00:00,  1.14s/it]\n",
      "100%|██████████| 1005/1005 [1:50:01<00:00,  2.14s/it]\n",
      "100%|██████████| 962/962 [3:02:28<00:00,  3.48s/it]\n",
      "100%|██████████| 1623/1623 [2:19:34<00:00,  2.74s/it]\n",
      "100%|██████████| 746/746 [1:51:36<00:00,  3.49s/it]\n",
      "100%|██████████| 1307/1307 [2:08:45<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "qid_docid2histvec = {} # mapping from (qid, docid) to histvec\n",
    "for qid in QUERIES.keys():\n",
    "    for docid in tqdm(candidates[qid]):\n",
    "        _hist = get_query_doc_feature(qid, docid).reshape(1,MAX_QLEN,30)\n",
    "        qid_docid2histvec[(qid, docid)] = _hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35764"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qid_docid2histvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_to_pickle = {\n",
    "    'QUERIES': QUERIES,\n",
    "    'candidates': candidates,# mapping qid to list of docids that corresponds to qid in the qrel file \n",
    "    'n_pos': n_pos, # n_pos[qid] = number of positive \n",
    "    'relevance': relevance,  # mapping (qid,docid) pairs to relevance (0,1,2)\n",
    "    'qid_docid2histvec': qid_docid2histvec, # mapping (qid, docid) to histvec\n",
    "    'instances': instances,  # instances[qid] = list (pos_docid, neg_docid) pairs for qid\n",
    "}\n",
    "PK_FOUT = '../data/DRMM+embedding_processed.pk'\n",
    "with open(PK_FOUT, 'wb') as f:\n",
    "    pk.dump(data_to_pickle, f, pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from DRMM import gen_DRMM_model\n",
    "\n",
    "scoring_model, ranking_model = gen_DRMM_model(MAX_QLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VALDATION_SPLIT = 0.2\n",
    "BATCH_SZ = 64\n",
    "NB_EPOCH = 50\n",
    "logdir = '../logs/relevance_matching_0131'\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "_callbacks = [ EarlyStopping(monitor='val_loss', patience=2),\n",
    "               TensorBoard(log_dir=logdir, histogram_freq=0, write_graph=False) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(idx_pairs, batch_size=BATCH_SZ): \n",
    "    # ** parameter `idx_pairs` is list of tuple (qid, pos_docid, neg_docid)**\n",
    "    np.random.shuffle(idx_pairs)\n",
    "    batches_pre_epoch = len(idx_pairs) // batch_size\n",
    "    samples_per_epoch = batches_pre_epoch * batch_size # make samples_per_epoch a multiple of batch size\n",
    "    counter = 0\n",
    "    y_true_batch_dummy = np.ones((batch_size))\n",
    "    while 1:\n",
    "        idx_batch = idx_pairs[batch_size*counter: min(samples_per_epoch, batch_size*(counter+1))]\n",
    "        idfs_batch, pos_batch, neg_batch = [], [], []\n",
    "        for qid, pos_docid, neg_docid in idx_batch:\n",
    "            idfs_batch.append(IDFs[qid])\n",
    "            pos_batch.append(qid_docid2histvec[(qid,pos_docid)].reshape(MAX_QLEN,30))\n",
    "            neg_batch.append(qid_docid2histvec[(qid,neg_docid)].reshape(MAX_QLEN,30))\n",
    "        idfs_batch, pos_batch, neg_batch = map(np.array, [idfs_batch, pos_batch, neg_batch])\n",
    "#         print idfs_batch.shape, pos_batch.shape, neg_batch.shape\n",
    "        counter += 1\n",
    "        if (counter >= batches_pre_epoch):\n",
    "            np.random.shuffle(idx_pairs)\n",
    "            counter=0\n",
    "        yield [idfs_batch, pos_batch, neg_batch], y_true_batch_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idx_pairs(qids):\n",
    "    idx_pairs = []\n",
    "    for qid in qids:\n",
    "        for posid, negid in instances[qid]:\n",
    "            idx_pairs.append( (qid,posid, negid) )\n",
    "    return idx_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_weights = ranking_model.get_weights()\n",
    "\n",
    "def shuffle_weights(model, weights=None):\n",
    "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "    This is a fast approximation of re-initializing the weights of a model.\n",
    "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "      (i.e., the weights have the same distribution along each dimension).\n",
    "    :param Model model: Modify the weights of the given model.\n",
    "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "      If `None`, permute the model's current weights.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TREC_output(qid, run_name = 'my_run', fpath = None):\n",
    "    res = [] # list of (score, pmcid) tuples\n",
    "    for docid in candidates[qid]:\n",
    "        input_idf = IDFs[qid].reshape((-1,MAX_QLEN))\n",
    "        input_hist = qid_docid2histvec[(qid,docid)]\n",
    "        score = scoring_model.predict([input_idf, input_hist])[0]\n",
    "        res.append( (score, docid) )\n",
    "    res = sorted(res, reverse=True)\n",
    "    fout = sys.stdout if fpath==None else open(fpath, 'a')\n",
    "    for rank, (score, docid) in enumerate(res[:2000]):\n",
    "        print >>fout, '%d  Q0  %s  %d  %f  %s' % (qid, docid, rank, score, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KFold(fpath, K = 5, run_name = 'my_run',  batch_size=BATCH_SZ, qids = sorted( QUERIES.keys() )):\n",
    "    open(fpath,'w').close() # clear previous content in file \n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(qids)\n",
    "    fold_sz = len(qids) / K\n",
    "    for fold in xrange(K):\n",
    "        print 'fold %d' % fold, \n",
    "        val_start, val_end = fold*fold_sz, (fold+1)*fold_sz\n",
    "        qids_val = qids[val_start:val_end] # train/val queries for each fold \n",
    "        qids_train = qids[:val_start] + qids[val_end:]\n",
    "        print qids_val\n",
    "        idx_pairs_train = get_idx_pairs(qids_train)\n",
    "        idx_pairs_val = get_idx_pairs(qids_val)\n",
    "        \n",
    "        shuffle_weights(ranking_model, initial_weights) # reset model parameters\n",
    "        ranking_model.fit_generator( batch_generator(idx_pairs_train, batch_size=batch_size), # train model \n",
    "                    samples_per_epoch = len(idx_pairs_train)//batch_size*batch_size,\n",
    "                    nb_epoch=10,\n",
    "                    validation_data=batch_generator(idx_pairs_val, batch_size=batch_size),\n",
    "                    nb_val_samples=len(idx_pairs_val)//batch_size*batch_size, \n",
    "                    callbacks = _callbacks)\n",
    "        print 'fold %d complete, outputting to %s...' % (fold, fpath)\n",
    "        for qid in qids_val:\n",
    "            TREC_output(qid, run_name = run_name, fpath = fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 [15]\n",
      "Epoch 1/10\n",
      "156544/156544 [==============================] - 13s - loss: 0.1041 - ranking_acc: 0.4509 - val_loss: 0.1011 - val_ranking_acc: 0.4341\n",
      "Epoch 2/10\n",
      "156544/156544 [==============================] - 12s - loss: 0.1013 - ranking_acc: 0.4507 - val_loss: 0.1007 - val_ranking_acc: 0.4277\n",
      "Epoch 3/10\n",
      "156544/156544 [==============================] - 12s - loss: 0.1008 - ranking_acc: 0.4509 - val_loss: 0.1006 - val_ranking_acc: 0.4272\n",
      "Epoch 4/10\n",
      "156544/156544 [==============================] - 13s - loss: 0.1006 - ranking_acc: 0.4506 - val_loss: 0.1004 - val_ranking_acc: 0.4255\n",
      "Epoch 5/10\n",
      "156544/156544 [==============================] - 13s - loss: 0.1005 - ranking_acc: 0.4502 - val_loss: 0.1004 - val_ranking_acc: 0.4336\n",
      "Epoch 6/10\n",
      "156544/156544 [==============================] - 12s - loss: 0.1004 - ranking_acc: 0.4504 - val_loss: 0.1003 - val_ranking_acc: 0.4192\n",
      "Epoch 7/10\n",
      "156544/156544 [==============================] - 13s - loss: 0.1004 - ranking_acc: 0.4500 - val_loss: 0.1003 - val_ranking_acc: 0.4302\n",
      "Epoch 8/10\n",
      "156544/156544 [==============================] - 12s - loss: 0.1003 - ranking_acc: 0.4500 - val_loss: 0.1002 - val_ranking_acc: 0.4297\n",
      "Epoch 9/10\n",
      "156544/156544 [==============================] - 12s - loss: 0.1003 - ranking_acc: 0.4498 - val_loss: 0.1003 - val_ranking_acc: 0.4153\n",
      "Epoch 10/10\n",
      "156544/156544 [==============================] - 12s - loss: 0.1003 - ranking_acc: 0.4498 - val_loss: 0.1002 - val_ranking_acc: 0.4287\n",
      "fold 0 complete, outputting to ../data/trec-output/0203_DRMM-embedding_LOO_10epoch.rankedlist...\n",
      "fold 1 [21]\n",
      "Epoch 1/10\n",
      "156288/156288 [==============================] - 12s - loss: 0.0956 - ranking_acc: 0.5565 - val_loss: 0.0884 - val_ranking_acc: 0.6009\n",
      "Epoch 2/10\n",
      "156288/156288 [==============================] - 12s - loss: 0.0931 - ranking_acc: 0.5630 - val_loss: 0.0872 - val_ranking_acc: 0.6156\n",
      "Epoch 3/10\n",
      "156288/156288 [==============================] - 13s - loss: 0.0925 - ranking_acc: 0.5650 - val_loss: 0.0880 - val_ranking_acc: 0.6101\n",
      "Epoch 4/10\n",
      "156288/156288 [==============================] - 13s - loss: 0.0921 - ranking_acc: 0.5670 - val_loss: 0.0866 - val_ranking_acc: 0.6188\n",
      "Epoch 5/10\n",
      "156288/156288 [==============================] - 12s - loss: 0.0918 - ranking_acc: 0.5681 - val_loss: 0.0883 - val_ranking_acc: 0.6080\n",
      "Epoch 6/10\n",
      "156288/156288 [==============================] - 12s - loss: 0.0916 - ranking_acc: 0.5687 - val_loss: 0.0880 - val_ranking_acc: 0.6101\n",
      "Epoch 7/10\n",
      "156288/156288 [==============================] - 13s - loss: 0.0914 - ranking_acc: 0.5698 - val_loss: 0.0886 - val_ranking_acc: 0.5986\n",
      "fold 1 complete, outputting to ../data/trec-output/0203_DRMM-embedding_LOO_10epoch.rankedlist...\n",
      "fold 2 [24]\n",
      "Epoch 1/10\n",
      "152640/152640 [==============================] - 12s - loss: 0.0991 - ranking_acc: 0.5524 - val_loss: 0.0993 - val_ranking_acc: 0.5180\n",
      "Epoch 2/10\n",
      "152640/152640 [==============================] - 12s - loss: 0.0938 - ranking_acc: 0.5609 - val_loss: 0.0993 - val_ranking_acc: 0.5145\n",
      "Epoch 3/10\n",
      "152640/152640 [==============================] - 12s - loss: 0.0933 - ranking_acc: 0.5639 - val_loss: 0.0987 - val_ranking_acc: 0.5215\n",
      "Epoch 4/10\n",
      "152640/152640 [==============================] - 13s - loss: 0.0929 - ranking_acc: 0.5654 - val_loss: 0.0986 - val_ranking_acc: 0.5272\n",
      "Epoch 5/10\n",
      "152640/152640 [==============================] - 12s - loss: 0.0926 - ranking_acc: 0.5671 - val_loss: 0.0986 - val_ranking_acc: 0.5240\n",
      "Epoch 6/10\n",
      "152640/152640 [==============================] - 12s - loss: 0.0922 - ranking_acc: 0.5684 - val_loss: 0.0987 - val_ranking_acc: 0.5200\n",
      "Epoch 7/10\n",
      "152640/152640 [==============================] - 13s - loss: 0.0919 - ranking_acc: 0.5694 - val_loss: 0.0980 - val_ranking_acc: 0.5280\n",
      "Epoch 8/10\n",
      "152640/152640 [==============================] - 13s - loss: 0.0916 - ranking_acc: 0.5708 - val_loss: 0.0987 - val_ranking_acc: 0.5199\n",
      "Epoch 9/10\n",
      "152640/152640 [==============================] - 12s - loss: 0.0913 - ranking_acc: 0.5718 - val_loss: 0.0980 - val_ranking_acc: 0.5288\n",
      "Epoch 10/10\n",
      "152640/152640 [==============================] - 13s - loss: 0.0910 - ranking_acc: 0.5731 - val_loss: 0.0978 - val_ranking_acc: 0.5248\n",
      "fold 2 complete, outputting to ../data/trec-output/0203_DRMM-embedding_LOO_10epoch.rankedlist...\n",
      "fold 3 [25]\n",
      "Epoch 1/10\n",
      "154176/154176 [==============================] - 12s - loss: 0.1060 - ranking_acc: 0.5079 - val_loss: 0.1113 - val_ranking_acc: 0.4963\n",
      "Epoch 2/10\n",
      "154176/154176 [==============================] - 12s - loss: 0.0949 - ranking_acc: 0.5493 - val_loss: 0.0885 - val_ranking_acc: 0.6248\n",
      "Epoch 3/10\n",
      "154176/154176 [==============================] - 12s - loss: 0.0933 - ranking_acc: 0.5572 - val_loss: 0.0808 - val_ranking_acc: 0.6518\n",
      "Epoch 4/10\n",
      "154176/154176 [==============================] - 12s - loss: 0.0927 - ranking_acc: 0.5593 - val_loss: 0.0785 - val_ranking_acc: 0.6580\n",
      "Epoch 5/10\n",
      "154176/154176 [==============================] - 13s - loss: 0.0923 - ranking_acc: 0.5606 - val_loss: 0.0757 - val_ranking_acc: 0.6685\n",
      "Epoch 6/10\n",
      "154176/154176 [==============================] - 12s - loss: 0.0921 - ranking_acc: 0.5617 - val_loss: 0.0748 - val_ranking_acc: 0.6722\n",
      "Epoch 7/10\n",
      "154176/154176 [==============================] - 12s - loss: 0.0920 - ranking_acc: 0.5625 - val_loss: 0.0745 - val_ranking_acc: 0.6713\n",
      "Epoch 8/10\n",
      "154176/154176 [==============================] - 13s - loss: 0.0919 - ranking_acc: 0.5628 - val_loss: 0.0737 - val_ranking_acc: 0.6739\n",
      "Epoch 9/10\n",
      "154176/154176 [==============================] - 12s - loss: 0.0918 - ranking_acc: 0.5634 - val_loss: 0.0723 - val_ranking_acc: 0.6795\n",
      "Epoch 10/10\n",
      "154176/154176 [==============================] - 12s - loss: 0.0918 - ranking_acc: 0.5634 - val_loss: 0.0734 - val_ranking_acc: 0.6748\n",
      "fold 3 complete, outputting to ../data/trec-output/0203_DRMM-embedding_LOO_10epoch.rankedlist...\n",
      "fold 4 [3]\n",
      "Epoch 1/10\n",
      "156160/156160 [==============================] - 12s - loss: 0.1072 - ranking_acc: 0.4681 - val_loss: 0.1035 - val_ranking_acc: 0.4121\n",
      "Epoch 2/10\n",
      "156160/156160 [==============================] - 12s - loss: 0.1024 - ranking_acc: 0.4684 - val_loss: 0.1020 - val_ranking_acc: 0.4114\n",
      "Epoch 3/10\n",
      "156160/156160 [==============================] - 12s - loss: 0.1015 - ranking_acc: 0.4677 - val_loss: 0.1014 - val_ranking_acc: 0.4116\n",
      "Epoch 4/10\n",
      "156160/156160 [==============================] - 13s - loss: 0.1011 - ranking_acc: 0.4679 - val_loss: 0.1014 - val_ranking_acc: 0.4085\n",
      "Epoch 5/10\n",
      "156160/156160 [==============================] - 13s - loss: 0.1009 - ranking_acc: 0.4683 - val_loss: 0.1014 - val_ranking_acc: 0.3998\n",
      "Epoch 6/10\n",
      "156160/156160 [==============================] - 13s - loss: 0.1008 - ranking_acc: 0.4688 - val_loss: 0.1012 - val_ranking_acc: 0.3984\n",
      "Epoch 7/10\n",
      "156160/156160 [==============================] - 13s - loss: 0.1007 - ranking_acc: 0.4694 - val_loss: 0.1013 - val_ranking_acc: 0.3931\n",
      "Epoch 8/10\n",
      "156160/156160 [==============================] - 13s - loss: 0.1006 - ranking_acc: 0.4702 - val_loss: 0.1015 - val_ranking_acc: 0.4033\n",
      "Epoch 9/10\n",
      "156160/156160 [==============================] - 13s - loss: 0.1005 - ranking_acc: 0.4711 - val_loss: 0.1015 - val_ranking_acc: 0.3859\n",
      "fold 4 complete, outputting to ../data/trec-output/0203_DRMM-embedding_LOO_10epoch.rankedlist...\n",
      "fold 5 [4]\n",
      "Epoch 1/10\n",
      "157056/157056 [==============================] - 13s - loss: 0.0985 - ranking_acc: 0.5455 - val_loss: 0.0930 - val_ranking_acc: 0.5854\n",
      "Epoch 2/10\n",
      "157056/157056 [==============================] - 13s - loss: 0.0952 - ranking_acc: 0.5469 - val_loss: 0.0941 - val_ranking_acc: 0.5695\n",
      "Epoch 3/10\n",
      "157056/157056 [==============================] - 13s - loss: 0.0946 - ranking_acc: 0.5515 - val_loss: 0.0947 - val_ranking_acc: 0.5672\n",
      "Epoch 4/10\n",
      "157056/157056 [==============================] - 13s - loss: 0.0942 - ranking_acc: 0.5548 - val_loss: 0.0955 - val_ranking_acc: 0.5728\n",
      "fold 5 complete, outputting to ../data/trec-output/0203_DRMM-embedding_LOO_10epoch.rankedlist...\n",
      "fold 6 [20]\n",
      "Epoch 1/10\n",
      "152640/152640 [==============================] - 12s - loss: 0.1102 - ranking_acc: 0.5039 - val_loss: 0.1029 - val_ranking_acc: 0.5245\n",
      "Epoch 2/10\n",
      "152640/152640 [==============================] - 12s - loss: 0.0982 - ranking_acc: 0.5281 - val_loss: 0.1032 - val_ranking_acc: 0.5065\n",
      "Epoch 3/10\n",
      "152640/152640 [==============================] - 12s - loss: 0.0963 - ranking_acc: 0.5382 - val_loss: 0.1028 - val_ranking_acc: 0.5052\n",
      "Epoch 4/10\n",
      "152640/152640 [==============================] - 12s - loss: 0.0952 - ranking_acc: 0.5446 - val_loss: 0.1033 - val_ranking_acc: 0.4929\n",
      "Epoch 5/10\n",
      "152640/152640 [==============================] - 13s - loss: 0.0944 - ranking_acc: 0.5506 - val_loss: 0.1034 - val_ranking_acc: 0.4819\n",
      "Epoch 6/10\n",
      "152640/152640 [==============================] - 13s - loss: 0.0934 - ranking_acc: 0.5577 - val_loss: 0.1069 - val_ranking_acc: 0.4514\n",
      "fold 6 complete, outputting to ../data/trec-output/0203_DRMM-embedding_LOO_10epoch.rankedlist...\n",
      "fold 7 [2]\n",
      "Epoch 1/10\n",
      "153856/153856 [==============================] - 12s - loss: 0.1054 - ranking_acc: 0.5625 - val_loss: 0.0921 - val_ranking_acc: 0.6198\n",
      "Epoch 2/10\n",
      "153856/153856 [==============================] - 12s - loss: 0.0970 - ranking_acc: 0.5607 - val_loss: 0.0916 - val_ranking_acc: 0.6828\n",
      "Epoch 3/10\n",
      "153856/153856 [==============================] - 13s - loss: 0.0957 - ranking_acc: 0.5608 - val_loss: 0.0926 - val_ranking_acc: 0.6638\n",
      "Epoch 4/10\n",
      "153856/153856 [==============================] - 12s - loss: 0.0950 - ranking_acc: 0.5631 - val_loss: 0.0940 - val_ranking_acc: 0.6079\n",
      "Epoch 5/10\n",
      "153856/153856 [==============================] - 12s - loss: 0.0944 - ranking_acc: 0.5641 - val_loss: 0.0950 - val_ranking_acc: 0.5572\n",
      "fold 7 complete, outputting to ../data/trec-output/0203_DRMM-embedding_LOO_10epoch.rankedlist...\n",
      "fold 8 [13]\n",
      "Epoch 1/10\n",
      "156224/156224 [==============================] - 13s - loss: 0.1033 - ranking_acc: 0.5432 - val_loss: 0.1043 - val_ranking_acc: 0.4817\n",
      "Epoch 2/10\n",
      " 70976/156224 [============>.................] - ETA: 6s - loss: 0.0960 - ranking_acc: 0.5489"
     ]
    }
   ],
   "source": [
    "KFold('../data/trec-output/0203_DRMM-embedding_LOO_10epoch.rankedlist', K=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:thesis_nb]",
   "language": "python",
   "name": "conda-env-thesis_nb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
