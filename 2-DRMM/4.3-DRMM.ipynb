{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "import nltk\n",
    "\n",
    "import os, sys, time\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cPickle as pk\n",
    "np.random.seed(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2V_FPATH = '/local/XW/DATA/WORD_EMBEDDINGS/W2V_BIO/wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "# GLOVE_FPATH = '/local/XW/DATA/WORD_EMBEDDINGS/glove.6B.200d.txt'\n",
    "WD_PLACEHOLDER = '</s>'\n",
    "PMC_PATH = '/local/XW/DATA/TREC/PMCs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/IRdata.pk') as f:\n",
    "    data_pickle = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['querypmc2histvec',\n",
       " 'pos_ids',\n",
       " 'pmcid_2relevance',\n",
       " 'hists_pos',\n",
       " 'hists_neg',\n",
       " 'word2vec',\n",
       " 'neg_ids',\n",
       " 'corpus']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pickle.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Prepare data (run once, then pickle to file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to be pickled: \n",
    "\n",
    "* `pmcid2fpath`    \n",
    "    mapping pmcid to the corresponding file path\n",
    "* `corpus: dict[str, str]`   \n",
    "    mapping pmcid to it's content, the content is a BOW representataion (a Counter), it is pickled into another file called `corpus.pk`\n",
    "* `QUERIES: dict[int, list<str>]`     \n",
    "    mapping qid to it's query represented as a list of words\n",
    "* `QUERIES_padded`   \n",
    "    same as `QUERIES`, but all queries are padded to the same length (=`MAX_QLEN`) with `WD_PLACEHOLDER`\n",
    "* `IDF: dict[str, float]`   \n",
    "    mapping a word to its idf\n",
    "* `relevance: dict[(int,str), int]`   \n",
    "    mapping (qid,docid) pairs to relevance (0,1,2)\n",
    "* `pos_ids, neg_ids`    \n",
    "    mapping `qid` to its list of [pos/neg docids]\n",
    "* `candidates: dict[int, list<str>]`  \n",
    "    mapping qid to list of its candidate docids (that appeared in the qrel)\n",
    "* `qid_docid2histvec: dict[(int,str), array]`    \n",
    "    mapping from (qid, docid) to the corresponding histvec\n",
    "* `instances: dict[int, list<(str,str)>]`    \n",
    "    mapping qid to list, instances[qid] = list of (pos_docid, neg_docid) pairs for qid,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_tree = etree.parse('data/topics2016.xml')\n",
    "\n",
    "def get_topic(i):# returns the summary string of the ith topic\n",
    "    summary = topic_tree.xpath('//topic[@number=\"%d\"]/summary/text()'%i)[0]\n",
    "    return str(summary).lower().strip()\n",
    "\n",
    "# build a mapping of article name (PMCID) to its file path\n",
    "pmcid2fpath = {}\n",
    "\n",
    "for subdir1 in os.listdir(PMC_PATH):\n",
    "    for subdir2 in os.listdir(os.path.join(PMC_PATH, subdir1)):\n",
    "        diry = os.path.join(PMC_PATH, subdir1, subdir2)\n",
    "#         print diry, len(os.listdir(diry))\n",
    "        for fn in os.listdir(diry):\n",
    "            pmcid = fn[:-5]\n",
    "            fpath = os.path.join(diry, fn)\n",
    "            pmcid2fpath[pmcid] = fpath\n",
    "\n",
    "def get_article_abstract(pmcid): # get article title and abstract\n",
    "    fpath = pmcid2fpath[pmcid]\n",
    "    tree = etree.parse(fpath)\n",
    "    ret = u'' + tree.xpath('string(//article-title)') + '\\n'\n",
    "    abstracts = tree.xpath('//abstract')\n",
    "#     abstracts = tree.xpath('//p')\n",
    "    ret += u' '.join( [abstract.xpath('string(.)') for abstract in abstracts] )\n",
    "    if len(ret.split())<20: \n",
    "        raise Exception(u'abstraction too short: '+pmcid + ret)\n",
    "    return ret.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'evaluation of a follow-up programme after curative resection for colorectal cancer\\nfrequent liver imaging can detect liver metastases from colorectal cancer at an asymptomatic stage. \\xa9 1999 cancer research campaign'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_article_abstract('2362203')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/local/XW/DATA/TREC/PMCs/pmc-02/17/1036271.nxml'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmcid2fpath['1036271']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### queries (stopwords removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwds = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERIES = {} # dict[int, list<str>] mapping qid to it's query represented as a list of words\n",
    "for qid in xrange(1,31):\n",
    "    q = [wd for wd in q.split() if (wd not in stopwds) and (wd in word2vec)]\n",
    "    QUERIES[qid] = get_topic(qid).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "MAX_QLEN = max( map(len, QUERIES.values()) ) \n",
    "print MAX_QLEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relevance and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37707/37707 [11:25<00:00, 55.05it/s]\n"
     ]
    }
   ],
   "source": [
    "relevance = {} # dict[(int,str), int] mapping (qid,docid) pairs to its relevance (0,1,2)\n",
    "candidates = {} # dict[int, list<str>] mapping qid to list of its candidate docids (that appeared in the qrel)\n",
    "corpus = {} # dict[str, str] mapping pmcid to it's content\n",
    "\n",
    "with open('data/qrels.txt') as f:\n",
    "    for line in tqdm(f, total=37707): \n",
    "        qid, _, pmcid, rel = line.split()\n",
    "        qid = int(qid)\n",
    "        try:\n",
    "            if pmcid not in corpus: \n",
    "                corpus[pmcid] = get_article_abstract(pmcid)\n",
    "            relevance[(qid,pmcid)] = int(rel)\n",
    "        except: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26255 articles are retrieved\n"
     ]
    }
   ],
   "source": [
    "print '%d articles are retrieved' % len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.vocabulary_ # mapping word to its internal index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idf(wd):\n",
    "    if wd ==WD_PLACEHOLDER: return -10.0\n",
    "    return vectorizer.idf_[ vectorizer.vocabulary_[wd] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word2vec = {} # maps word ---> embedding vector\n",
    "with open(W2V_FPATH) as f:\n",
    "    for line in tqdm(f, total=5443657): # 5443657 400000\n",
    "        vals = line.split()\n",
    "        word = vals[0]\n",
    "        if word in vocab:\n",
    "            vec = np.asarray(vals[1:], dtype='float')\n",
    "            word2vec[word] = vec\n",
    "print 'found %d word vectors.' % len(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwds = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_topic(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_queries = [get_topic(i) for i in xrange(1,31)] \n",
    "QUERIES = []\n",
    "for q in _queries:\n",
    "    q2 =  # filter out stopword and words not in w2v\n",
    "    QUERIES.append(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print map(len, QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = max(map(len, QUERIES)) # = max query length\n",
    "print N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# padding queries to the same length N\n",
    "WD_PLACEHOLDER = '</s>'\n",
    "def pad_query(q, SZ=N):\n",
    "    return q + [WD_PLACEHOLDER]*(SZ-len(q))\n",
    "QUERIES = map(pad_query, QUERIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_histvec(q_wd, doc):\n",
    "    if q_wd == WD_PLACEHOLDER: \n",
    "        return np.zeros(30)\n",
    "    qvec = word2vec[q_wd]\n",
    "#     dvecs = np.vstack( [word2vec.get(wd, randvec) for wd in nltk.word_tokenize(doc)] )\n",
    "    dvecs = np.vstack( [ word2vec[wd] for wd in nltk.word_tokenize(doc) if wd in word2vec] )\n",
    "    cossims = np.dot(dvecs, qvec) / norm(qvec) / norm(dvecs, axis=1)\n",
    "    hist, _ = np.histogram( cossims[cossims<1.0], bins=29, range=(-1,1) )\n",
    "    ones = len(cossims) - sum(hist)\n",
    "    ret = np.array( list(hist) + [ones] )\n",
    "    ret = np.log(ret+1)\n",
    "    return ret # np.reshape(ret, (-1, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_query_doc_feature(query, pmcid): # query: list of words\n",
    "    doc = get_article_abstract(pmcid)\n",
    "    return np.array([ get_histvec(wd, doc) for wd in query])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = 30 # nubmer of used topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_ids, neg_ids = [], [] # pos_ids[q] is a list (positive pmcids for query `q`)\n",
    "hists_pos, hists_neg = [], [] # hists_pos[q] is a list (positive hists for query `q`)\n",
    "                              # hists_pos[q][i] is an array of size N*30 (the ith hists-feature array for query q)\n",
    "for topic in xrange(1,T+1):\n",
    "    query = QUERIES[topic-1]\n",
    "    pos_ids_q, neg_ids_q = [], []\n",
    "    hists_pos_q, hists_neg_q = [], []\n",
    "    relevance = pmcid_2relevance[topic]\n",
    "    for pmcid in tqdm(relevance.keys()):\n",
    "        if relevance[pmcid]==0: \n",
    "            neg_ids_q.append(pmcid)\n",
    "            hists_neg_q.append(get_query_doc_feature(query,pmcid))\n",
    "        else: \n",
    "            pos_ids_q.append(pmcid)\n",
    "            hists_pos_q.append(get_query_doc_feature(query,pmcid))\n",
    "    hists_pos_q, hists_neg_q = map(np.array, [hists_pos_q, hists_neg_q])\n",
    "    hists_pos.append(hists_pos_q); hists_neg.append(hists_neg_q)\n",
    "    pos_ids.append(pos_ids_q); neg_ids.append(neg_ids_q)\n",
    "print len(pos_ids), len(neg_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print map(len, hists_pos)\n",
    "print map(len, hists_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VALDATION_SPLIT = 0.2\n",
    "BATCH_SZ = 128\n",
    "NB_EPOCH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_pairs = [] # list of list, where idx_pairs[q] is a list of tuples of the form: `(q, pos_idx, neg_idx)` for query q\n",
    "for q in xrange(T):\n",
    "    hists_pos_q, hists_neg_q = hists_pos[q], hists_neg[q]\n",
    "    idx_pairs_q = []\n",
    "    for pidx in xrange(len(hists_pos_q)): # here the idx are just row index in hists array\n",
    "        for nidx in xrange(len(hists_neg_q)):\n",
    "            idx_pairs_q.append( (q, pidx, nidx) )\n",
    "    idx_pairs.append(idx_pairs_q)\n",
    "\n",
    "n_val_queries = int(T * VALDATION_SPLIT)\n",
    "idx_pairs_train = reduce( lambda x,y: x+y, idx_pairs[:-n_val_queries] )\n",
    "idx_pairs_val   = reduce( lambda x,y: x+y, idx_pairs[-n_val_queries:] )\n",
    "\n",
    "idx_pairs_train, idx_pairs_val = map(np.array, [idx_pairs_train, idx_pairs_val])\n",
    "\n",
    "print idx_pairs_train.shape, idx_pairs_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDFs = [ np.array ([ get_idf(wd) for wd in query]) for query in QUERIES]\n",
    "IDFs = np.array(IDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(idx_pairs, batch_size=BATCH_SZ): \n",
    "    np.random.shuffle(idx_pairs)\n",
    "    batches_pre_epoch = len(idx_pairs) // batch_size\n",
    "    samples_per_epoch = batches_pre_epoch * batch_size # make samples_per_epoch a multiple of batch size\n",
    "    counter = 0\n",
    "    y_true_batch_dummy = np.ones((batch_size))\n",
    "    while 1:\n",
    "        idx_batch = idx_pairs[batch_size*counter: min(samples_per_epoch, batch_size*(counter+1))]\n",
    "        idfs_batch, pos_batch, neg_batch = [], [], []\n",
    "        for q, pidx, nidx in idx_batch:\n",
    "            idfs_batch.append(IDFs[q])\n",
    "            pos_batch.append(hists_pos[q][pidx])\n",
    "            neg_batch.append(hists_neg[q][nidx])\n",
    "        idfs_batch, pos_batch, neg_batch = map(np.array, [idfs_batch, pos_batch, neg_batch])\n",
    "#         print idfs_batch.shape, pos_batch.shape, neg_batch.shape\n",
    "        counter += 1\n",
    "        if (counter >= batches_pre_epoch):\n",
    "            np.random.shuffle(idx_pairs)\n",
    "            counter=0\n",
    "        \n",
    "        yield [idfs_batch, pos_batch, neg_batch], y_true_batch_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[idfs, pos, neg], ytrue = batch_generator(idx_pairs_train).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IDFs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Define the deep relevance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function for visualization of model\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "def viz_model(model):\n",
    "    return SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct the relevance IR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, InputLayer, Flatten, Input, Merge, merge, Reshape\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2 main components of the structure: feed forward network and gating\n",
    "feed_forward = Sequential(\n",
    "    [Dense(input_dim=30, output_dim=10, activation='relu', bias=False),\n",
    "     Dense(output_dim=5, activation='relu', bias=False),\n",
    "     Dense(output_dim=1, activation='tanh', bias=False)], \n",
    "    name='feed_forward_nw')\n",
    "\n",
    "# ***note: have to wrap ops into Lambda layers !!***\n",
    "# cf: https://groups.google.com/forum/#!topic/keras-users/fbRS-FkZw_Q\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "input_idf = Input(shape=(N,), name='input_idf')\n",
    "\n",
    "w = K.variable(1, name='w_g')\n",
    "def scale(x): \n",
    "    return tf.mul(x,w)\n",
    "def scale_output_shape(input_shape): return input_shape\n",
    "\n",
    "scaled = Lambda(scale, scale_output_shape, name='softmax_scale')(input_idf)\n",
    "gs_out = Activation('softmax', name='softmax')(scaled)\n",
    "gating = Model(input=input_idf, output=gs_out, name='gating')\n",
    "\n",
    "# first input: hist vectors\n",
    "input_hists = Input(shape=(N,30), name='input_hists')\n",
    "\n",
    "def slicei(x, i): return x[:,i,:]\n",
    "def slicei_output_shape(input_shape): return (input_shape[0], input_shape[2])\n",
    "zs = [ feed_forward( Lambda(lambda x:slicei(x,i), slicei_output_shape, name='slice%d'%i)(input_hists) )\\\n",
    "          for i in xrange(N) ]\n",
    "\n",
    "def concat(x): return K.concatenate(x) \n",
    "def concat_output_shape(input_shape): return (input_shape[0][0], N)\n",
    "zs = Lambda(concat, concat_output_shape, name='concat_zs')(zs)\n",
    "\n",
    "# second input: idf scores of each query term \n",
    "input_idf = Input(shape=(N,), name='input_idf')\n",
    "gs = gating(input_idf)\n",
    "\n",
    "def innerprod(x): return K.sum( tf.mul(x[0],x[1]), axis=1)\n",
    "def innerprod_output_shape(input_shape): return (input_shape[0][0],1)\n",
    "scores = Lambda(innerprod, innerprod_output_shape, name='innerprod_zs_gs')([zs, gs])\n",
    "\n",
    "scoring_model = Model(input=[input_idf, input_hists], output=[scores], name='scoring_model')\n",
    "\n",
    "# third input -- the negative hists vector \n",
    "input_hists_neg = Input(shape=(N,30), name='input_hists_neg')\n",
    "\n",
    "zs_neg = [ feed_forward( Lambda(lambda x:slicei(x,i), slicei_output_shape, name='slice%d_neg'%i)(input_hists_neg) )\\\n",
    "          for i in xrange(N) ]\n",
    "\n",
    "zs_neg = Lambda(concat, concat_output_shape, name='concat_zs_neg')(zs_neg)\n",
    "\n",
    "scores_neg = Lambda(innerprod, innerprod_output_shape, name='innerprod_zs_gs_neg')([zs_neg, gs])\n",
    "\n",
    "two_score_model = Model(input=[input_idf, input_hists, input_hists_neg], \n",
    "                        output=[scores, scores_neg], \n",
    "                        name='two_score_model')\n",
    "\n",
    "def diff(x): return tf.sub(x[0], x[1]) #x[0]-x[1]\n",
    "def diff_output_shape(input_shape): return input_shape[0]\n",
    "posneg_score_diff = Lambda(diff, diff_output_shape, name='posneg_score_diff')([scores, scores_neg])\n",
    "ranking_model = Model(input=[input_idf, input_hists,  input_hists_neg]\n",
    "                      , output=[posneg_score_diff]\n",
    "                      , name='ranking_model')\n",
    "\n",
    "# define my loss function: hinge of score_pos - score_neg\n",
    "def pairwise_hinge(y_true, y_pred): # y_pred = score_pos - score_neg, **y_true doesn't matter here**\n",
    "    return K.mean( K.maximum(1. - y_pred, y_true*0.0) )  \n",
    "\n",
    "# self-defined metrics\n",
    "def ranking_acc(y_true, y_pred):\n",
    "    y_pred = y_pred > 0 \n",
    "    return K.mean(y_pred)\n",
    "\n",
    "ranking_model.compile(optimizer='adagrad', loss=pairwise_hinge, metrics=[ranking_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_weights = ranking_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gating.predict(IDFs[0].reshape(-1,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "viz_model(ranking_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "viz_model(scoring_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "querypmc2histvec = {} # mapping from (query_idx, pmcid) to the corresponding input vectors (idf_input, hist_input) tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for q in tqdm(xrange(30)):\n",
    "    query = QUERIES[q]\n",
    "    for pmcid in pmcid_2relevance[q+1].keys():\n",
    "        _idf = IDFs[q].reshape(-1,N)\n",
    "        _hist = get_query_doc_feature(query, pmcid).reshape(1,N,30)\n",
    "        querypmc2histvec[(q, pmcid)] = (_idf, _hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_to_pickle = {\n",
    "    'querypmc2histvec': querypmc2histvec,\n",
    "    'hists_pos': hists_pos,\n",
    "    'hists_neg': hists_neg,\n",
    "    'pos_ids': pos_ids,\n",
    "    'neg_ids': neg_ids,\n",
    "    'pmcid_2relevance': pmcid_2relevance,\n",
    "    'corpus': corpus,\n",
    "    'word2vec': word2vec\n",
    "}\n",
    "with open('data/IRdata.pk', 'wb') as f:\n",
    "    pk.dump(data_to_pickle, f, pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model using `fit_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir = './logs/relevance_matching'\n",
    "_callbacks = [ EarlyStopping(monitor='val_loss', patience=2),\n",
    "               TensorBoard(log_dir=logdir, histogram_freq=0, write_graph=False) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 30-5-1 tanh\n",
    "ranking_model.fit_generator( batch_generator(idx_pairs_train), \n",
    "                    samples_per_epoch = len(idx_pairs_train)//BATCH_SZ*BATCH_SZ,\n",
    "                    nb_epoch=NB_EPOCH,\n",
    "                    validation_data=batch_generator(idx_pairs_val),\n",
    "                    nb_val_samples=len(idx_pairs_val)//BATCH_SZ*BATCH_SZ, \n",
    "                    callbacks = _callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gating.predict(IDFs[0].reshape(-1,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_weights(model, weights=None):\n",
    "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "    This is a fast approximation of re-initializing the weights of a model.\n",
    "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "      (i.e., the weights have the same distribution along each dimension).\n",
    "    :param Model model: Modify the weights of the given model.\n",
    "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "      If `None`, permute the model's current weights.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TREC_output(topic_id, pmcids, run_name = 'my_run', fpath = None):\n",
    "    query = QUERIES[topic_id]\n",
    "    res = [] # list of (score, pmcid) tuples\n",
    "    for pmcid in tqdm(pmcids):\n",
    "        input_idf, input_hist = querypmc2histvec[(topic_id,pmcid)] # get_query_doc_feature(query, pmcid).reshape(1,N,30)\n",
    "        score = scoring_model.predict([input_idf, input_hist])[0]\n",
    "        res.append( (score, pmcid) )\n",
    "    res = sorted(res, reverse=True)\n",
    "#     print res[:10]\n",
    "    fout = sys.stdout if fpath==None else open(fpath, 'a')\n",
    "    for rank, (score, pmcid) in enumerate(res[:1000],1):\n",
    "        print >>fout, '%d  Q0  %s  %d  %f  %s' % (topic_id+1, pmcid, rank, score, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LOO(fpath):\n",
    "    for q in xrange(30):\n",
    "        print '### loo for topic %d ###' % (q+1)\n",
    "        shuffle_weights(ranking_model, initial_weights)\n",
    "        idx_pairs_train = reduce( lambda x,y: x+y, idx_pairs[:q]+idx_pairs[q+1:] )\n",
    "        idx_pairs_val   = idx_pairs[q]\n",
    "        ranking_model.fit_generator( batch_generator(idx_pairs_train), \n",
    "                    samples_per_epoch = len(idx_pairs_train)//BATCH_SZ*BATCH_SZ,\n",
    "                    nb_epoch=20,\n",
    "                    validation_data=batch_generator(idx_pairs_val),\n",
    "                    nb_val_samples=len(idx_pairs_val)//BATCH_SZ*BATCH_SZ, \n",
    "                    callbacks = _callbacks)\n",
    "        sys.stderr.flush()\n",
    "        TREC_output(q, pmcid_2relevance[q+1].keys(), fpath=fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fpath = '/local/SOFT/Trec_eval/my_run_loo.txt'\n",
    "open(fpath, 'w').close() # clear previous content of the file\n",
    "LOO(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## below are some testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zip(pos_ids[0][:10], neg_ids[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = QUERIES[0]\n",
    "pos_sample = get_query_doc_feature(query, '3429740')\n",
    "neg_sample = get_query_doc_feature(query, '3921765')\n",
    "pair_sample = np.array([pos_sample, neg_sample])\n",
    "_idf = np.array([get_idf(wd) for wd in query])\n",
    "idf_sample = np.vstack([_idf]*2)\n",
    "\n",
    "print idf_sample.shape, pair_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_sample[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_forward.predict(pos_sample[-1].reshape(-1,30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test `scoring_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoring_model.predict([idf_sample,pair_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print QUERIES[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = feed_forward.predict(pos_sample)\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = gating.predict(idf_sample)[0]\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b.dot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = feed_forward.predict(neg_sample)\n",
    "# print c\n",
    "print b.dot(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> the scoring model works all right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test ranking_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranking_model.predict( [idf_sample, pair_sample, np.array([neg_sample, pos_sample]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.08474284 - -0.15973815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_score(pmcid):\n",
    "    query = QUERIES[0]\n",
    "    _idf = np.array([get_idf(wd) for wd in query])\n",
    "    _idf = np.vstack([_idf])\n",
    "    _hist = get_query_doc_feature(query, pmcid).reshape(1,N,30)\n",
    "    return scoring_model.predict([_idf, _hist])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " query = QUERIES[0]\n",
    "_idf = np.array([get_idf(wd) for wd in query])\n",
    "_idf = np.vstack([_idf])\n",
    "# pos_sample = get_query_doc_feature(query, '3429740')\n",
    "scoring_model.predict( [_idf, pos_sample.reshape(1,N,30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_score('3429740')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip( map(predict_score, pos_ids[0][:10]), map(predict_score, neg_ids[0][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_score_diff( (pmcid_pos, pmcid_neg) ):\n",
    "    query = QUERIES[0]\n",
    "    _idf = np.array([get_idf(wd) for wd in query])\n",
    "    _idf = np.vstack([_idf])\n",
    "    hist_pos = get_query_doc_feature(query, pmcid_pos).reshape((1,11,30))\n",
    "    hist_neg = get_query_doc_feature(query, pmcid_neg).reshape((1,11,30))\n",
    "    return ranking_model.predict([_idf, hist_pos, hist_neg])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the scoring model (metrics=AP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AP(pos_scores, neg_scores):\n",
    "    Q = len(pos_scores)\n",
    "    pos_tags = [1] * len(pos_scores)\n",
    "    neg_tags = [0] * len(neg_scores)\n",
    "    all_tagged = zip(pos_scores, pos_tags) + zip(neg_scores, neg_tags)\n",
    "    ranked_list = sorted(all_tagged, reverse=True)\n",
    "#     print ranked_list[:20]\n",
    "    ranked_tag = zip(*ranked_list)[1]\n",
    "#     print ranked_tag[:20]\n",
    "    precision_at_i = []\n",
    "    corr, total = 0.0, 0\n",
    "    while corr<Q:\n",
    "        if ranked_tag[total]==1: \n",
    "            corr += 1\n",
    "            precision_at_i.append(corr*1.0 / (total+1) )\n",
    "        total += 1\n",
    "#     print precision_at_i[:20]\n",
    "    return np.mean(precision_at_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AP_of_topic(q):\n",
    "    query = QUERIES[q]\n",
    "    _idf = np.array([get_idf(wd) for wd in query])\n",
    "    _idfs = np.vstack([_idf]*len(hists_pos[q]))\n",
    "    pos_scores = scoring_model.predict( [ _idfs, hists_pos[q]])\n",
    "    _idfs = np.vstack([_idf]*len(hists_neg[q]))\n",
    "    neg_scores = scoring_model.predict( [ _idfs, hists_neg[q]])\n",
    "#     print 'mean:', pos_scores.mean(), neg_scores.mean()\n",
    "#     print 'max:',pos_scores.max(), neg_scores.max()\n",
    "#     print 'min:',pos_scores.min(), neg_scores.min()\n",
    "    return AP(pos_scores, neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(T):\n",
    "    print i+1, AP_of_topic(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for q in xrange(30):\n",
    "    TREC_output(q, pmcid_2relevance[q+1].keys(), fpath='/local/SOFT/Trec_eval/my_run_validation.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To evaluate:\n",
    "\n",
    "trec_eval -q -c -M1000 official_qrels submitted_results"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:thesis_nb]",
   "language": "python",
   "name": "conda-env-thesis_nb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
