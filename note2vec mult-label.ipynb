{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cPickle as pk\n",
    "np.random.seed(1) # to be reproductive\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import Conv2D, MaxPooling2D, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "NOTE_DATA_DIR = '/local/XW/DATA/MIMIC/noteevents_by_sid/'\n",
    "ICD_FPATH = 'data/subject_diag_icds.txt'\n",
    "PK_FPATH = 'data/diag_processed_data.pk' # './processed_data_small.pk'\n",
    "MODEL_PATH = './models/'\n",
    "LOG_PATH = './logs/'\n",
    "# constants\n",
    "N_LABELS = 50\n",
    "K_ICDS_TOKEEP = N_LABELS - 1 # predict only on top K frequent icd codes\n",
    "N_SUBJECTS = 41886\n",
    "# word2vec configurations\n",
    "GLOVE_DIR = '/local/XW/DATA/glove.6B/'\n",
    "MAX_SEQ_LEN = 1000 # max length of input sequence (pad/truncate to fix length)\n",
    "MAX_NB_WORDS = 20000 # top 20k most freq words\n",
    "EMBEDDING_DIM = 100\n",
    "# learning configurations\n",
    "VALIDATION_SPLIT = 0.2\n",
    "N_EPOCHS = 20\n",
    "SZ_BATCH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load pickled data\n",
    "pk_data = pk.load(open(PK_FPATH, 'rb'))\n",
    "embedding_matrix = pk_data['embedding_matrix']\n",
    "X_train, Y_train = pk_data['X_train'], pk_data['Y_train']\n",
    "X_val, Y_val = pk_data['X_val'], pk_data['Y_val']\n",
    "nb_words = MAX_NB_WORDS # forgot to pickle this number..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n",
      "11730\n",
      "(36916, 1000) (36916, 50)\n"
     ]
    }
   ],
   "source": [
    "# found one row that is ALL ) (strange?)\n",
    "print np.min( np.sum(Y_train, axis=1) ), np.min( np.sum(Y_val, axis=1) )\n",
    "print np.argmin( np.sum(Y_train, axis=1) )\n",
    "Y_train[11730]\n",
    "Y_train = np.delete(Y_train, 11730, axis=0)\n",
    "X_train = np.delete(X_train, 11730, axis=0)\n",
    "print X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** this metrics is the continus relaxation of what we really want, so the acc output during training is not precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relax_acc(y_true, y_pred): # shape: (None,N_LABELS)\n",
    "    '''relaxed accuracy for the case when y_true is K-hot \n",
    "    if the predicted icd code is in the patient's icds, then it's good\n",
    "    \n",
    "    **note:**\n",
    "    the y_pred is the softmax output, we need to make it into 1-hot encoding \n",
    "    * via K.round() -- doesn't work well , lots of 0s\n",
    "    * by hand -- doesn't work either: \n",
    "    >InvalidArgumentError: You must feed a value for placeholder tensor 'embedding_input_4' with dtype int32\n",
    "    \n",
    "    ==> so the output is not the accuracy as we defined, but a *continus relaxation* version...\n",
    "    '''\n",
    "    y_int = y_pred * y_true # element-wise mul, intersection\n",
    "\n",
    "    return K.mean( K.sum(y_int, axis=-1) )\n",
    "\n",
    "def multlabel_prec(y_true, y_pred):\n",
    "    y_pred = K.round(K.clip(y_pred, 0, 1)) # turn to 0/1 \n",
    "    tp = K.sum(y_true * y_pred, axis =-1)\n",
    "    sum_true = K.sum(y_true, axis=-1)\n",
    "    sum_pred = K.sum(y_pred, axis=-1)\n",
    "    return K.mean(tp/(sum_pred+1e-10)) # to avoid NaN precision\n",
    "    \n",
    "def multlabel_recall(y_true, y_pred):\n",
    "    y_pred = K.round(K.clip(y_pred, 0, 1)) # turn to 0/1 \n",
    "    tp = K.sum(y_true * y_pred, axis =-1)\n",
    "    sum_true = K.sum(y_true, axis=-1)\n",
    "    sum_pred = K.sum(y_pred, axis=-1)\n",
    "    return K.mean(tp/sum_true) \n",
    "\n",
    "def multlabel_F1(y_true, y_pred):\n",
    "    y_pred = K.round(K.clip(y_pred, 0, 1)) # turn to 0/1 \n",
    "    tp = K.sum(y_true * y_pred, axis =-1)\n",
    "    sum_true = K.sum(y_true, axis=-1)\n",
    "    sum_pred = K.sum(y_pred, axis=-1)\n",
    "    return 2*K.mean(tp/(sum_true+sum_pred))\n",
    "\n",
    "def multlabel_acc(y_true, y_pred):\n",
    "    y_pred = K.round(K.clip(y_pred, 0, 1)) # turn to 0/1 \n",
    "    intersect = y_true * y_pred\n",
    "    intersect = K.sum(intersect, axis=-1)\n",
    "    union = K.clip(y_true+y_pred, 0, 1)\n",
    "    union = K.sum(union, axis=-1)\n",
    "    return K.mean(intersect/union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    print 'evaluation on training set:'\n",
    "    print model.evaluate(X_train, Y_train, batch_size=128)\n",
    "    print 'evaluation on validation set:'\n",
    "    print model.evaluate(X_val, Y_val, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wraps up operations on models\n",
    "def compile_fit_evaluate(model, quick_test=False, print_summary=True,\n",
    "                         save_log=True, save_model=True, del_model=False):\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=[multlabel_prec, multlabel_recall, multlabel_F1, multlabel_acc])\n",
    "    if print_summary:\n",
    "        print model.summary()\n",
    "        \n",
    "    if quick_test: # use tiny data for quick test\n",
    "        print '(quick test mode)'\n",
    "        model.fit(X_train[:100], Y_train[:100], nb_epoch=1)\n",
    "        return  \n",
    "    \n",
    "    _callbacks = [EarlyStopping(monitor='val_loss', patience=2)] #[RelaxAccHistory()]\n",
    "    if save_log:\n",
    "        logdir = os.path.join( LOG_PATH, time.strftime('%m%d')+'_'+str(model.name) )\n",
    "        if not os.path.exists(logdir):\n",
    "            os.makedirs(logdir)\n",
    "        _callbacks.append(TensorBoard(log_dir=logdir))\n",
    "        print 'run \"tensorboard --logdir=%s\" to launch tensorboard'%logdir\n",
    "    \n",
    "    model.fit( X_train, Y_train, \n",
    "              validation_data=(X_val, Y_val),\n",
    "              nb_epoch=N_EPOCHS, batch_size=SZ_BATCH,\n",
    "              callbacks=_callbacks )\n",
    "    \n",
    "    print 'evaluating model...'\n",
    "    evaluate_model(model)\n",
    "    \n",
    "    if save_model: \n",
    "        model_fpath = os.path.join( MODEL_PATH, '%s.h5'% str(model.name) )\n",
    "        model.save(model_fpath)\n",
    "    \n",
    "    if del_model:\n",
    "        del model # delete the model to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ''' ***NOTE***\n",
    "# To load models from file, we have to modify metrics.py at: \n",
    "# `/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras` \n",
    "# to add the `multlabel_XXX` function, otherwise throws exception ! \n",
    "\n",
    "# cf issue: https://github.com/fchollet/keras/issues/3911\n",
    "# '''\n",
    "# m = load_model(os.path.sep.join([MODEL_PATH, 'model_1conv1d.h5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flag_quick_test = 0 # set to False/0 to run on whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 1000, 100)     0           embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 996, 128)      64128       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 199, 128)      0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25472)         0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 50)            1273650     flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1337778\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1019_model_1conv1d\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 423s - loss: 0.1689 - multlabel_prec: 0.8790 - multlabel_recall: 0.5959 - multlabel_F1: 0.6573 - multlabel_acc: 0.5342 - val_loss: 0.2869 - val_multlabel_prec: 0.7227 - val_multlabel_recall: 0.4683 - val_multlabel_F1: 0.5067 - val_multlabel_acc: 0.3747\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 417s - loss: 0.1634 - multlabel_prec: 0.8830 - multlabel_recall: 0.6091 - multlabel_F1: 0.6691 - multlabel_acc: 0.5474 - val_loss: 0.3011 - val_multlabel_prec: 0.6719 - val_multlabel_recall: 0.4883 - val_multlabel_F1: 0.4930 - val_multlabel_acc: 0.3631\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 423s - loss: 0.1606 - multlabel_prec: 0.8841 - multlabel_recall: 0.6216 - multlabel_F1: 0.6782 - multlabel_acc: 0.5576 - val_loss: 0.3223 - val_multlabel_prec: 0.7928 - val_multlabel_recall: 0.4385 - val_multlabel_F1: 0.5011 - val_multlabel_acc: 0.3726\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 412s - loss: 0.1570 - multlabel_prec: 0.8850 - multlabel_recall: 0.6334 - multlabel_F1: 0.6874 - multlabel_acc: 0.5682 - val_loss: 0.3340 - val_multlabel_prec: 0.7967 - val_multlabel_recall: 0.4354 - val_multlabel_F1: 0.5011 - val_multlabel_acc: 0.3726\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 178s   \n",
      "[0.14796571364762118, 0.94358921516030747, 0.56665266581870244, 0.66177252544355958, 0.5435349544319924]\n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 44s    \n",
      "[0.33396343043222759, 0.79669323226769784, 0.43538398353040042, 0.50114596001129774, 0.37261898723115178]\n"
     ]
    }
   ],
   "source": [
    "model_1conv1d_dropout = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQ_LEN, trainable=False # keep the embeddings fixed\n",
    "             ),# embedding layer\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dropout(p=0.2),\n",
    "            Dense(N_LABELS, activation='sigmoid') \n",
    "        ], \n",
    "        name='model_1conv1d_dropout')\n",
    "compile_fit_evaluate(model_1conv1d, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_9 (Embedding)          (None, 1000, 100)     0           embedding_input_9[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_9 (Convolution1D)  (None, 996, 128)      64128       embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_9 (MaxPooling1D)    (None, 199, 128)      0           convolution1d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_10 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_9[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_10 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 4992)          0           maxpooling1d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 4992)          0           flatten_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 50)            249650      dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 395826\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1019_model_2conv1d_dropout\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 509s - loss: 0.2612 - multlabel_prec: 0.8845 - multlabel_recall: 0.3620 - multlabel_F1: 0.4500 - multlabel_acc: 0.3278 - val_loss: 0.2395 - val_multlabel_prec: 0.9175 - val_multlabel_recall: 0.3836 - val_multlabel_F1: 0.4759 - val_multlabel_acc: 0.3552\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 514s - loss: 0.2324 - multlabel_prec: 0.8905 - multlabel_recall: 0.4028 - multlabel_F1: 0.4928 - multlabel_acc: 0.3678 - val_loss: 0.2334 - val_multlabel_prec: 0.8495 - val_multlabel_recall: 0.4194 - val_multlabel_F1: 0.5072 - val_multlabel_acc: 0.3793\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 509s - loss: 0.2239 - multlabel_prec: 0.8837 - multlabel_recall: 0.4213 - multlabel_F1: 0.5115 - multlabel_acc: 0.3847 - val_loss: 0.2258 - val_multlabel_prec: 0.8949 - val_multlabel_recall: 0.4132 - val_multlabel_F1: 0.5060 - val_multlabel_acc: 0.3806\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 515s - loss: 0.2180 - multlabel_prec: 0.8815 - multlabel_recall: 0.4329 - multlabel_F1: 0.5238 - multlabel_acc: 0.3962 - val_loss: 0.2311 - val_multlabel_prec: 0.8981 - val_multlabel_recall: 0.4116 - val_multlabel_F1: 0.5037 - val_multlabel_acc: 0.3793\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 506s - loss: 0.2130 - multlabel_prec: 0.8812 - multlabel_recall: 0.4421 - multlabel_F1: 0.5334 - multlabel_acc: 0.4052 - val_loss: 0.2274 - val_multlabel_prec: 0.8580 - val_multlabel_recall: 0.4320 - val_multlabel_F1: 0.5194 - val_multlabel_acc: 0.3913\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 504s - loss: 0.2084 - multlabel_prec: 0.8794 - multlabel_recall: 0.4509 - multlabel_F1: 0.5423 - multlabel_acc: 0.4138 - val_loss: 0.2240 - val_multlabel_prec: 0.8761 - val_multlabel_recall: 0.4329 - val_multlabel_F1: 0.5216 - val_multlabel_acc: 0.3944\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 504s - loss: 0.2041 - multlabel_prec: 0.8783 - multlabel_recall: 0.4603 - multlabel_F1: 0.5511 - multlabel_acc: 0.4223 - val_loss: 0.2355 - val_multlabel_prec: 0.9075 - val_multlabel_recall: 0.4126 - val_multlabel_F1: 0.5034 - val_multlabel_acc: 0.3795\n",
      "Epoch 8/20\n",
      "36916/36916 [==============================] - 503s - loss: 0.2000 - multlabel_prec: 0.8773 - multlabel_recall: 0.4693 - multlabel_F1: 0.5595 - multlabel_acc: 0.4305 - val_loss: 0.2286 - val_multlabel_prec: 0.8859 - val_multlabel_recall: 0.4247 - val_multlabel_F1: 0.5197 - val_multlabel_acc: 0.3919\n",
      "Epoch 9/20\n",
      "36916/36916 [==============================] - 508s - loss: 0.1961 - multlabel_prec: 0.8772 - multlabel_recall: 0.4795 - multlabel_F1: 0.5690 - multlabel_acc: 0.4401 - val_loss: 0.2264 - val_multlabel_prec: 0.8332 - val_multlabel_recall: 0.4512 - val_multlabel_F1: 0.5315 - val_multlabel_acc: 0.4009\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 210s   \n",
      "[0.18660842960944399, 0.88682974126282388, 0.48829225247233593, 0.58375657676216686, 0.45461712990903508]\n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 52s    \n",
      "[0.22636403118702506, 0.83315276254849646, 0.4512173499855916, 0.5315278532000246, 0.40089527820375737]\n"
     ]
    }
   ],
   "source": [
    "# 2 conv1d layers\n",
    "model_2conv1d_dropout = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dropout(p=0.2),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_2conv1d_dropout')\n",
    "compile_fit_evaluate(model_2conv1d_dropout, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_10 (Embedding)         (None, 1000, 100)     0           embedding_input_10[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_11 (Convolution1D) (None, 996, 128)      64128       embedding_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_11 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_12 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_12 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)             (None, 4992)          0           maxpooling1d_12[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 4992)          0           flatten_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 50)            249650      dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 395826\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1019_model_2conv1d_dropout0.5\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 513s - loss: 0.2604 - multlabel_prec: 0.8813 - multlabel_recall: 0.3641 - multlabel_F1: 0.4512 - multlabel_acc: 0.3282 - val_loss: 0.2404 - val_multlabel_prec: 0.8335 - val_multlabel_recall: 0.3979 - val_multlabel_F1: 0.4842 - val_multlabel_acc: 0.3560\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 507s - loss: 0.2331 - multlabel_prec: 0.8897 - multlabel_recall: 0.4015 - multlabel_F1: 0.4912 - multlabel_acc: 0.3664 - val_loss: 0.2335 - val_multlabel_prec: 0.8318 - val_multlabel_recall: 0.4418 - val_multlabel_F1: 0.5182 - val_multlabel_acc: 0.3891\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 508s - loss: 0.2260 - multlabel_prec: 0.8861 - multlabel_recall: 0.4154 - multlabel_F1: 0.5065 - multlabel_acc: 0.3802 - val_loss: 0.2336 - val_multlabel_prec: 0.8551 - val_multlabel_recall: 0.4333 - val_multlabel_F1: 0.5157 - val_multlabel_acc: 0.3873\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 508s - loss: 0.2213 - multlabel_prec: 0.8850 - multlabel_recall: 0.4245 - multlabel_F1: 0.5161 - multlabel_acc: 0.3887 - val_loss: 0.2271 - val_multlabel_prec: 0.8567 - val_multlabel_recall: 0.4316 - val_multlabel_F1: 0.5208 - val_multlabel_acc: 0.3914\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 547s - loss: 0.2177 - multlabel_prec: 0.8821 - multlabel_recall: 0.4316 - multlabel_F1: 0.5230 - multlabel_acc: 0.3953 - val_loss: 0.2349 - val_multlabel_prec: 0.8962 - val_multlabel_recall: 0.4105 - val_multlabel_F1: 0.5071 - val_multlabel_acc: 0.3811\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 548s - loss: 0.2147 - multlabel_prec: 0.8820 - multlabel_recall: 0.4370 - multlabel_F1: 0.5288 - multlabel_acc: 0.4008 - val_loss: 0.2239 - val_multlabel_prec: 0.8913 - val_multlabel_recall: 0.4138 - val_multlabel_F1: 0.5107 - val_multlabel_acc: 0.3831\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 514s - loss: 0.2122 - multlabel_prec: 0.8826 - multlabel_recall: 0.4407 - multlabel_F1: 0.5333 - multlabel_acc: 0.4050 - val_loss: 0.2243 - val_multlabel_prec: 0.8842 - val_multlabel_recall: 0.4265 - val_multlabel_F1: 0.5202 - val_multlabel_acc: 0.3929\n",
      "Epoch 8/20\n",
      "36916/36916 [==============================] - 508s - loss: 0.2101 - multlabel_prec: 0.8815 - multlabel_recall: 0.4455 - multlabel_F1: 0.5375 - multlabel_acc: 0.4092 - val_loss: 0.2236 - val_multlabel_prec: 0.8579 - val_multlabel_recall: 0.4442 - val_multlabel_F1: 0.5322 - val_multlabel_acc: 0.4028\n",
      "Epoch 9/20\n",
      "36916/36916 [==============================] - 507s - loss: 0.2081 - multlabel_prec: 0.8812 - multlabel_recall: 0.4497 - multlabel_F1: 0.5418 - multlabel_acc: 0.4133 - val_loss: 0.2269 - val_multlabel_prec: 0.8288 - val_multlabel_recall: 0.4571 - val_multlabel_F1: 0.5369 - val_multlabel_acc: 0.4058\n",
      "Epoch 10/20\n",
      "36916/36916 [==============================] - 508s - loss: 0.2062 - multlabel_prec: 0.8824 - multlabel_recall: 0.4521 - multlabel_F1: 0.5449 - multlabel_acc: 0.4163 - val_loss: 0.2272 - val_multlabel_prec: 0.8228 - val_multlabel_recall: 0.4696 - val_multlabel_F1: 0.5433 - val_multlabel_acc: 0.4121\n",
      "Epoch 11/20\n",
      "36916/36916 [==============================] - 507s - loss: 0.2049 - multlabel_prec: 0.8807 - multlabel_recall: 0.4548 - multlabel_F1: 0.5468 - multlabel_acc: 0.4181 - val_loss: 0.2303 - val_multlabel_prec: 0.8702 - val_multlabel_recall: 0.4231 - val_multlabel_F1: 0.5155 - val_multlabel_acc: 0.3884\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 209s   \n",
      "[0.20206636394500113, 0.89436052989722048, 0.44045239170939166, 0.54175107219984142, 0.41321661346317023]\n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 52s    \n",
      "[0.23034473233064601, 0.87016712016901221, 0.42312778882389274, 0.5154620102108306, 0.38837230048273663]\n"
     ]
    }
   ],
   "source": [
    "model_2conv1d_dropout = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dropout(p=0.5),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_2conv1d_dropout0.5')\n",
    "compile_fit_evaluate(model_2conv1d_dropout, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_11 (Embedding)         (None, 1000, 100)     0           embedding_input_11[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_13 (Convolution1D) (None, 996, 128)      64128       embedding_11[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_13 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_14 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_13[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_14 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_15 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_14[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_15 (MaxPooling1D)   (None, 7, 128)        0           convolution1d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 896)           0           maxpooling1d_15[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 896)           0           flatten_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 50)            44850       dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 273074\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1019_model_3conv1d_dropout\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 523s - loss: 0.2705 - multlabel_prec: 0.8731 - multlabel_recall: 0.3481 - multlabel_F1: 0.4339 - multlabel_acc: 0.3117 - val_loss: 0.2461 - val_multlabel_prec: 0.9212 - val_multlabel_recall: 0.3637 - val_multlabel_F1: 0.4557 - val_multlabel_acc: 0.3379\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 521s - loss: 0.2399 - multlabel_prec: 0.8894 - multlabel_recall: 0.3893 - multlabel_F1: 0.4776 - multlabel_acc: 0.3541 - val_loss: 0.2337 - val_multlabel_prec: 0.8936 - val_multlabel_recall: 0.4043 - val_multlabel_F1: 0.4923 - val_multlabel_acc: 0.3693\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 526s - loss: 0.2308 - multlabel_prec: 0.8822 - multlabel_recall: 0.4093 - multlabel_F1: 0.4981 - multlabel_acc: 0.3721 - val_loss: 0.2403 - val_multlabel_prec: 0.8120 - val_multlabel_recall: 0.4466 - val_multlabel_F1: 0.5247 - val_multlabel_acc: 0.3947\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 520s - loss: 0.2259 - multlabel_prec: 0.8788 - multlabel_recall: 0.4195 - multlabel_F1: 0.5097 - multlabel_acc: 0.3825 - val_loss: 0.2281 - val_multlabel_prec: 0.8548 - val_multlabel_recall: 0.4383 - val_multlabel_F1: 0.5252 - val_multlabel_acc: 0.3961\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 521s - loss: 0.2217 - multlabel_prec: 0.8752 - multlabel_recall: 0.4282 - multlabel_F1: 0.5192 - multlabel_acc: 0.3908 - val_loss: 0.2288 - val_multlabel_prec: 0.8830 - val_multlabel_recall: 0.4109 - val_multlabel_F1: 0.5046 - val_multlabel_acc: 0.3783\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 520s - loss: 0.2184 - multlabel_prec: 0.8733 - multlabel_recall: 0.4354 - multlabel_F1: 0.5267 - multlabel_acc: 0.3976 - val_loss: 0.2229 - val_multlabel_prec: 0.8957 - val_multlabel_recall: 0.4210 - val_multlabel_F1: 0.5132 - val_multlabel_acc: 0.3878\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 522s - loss: 0.2152 - multlabel_prec: 0.8734 - multlabel_recall: 0.4409 - multlabel_F1: 0.5332 - multlabel_acc: 0.4038 - val_loss: 0.2228 - val_multlabel_prec: 0.8643 - val_multlabel_recall: 0.4372 - val_multlabel_F1: 0.5281 - val_multlabel_acc: 0.3994\n",
      "Epoch 8/20\n",
      "36916/36916 [==============================] - 521s - loss: 0.2124 - multlabel_prec: 0.8715 - multlabel_recall: 0.4452 - multlabel_F1: 0.5380 - multlabel_acc: 0.4082 - val_loss: 0.2248 - val_multlabel_prec: 0.9075 - val_multlabel_recall: 0.4195 - val_multlabel_F1: 0.5137 - val_multlabel_acc: 0.3886\n",
      "Epoch 9/20\n",
      "36916/36916 [==============================] - 523s - loss: 0.2095 - multlabel_prec: 0.8715 - multlabel_recall: 0.4501 - multlabel_F1: 0.5430 - multlabel_acc: 0.4127 - val_loss: 0.2233 - val_multlabel_prec: 0.8722 - val_multlabel_recall: 0.4416 - val_multlabel_F1: 0.5304 - val_multlabel_acc: 0.4022\n",
      "Epoch 10/20\n",
      "36916/36916 [==============================] - 551s - loss: 0.2072 - multlabel_prec: 0.8699 - multlabel_recall: 0.4546 - multlabel_F1: 0.5477 - multlabel_acc: 0.4172 - val_loss: 0.2261 - val_multlabel_prec: 0.8754 - val_multlabel_recall: 0.4349 - val_multlabel_F1: 0.5237 - val_multlabel_acc: 0.3964\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 233s   \n",
      "[0.20289648318519338, 0.89269097672987507, 0.44812569995137774, 0.54494849574283011, 0.41570314807924913]\n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 56s    \n",
      "[0.22611661563910079, 0.87537567743902178, 0.43492261549868216, 0.52374676826754818, 0.39637121542043374]\n"
     ]
    }
   ],
   "source": [
    "model_3conv1d_dropout =Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dropout(p=0.5),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_3conv1d_dropout')\n",
    "\n",
    "compile_fit_evaluate(model_3conv1d_dropout, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 1000, 100)     0           embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 996, 128)      64128       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 199, 128)      0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25472)         0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 50)            1273650     flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1337778\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1018_model_1conv1d\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 411s - loss: 0.2759 - multlabel_prec: 0.8881 - multlabel_recall: 0.3695 - multlabel_F1: 0.4585 - multlabel_acc: 0.3352 - val_loss: 0.2409 - val_multlabel_prec: 0.8696 - val_multlabel_recall: 0.4040 - val_multlabel_F1: 0.4912 - val_multlabel_acc: 0.3668\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 396s - loss: 0.2259 - multlabel_prec: 0.8921 - multlabel_recall: 0.4152 - multlabel_F1: 0.5062 - multlabel_acc: 0.3810 - val_loss: 0.2394 - val_multlabel_prec: 0.9037 - val_multlabel_recall: 0.4022 - val_multlabel_F1: 0.4931 - val_multlabel_acc: 0.3695\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 395s - loss: 0.2112 - multlabel_prec: 0.8930 - multlabel_recall: 0.4426 - multlabel_F1: 0.5331 - multlabel_acc: 0.4064 - val_loss: 0.2397 - val_multlabel_prec: 0.8614 - val_multlabel_recall: 0.4257 - val_multlabel_F1: 0.5098 - val_multlabel_acc: 0.3826\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 397s - loss: 0.2002 - multlabel_prec: 0.8876 - multlabel_recall: 0.4718 - multlabel_F1: 0.5595 - multlabel_acc: 0.4323 - val_loss: 0.2504 - val_multlabel_prec: 0.7782 - val_multlabel_recall: 0.4479 - val_multlabel_F1: 0.5116 - val_multlabel_acc: 0.3800\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 395s - loss: 0.1915 - multlabel_prec: 0.8847 - multlabel_recall: 0.5019 - multlabel_F1: 0.5851 - multlabel_acc: 0.4576 - val_loss: 0.2820 - val_multlabel_prec: 0.8317 - val_multlabel_recall: 0.4299 - val_multlabel_F1: 0.5132 - val_multlabel_acc: 0.3837\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 167s   \n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 41s    \n"
     ]
    }
   ],
   "source": [
    "# with only 1 conv1d layer\n",
    "model_1conv1d = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQ_LEN, trainable=False # keep the embeddings fixed\n",
    "             ),# embedding layer\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dense(N_LABELS, activation='sigmoid') \n",
    "        ], \n",
    "        name='model_1conv1d')\n",
    "compile_fit_evaluate(model_1conv1d, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 1000, 100)     0           embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 996, 128)      64128       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_2 (MaxPooling1D)    (None, 199, 128)      0           convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_3 (Convolution1D)  (None, 195, 128)      82048       maxpooling1d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_3 (MaxPooling1D)    (None, 39, 128)       0           convolution1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 4992)          0           maxpooling1d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 50)            249650      flatten_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 395826\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1018_model_2conv1d\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 483s - loss: 0.2609 - multlabel_prec: 0.8899 - multlabel_recall: 0.3632 - multlabel_F1: 0.4512 - multlabel_acc: 0.3288 - val_loss: 0.2407 - val_multlabel_prec: 0.9373 - val_multlabel_recall: 0.3667 - val_multlabel_F1: 0.4632 - val_multlabel_acc: 0.3447\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 481s - loss: 0.2328 - multlabel_prec: 0.8906 - multlabel_recall: 0.4031 - multlabel_F1: 0.4926 - multlabel_acc: 0.3679 - val_loss: 0.2400 - val_multlabel_prec: 0.8563 - val_multlabel_recall: 0.4343 - val_multlabel_F1: 0.5122 - val_multlabel_acc: 0.3845\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 481s - loss: 0.2232 - multlabel_prec: 0.8850 - multlabel_recall: 0.4227 - multlabel_F1: 0.5139 - multlabel_acc: 0.3872 - val_loss: 0.2274 - val_multlabel_prec: 0.8861 - val_multlabel_recall: 0.4286 - val_multlabel_F1: 0.5134 - val_multlabel_acc: 0.3879\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 485s - loss: 0.2161 - multlabel_prec: 0.8836 - multlabel_recall: 0.4362 - multlabel_F1: 0.5273 - multlabel_acc: 0.3999 - val_loss: 0.2323 - val_multlabel_prec: 0.9132 - val_multlabel_recall: 0.4046 - val_multlabel_F1: 0.4955 - val_multlabel_acc: 0.3713\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 481s - loss: 0.2101 - multlabel_prec: 0.8821 - multlabel_recall: 0.4485 - multlabel_F1: 0.5394 - multlabel_acc: 0.4112 - val_loss: 0.2254 - val_multlabel_prec: 0.8509 - val_multlabel_recall: 0.4458 - val_multlabel_F1: 0.5273 - val_multlabel_acc: 0.3991\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 486s - loss: 0.2041 - multlabel_prec: 0.8799 - multlabel_recall: 0.4610 - multlabel_F1: 0.5514 - multlabel_acc: 0.4227 - val_loss: 0.2277 - val_multlabel_prec: 0.8616 - val_multlabel_recall: 0.4397 - val_multlabel_F1: 0.5236 - val_multlabel_acc: 0.3951\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 487s - loss: 0.1979 - multlabel_prec: 0.8808 - multlabel_recall: 0.4743 - multlabel_F1: 0.5643 - multlabel_acc: 0.4355 - val_loss: 0.2294 - val_multlabel_prec: 0.8216 - val_multlabel_recall: 0.4561 - val_multlabel_F1: 0.5339 - val_multlabel_acc: 0.4024\n",
      "Epoch 8/20\n",
      "36916/36916 [==============================] - 480s - loss: 0.1918 - multlabel_prec: 0.8784 - multlabel_recall: 0.4906 - multlabel_F1: 0.5785 - multlabel_acc: 0.4499 - val_loss: 0.2303 - val_multlabel_prec: 0.8223 - val_multlabel_recall: 0.4581 - val_multlabel_F1: 0.5359 - val_multlabel_acc: 0.4045\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 198s   \n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 49s    \n"
     ]
    }
   ],
   "source": [
    "# 2 conv1d layers\n",
    "model_2conv1d = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_2conv1d')\n",
    "compile_fit_evaluate(model_2conv1d, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 1000, 100)     0           embedding_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_4 (Convolution1D)  (None, 996, 128)      64128       embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_4 (MaxPooling1D)    (None, 199, 128)      0           convolution1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_5 (Convolution1D)  (None, 195, 128)      82048       maxpooling1d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_5 (MaxPooling1D)    (None, 39, 128)       0           convolution1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_6 (Convolution1D)  (None, 35, 128)       82048       maxpooling1d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_6 (MaxPooling1D)    (None, 7, 128)        0           convolution1d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 896)           0           maxpooling1d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 50)            44850       flatten_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 273074\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1018_model_3conv1d\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 499s - loss: 0.2685 - multlabel_prec: 0.8960 - multlabel_recall: 0.3453 - multlabel_F1: 0.4337 - multlabel_acc: 0.3126 - val_loss: 0.2651 - val_multlabel_prec: 0.7968 - val_multlabel_recall: 0.3965 - val_multlabel_F1: 0.4757 - val_multlabel_acc: 0.3459\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 498s - loss: 0.2406 - multlabel_prec: 0.8924 - multlabel_recall: 0.3875 - multlabel_F1: 0.4763 - multlabel_acc: 0.3531 - val_loss: 0.2548 - val_multlabel_prec: 0.7682 - val_multlabel_recall: 0.4217 - val_multlabel_F1: 0.4896 - val_multlabel_acc: 0.3569\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 501s - loss: 0.2315 - multlabel_prec: 0.8869 - multlabel_recall: 0.4092 - multlabel_F1: 0.4984 - multlabel_acc: 0.3730 - val_loss: 0.2301 - val_multlabel_prec: 0.8751 - val_multlabel_recall: 0.4165 - val_multlabel_F1: 0.4993 - val_multlabel_acc: 0.3741\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 496s - loss: 0.2253 - multlabel_prec: 0.8806 - multlabel_recall: 0.4228 - multlabel_F1: 0.5125 - multlabel_acc: 0.3854 - val_loss: 0.2358 - val_multlabel_prec: 0.9062 - val_multlabel_recall: 0.4043 - val_multlabel_F1: 0.5013 - val_multlabel_acc: 0.3767\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 497s - loss: 0.2205 - multlabel_prec: 0.8753 - multlabel_recall: 0.4352 - multlabel_F1: 0.5254 - multlabel_acc: 0.3971 - val_loss: 0.2283 - val_multlabel_prec: 0.9061 - val_multlabel_recall: 0.4116 - val_multlabel_F1: 0.5088 - val_multlabel_acc: 0.3838\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 496s - loss: 0.2167 - multlabel_prec: 0.8728 - multlabel_recall: 0.4426 - multlabel_F1: 0.5332 - multlabel_acc: 0.4041 - val_loss: 0.2278 - val_multlabel_prec: 0.8923 - val_multlabel_recall: 0.4215 - val_multlabel_F1: 0.5178 - val_multlabel_acc: 0.3912\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 496s - loss: 0.2129 - multlabel_prec: 0.8713 - multlabel_recall: 0.4496 - multlabel_F1: 0.5406 - multlabel_acc: 0.4112 - val_loss: 0.2283 - val_multlabel_prec: 0.8692 - val_multlabel_recall: 0.4263 - val_multlabel_F1: 0.5193 - val_multlabel_acc: 0.3920\n",
      "Epoch 8/20\n",
      "36916/36916 [==============================] - 497s - loss: 0.2093 - multlabel_prec: 0.8713 - multlabel_recall: 0.4556 - multlabel_F1: 0.5466 - multlabel_acc: 0.4168 - val_loss: 0.2246 - val_multlabel_prec: 0.8242 - val_multlabel_recall: 0.4658 - val_multlabel_F1: 0.5429 - val_multlabel_acc: 0.4114\n",
      "Epoch 9/20\n",
      "36916/36916 [==============================] - 496s - loss: 0.2056 - multlabel_prec: 0.8707 - multlabel_recall: 0.4629 - multlabel_F1: 0.5541 - multlabel_acc: 0.4242 - val_loss: 0.2296 - val_multlabel_prec: 0.8808 - val_multlabel_recall: 0.4303 - val_multlabel_F1: 0.5201 - val_multlabel_acc: 0.3931\n",
      "Epoch 10/20\n",
      "36916/36916 [==============================] - 501s - loss: 0.2015 - multlabel_prec: 0.8698 - multlabel_recall: 0.4725 - multlabel_F1: 0.5631 - multlabel_acc: 0.4329 - val_loss: 0.2233 - val_multlabel_prec: 0.8249 - val_multlabel_recall: 0.4619 - val_multlabel_F1: 0.5382 - val_multlabel_acc: 0.4069\n",
      "Epoch 11/20\n",
      "36916/36916 [==============================] - 496s - loss: 0.1971 - multlabel_prec: 0.8705 - multlabel_recall: 0.4818 - multlabel_F1: 0.5720 - multlabel_acc: 0.4417 - val_loss: 0.2295 - val_multlabel_prec: 0.7916 - val_multlabel_recall: 0.4760 - val_multlabel_F1: 0.5423 - val_multlabel_acc: 0.4092\n",
      "Epoch 12/20\n",
      "36916/36916 [==============================] - 497s - loss: 0.1929 - multlabel_prec: 0.8694 - multlabel_recall: 0.4938 - multlabel_F1: 0.5825 - multlabel_acc: 0.4524 - val_loss: 0.2301 - val_multlabel_prec: 0.8055 - val_multlabel_recall: 0.4711 - val_multlabel_F1: 0.5393 - val_multlabel_acc: 0.4076\n",
      "Epoch 13/20\n",
      "36916/36916 [==============================] - 498s - loss: 0.1885 - multlabel_prec: 0.8703 - multlabel_recall: 0.5054 - multlabel_F1: 0.5931 - multlabel_acc: 0.4635 - val_loss: 0.2361 - val_multlabel_prec: 0.8286 - val_multlabel_recall: 0.4563 - val_multlabel_F1: 0.5316 - val_multlabel_acc: 0.4014\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 204s   \n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 51s    \n"
     ]
    }
   ],
   "source": [
    "# 3 conv1d layers \n",
    "model_3conv1d =Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_3conv1d')\n",
    "\n",
    "compile_fit_evaluate(model_3conv1d, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 1000, 100)     0           embedding_input_4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 1000, 100, 1)  0           embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 996, 96, 8)    208         reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 99, 9, 8)      0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 7128)          0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 50)            356450      flatten_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 356658\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1018_model_1conv2d\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 771s - loss: 0.4867 - multlabel_prec: 0.7838 - multlabel_recall: 0.3370 - multlabel_F1: 0.4070 - multlabel_acc: 0.2841 - val_loss: 0.2843 - val_multlabel_prec: 0.8587 - val_multlabel_recall: 0.3420 - val_multlabel_F1: 0.4336 - val_multlabel_acc: 0.3086\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 770s - loss: 0.2663 - multlabel_prec: 0.9096 - multlabel_recall: 0.3325 - multlabel_F1: 0.4324 - multlabel_acc: 0.3107 - val_loss: 0.2708 - val_multlabel_prec: 0.9581 - val_multlabel_recall: 0.3194 - val_multlabel_F1: 0.4245 - val_multlabel_acc: 0.3073\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 772s - loss: 0.2548 - multlabel_prec: 0.8967 - multlabel_recall: 0.3571 - multlabel_F1: 0.4522 - multlabel_acc: 0.3294 - val_loss: 0.2594 - val_multlabel_prec: 0.9160 - val_multlabel_recall: 0.3672 - val_multlabel_F1: 0.4587 - val_multlabel_acc: 0.3392\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 769s - loss: 0.2485 - multlabel_prec: 0.8919 - multlabel_recall: 0.3715 - multlabel_F1: 0.4636 - multlabel_acc: 0.3405 - val_loss: 0.2562 - val_multlabel_prec: 0.8928 - val_multlabel_recall: 0.3657 - val_multlabel_F1: 0.4593 - val_multlabel_acc: 0.3390\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 769s - loss: 0.2428 - multlabel_prec: 0.8895 - multlabel_recall: 0.3817 - multlabel_F1: 0.4721 - multlabel_acc: 0.3485 - val_loss: 0.2591 - val_multlabel_prec: 0.9017 - val_multlabel_recall: 0.3712 - val_multlabel_F1: 0.4617 - val_multlabel_acc: 0.3412\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 770s - loss: 0.2378 - multlabel_prec: 0.8888 - multlabel_recall: 0.3901 - multlabel_F1: 0.4794 - multlabel_acc: 0.3556 - val_loss: 0.2618 - val_multlabel_prec: 0.8763 - val_multlabel_recall: 0.3814 - val_multlabel_F1: 0.4670 - val_multlabel_acc: 0.3455\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 773s - loss: 0.2331 - multlabel_prec: 0.8873 - multlabel_recall: 0.3984 - multlabel_F1: 0.4867 - multlabel_acc: 0.3622 - val_loss: 0.2592 - val_multlabel_prec: 0.7388 - val_multlabel_recall: 0.4189 - val_multlabel_F1: 0.4817 - val_multlabel_acc: 0.3482\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 287s   \n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 71s    \n"
     ]
    }
   ],
   "source": [
    "# 2d conv models\n",
    "'''for 2d conv, the nb_filters cann't be too big: \n",
    "   128*MAX_SEQ_LEN*EMBEDDING_DIM is too much memory\n",
    "   nb_filter = 64 is fine for 1 conv2d layer\n",
    "'''\n",
    "model_1conv2d = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQ_LEN, trainable=False),\n",
    "            Reshape( (MAX_SEQ_LEN, EMBEDDING_DIM, 1) ), # **need to manually reshape and add a channel**\n",
    "            Conv2D(8, 5, 5, activation='relu' ), # , input_shape=(MAX_SEQ_LEN, EMBEDDING_DIM, 1)\n",
    "            MaxPooling2D((10,10)),# need to downsample heavily to reduce parameters... \n",
    "            Flatten(),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_1conv2d')\n",
    "# model_1conv2d.summary()\n",
    "compile_fit_evaluate(model_1conv2d, flag_quick_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_5 (Embedding)          (None, 1000, 100)     0           embedding_input_5[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 1000, 100, 1)  0           embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 996, 96, 32)   832         reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 199, 19, 32)   0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 195, 15, 8)    6408        maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 97, 7, 8)      0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 5432)          0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 50)            271650      flatten_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 278890\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1018_model_2conv2d\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 1783s - loss: 0.2922 - multlabel_prec: 0.9106 - multlabel_recall: 0.3186 - multlabel_F1: 0.4160 - multlabel_acc: 0.2962 - val_loss: 0.2810 - val_multlabel_prec: 0.9706 - val_multlabel_recall: 0.3051 - val_multlabel_F1: 0.4141 - val_multlabel_acc: 0.2983\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 1772s - loss: 0.2763 - multlabel_prec: 0.9489 - multlabel_recall: 0.3125 - multlabel_F1: 0.4174 - multlabel_acc: 0.2991 - val_loss: 0.2751 - val_multlabel_prec: 0.9691 - val_multlabel_recall: 0.3090 - val_multlabel_F1: 0.4165 - val_multlabel_acc: 0.3004\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 1770s - loss: 0.2647 - multlabel_prec: 0.9167 - multlabel_recall: 0.3308 - multlabel_F1: 0.4317 - multlabel_acc: 0.3105 - val_loss: 0.2602 - val_multlabel_prec: 0.8452 - val_multlabel_recall: 0.3736 - val_multlabel_F1: 0.4577 - val_multlabel_acc: 0.3329\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 1769s - loss: 0.2555 - multlabel_prec: 0.9019 - multlabel_recall: 0.3501 - multlabel_F1: 0.4451 - multlabel_acc: 0.3235 - val_loss: 0.2541 - val_multlabel_prec: 0.9276 - val_multlabel_recall: 0.3394 - val_multlabel_F1: 0.4419 - val_multlabel_acc: 0.3220\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 1765s - loss: 0.2535 - multlabel_prec: 0.8959 - multlabel_recall: 0.3625 - multlabel_F1: 0.4547 - multlabel_acc: 0.3325 - val_loss: 0.2519 - val_multlabel_prec: 0.8611 - val_multlabel_recall: 0.3830 - val_multlabel_F1: 0.4679 - val_multlabel_acc: 0.3450\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 1769s - loss: 0.2520 - multlabel_prec: 0.8941 - multlabel_recall: 0.3674 - multlabel_F1: 0.4582 - multlabel_acc: 0.3358 - val_loss: 0.2519 - val_multlabel_prec: 0.9239 - val_multlabel_recall: 0.3668 - val_multlabel_F1: 0.4534 - val_multlabel_acc: 0.3351\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 1763s - loss: 0.2475 - multlabel_prec: 0.8921 - multlabel_recall: 0.3719 - multlabel_F1: 0.4619 - multlabel_acc: 0.3393 - val_loss: 0.2491 - val_multlabel_prec: 0.9058 - val_multlabel_recall: 0.3745 - val_multlabel_F1: 0.4633 - val_multlabel_acc: 0.3431\n",
      "Epoch 8/20\n",
      "36916/36916 [==============================] - 1766s - loss: 0.2455 - multlabel_prec: 0.8922 - multlabel_recall: 0.3738 - multlabel_F1: 0.4636 - multlabel_acc: 0.3408 - val_loss: 0.2478 - val_multlabel_prec: 0.8774 - val_multlabel_recall: 0.3785 - val_multlabel_F1: 0.4688 - val_multlabel_acc: 0.3455\n",
      "Epoch 9/20\n",
      "36916/36916 [==============================] - 1765s - loss: 0.2459 - multlabel_prec: 0.8913 - multlabel_recall: 0.3774 - multlabel_F1: 0.4670 - multlabel_acc: 0.3439 - val_loss: 0.2468 - val_multlabel_prec: 0.8284 - val_multlabel_recall: 0.4050 - val_multlabel_F1: 0.4833 - val_multlabel_acc: 0.3557\n",
      "Epoch 10/20\n",
      "36916/36916 [==============================] - 1765s - loss: 0.2423 - multlabel_prec: 0.8917 - multlabel_recall: 0.3807 - multlabel_F1: 0.4701 - multlabel_acc: 0.3466 - val_loss: 0.2448 - val_multlabel_prec: 0.9168 - val_multlabel_recall: 0.3674 - val_multlabel_F1: 0.4625 - val_multlabel_acc: 0.3419\n",
      "Epoch 11/20\n",
      "36916/36916 [==============================] - 1766s - loss: 0.2453 - multlabel_prec: 0.8884 - multlabel_recall: 0.3833 - multlabel_F1: 0.4722 - multlabel_acc: 0.3484 - val_loss: 0.2442 - val_multlabel_prec: 0.8746 - val_multlabel_recall: 0.3867 - val_multlabel_F1: 0.4746 - val_multlabel_acc: 0.3507\n",
      "Epoch 12/20\n",
      "36916/36916 [==============================] - 1767s - loss: 0.2401 - multlabel_prec: 0.8914 - multlabel_recall: 0.3857 - multlabel_F1: 0.4750 - multlabel_acc: 0.3515 - val_loss: 0.2448 - val_multlabel_prec: 0.8278 - val_multlabel_recall: 0.3937 - val_multlabel_F1: 0.4784 - val_multlabel_acc: 0.3499\n",
      "Epoch 13/20\n",
      "36916/36916 [==============================] - 1762s - loss: 0.2436 - multlabel_prec: 0.8879 - multlabel_recall: 0.3883 - multlabel_F1: 0.4766 - multlabel_acc: 0.3528 - val_loss: 0.2439 - val_multlabel_prec: 0.8294 - val_multlabel_recall: 0.3980 - val_multlabel_F1: 0.4802 - val_multlabel_acc: 0.3518\n",
      "Epoch 14/20\n",
      "36916/36916 [==============================] - 1761s - loss: 0.2383 - multlabel_prec: 0.8890 - multlabel_recall: 0.3899 - multlabel_F1: 0.4788 - multlabel_acc: 0.3547 - val_loss: 0.2498 - val_multlabel_prec: 0.8948 - val_multlabel_recall: 0.3886 - val_multlabel_F1: 0.4719 - val_multlabel_acc: 0.3499\n",
      "Epoch 15/20\n",
      "36916/36916 [==============================] - 1763s - loss: 0.2384 - multlabel_prec: 0.8878 - multlabel_recall: 0.3931 - multlabel_F1: 0.4812 - multlabel_acc: 0.3571 - val_loss: 0.2526 - val_multlabel_prec: 0.7634 - val_multlabel_recall: 0.4414 - val_multlabel_F1: 0.4996 - val_multlabel_acc: 0.3680\n",
      "Epoch 16/20\n",
      "36916/36916 [==============================] - 1763s - loss: 0.2373 - multlabel_prec: 0.8861 - multlabel_recall: 0.3943 - multlabel_F1: 0.4827 - multlabel_acc: 0.3584 - val_loss: 0.2483 - val_multlabel_prec: 0.8862 - val_multlabel_recall: 0.3805 - val_multlabel_F1: 0.4720 - val_multlabel_acc: 0.3489\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 564s   \n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 139s   \n"
     ]
    }
   ],
   "source": [
    "model_2conv2d = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQ_LEN, trainable=False),\n",
    "            Reshape( (MAX_SEQ_LEN, EMBEDDING_DIM, 1) ), # **need to manually reshape and add a channel**\n",
    "            Conv2D(32, 5, 5, activation='relu' ), # , input_shape=(MAX_SEQ_LEN, EMBEDDING_DIM, 1)\n",
    "            MaxPooling2D((5,5)),\n",
    "            Conv2D(8, 5, 5, activation='relu' ), \n",
    "            MaxPooling2D((2,2)),\n",
    "            Flatten(),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_2conv2d')\n",
    "compile_fit_evaluate(model_2conv2d, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_6 (Embedding)          (None, 1000, 100)     0           embedding_input_6[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)              (None, 1000, 100, 1)  0           embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 996, 96, 64)   1664        reshape_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 199, 19, 64)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 195, 15, 32)   51232       maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 97, 7, 32)     0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 93, 3, 8)      6408        maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_6 (MaxPooling2D)    (None, 46, 1, 8)      0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 368)           0           maxpooling2d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 50)            18450       flatten_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 77754\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_3conv2d = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQ_LEN, trainable=False),\n",
    "            Reshape( (MAX_SEQ_LEN, EMBEDDING_DIM, 1) ), # **need to manually reshape and add a channel**\n",
    "            Conv2D(64, 5, 5, activation='relu' ), # , input_shape=(MAX_SEQ_LEN, EMBEDDING_DIM, 1)\n",
    "            MaxPooling2D((5,5)),\n",
    "            Conv2D(32, 5, 5, activation='relu' ), \n",
    "            MaxPooling2D((2,2)),\n",
    "            Conv2D(8, 5, 5, activation='relu' ), \n",
    "            MaxPooling2D((2,2)),\n",
    "            Flatten(),\n",
    "            Dense(N_LABELS, activation='softmax') ],\n",
    "        name='model_3conv2d')\n",
    "print model_3conv2d.summary()\n",
    "# maybe this is too slow to compute? estimated time: 100 * 30 * (N_EPOCH+1) ~= 9hours ...\n",
    "# compile_fit_evaluate(model_3conv2d, flag_quick_test) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:thesis_nb]",
   "language": "python",
   "name": "conda-env-thesis_nb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
