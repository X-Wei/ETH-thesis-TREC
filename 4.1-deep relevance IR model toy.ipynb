{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep relevance matching model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cPickle as pk\n",
    "np.random.seed(1) # to be reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2V_FPATH = '/local/XW/DATA/WORD_EMBEDDINGS/biomed-w2v-200.txt'\n",
    "GLOVE_FPATH = '/local/XW/DATA/WORD_EMBEDDINGS/glove.6B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:13<00:00, 30271.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec = {} # maps word ---> embedding vector\n",
    "with open(GLOVE_FPATH) as f:\n",
    "    for line in tqdm(f, total=400000): #5443657\n",
    "        vals = line.split()\n",
    "        word = vals[0]\n",
    "        vec = np.asarray(vals[1:], dtype='float')\n",
    "#         vec /=  norm(vec)\n",
    "        word2vec[word] = vec\n",
    "print 'found %d word vectors.' % len(word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = 'football game article'\n",
    "documents = \\\n",
    "[\"about the bible quiz answers in article healta saturn wwc edu healta saturn wwc edu tammy r healy writes the cheribums are on the ark of the covenant when god said make no graven image he was refering to idols which were created to be worshipped the ark of the covenant wasn t wrodhipped and only the high priest could enter the holy of holies where it was kept once a year on the day of atonement i am not familiar with or knowledgeable about the original language but i believe there is a word for idol and that the translator would have used the word idol instead of graven image had the original said idol so i think you re wrong here but then again i could be too i just suggesting a way to determine whether the interpretation you offer is correct dean kaflowitz\",\n",
    " \"amusing atheists and agnostics in article timmbake mcl timmbake mcl ucsb edu clam bake timmons writes fallacy atheism is a faith lo i hear the faq beckoning once again wonderful rule deleted you re correct you didn t say anything about a conspiracy correction hard atheism is a faith yes rule don t mix apples with oranges how can you say that the extermination by the mongols was worse than stalin khan conquered people unsympathetic to his cause that was atrocious but stalin killed millions of his own people who loved and worshipped him and his atheist state how can anyone be worse than that i will not explain this to you again stalin did nothing in the name of atheism whethe he was or was not an atheist is irrelevant get a grip man the stalin example was brought up not as an indictment of atheism but merely as another example of how people will kill others under any name that s fit for the occasion no look again while you never said it the implication is pretty clear i m sorry but i can only respond to your words not your true meaning usenet is a slippery medium deleted wrt the burden of proof so hard atheism has nothing to prove then how does it justify that god does not exist i know there s the faq etc but guess what if those justifications were so compelling why aren t people flocking to hard atheism they re not and they won t i for one will discourage people from hard atheism by pointing out those very sources as reliable statements on hard atheism look i m not supporting any dogmatic position i d be a fool to say that in the large group of people that are atheists no people exist who wish to proselytize in the same fashion as religion how many hard atheists do you see posting here anyway maybe i mm just not looking hard enough second what makes you think i m defending any given religion i m merely recognizing hard atheism for what it is a faith i never meant to do so although i understand where you might get that idea i was merely using the bible example as an allegory to illustrate my point and yes by we i am referring to every reader of the post where is the evidence that the poster stated that he relied upon evidence for what who i think i may have lost this thread why theists are arrogant deleted because they say such and such is absolutely unalterably true because my dogma says it is true i am not prepared to issue blanket statements indicting all theists of arrogance as you are wont to do with atheists bzzt by virtue of your innocent little pronoun they you ve just issued a blanket statement at least i will apologize by qualifying my original statement with hard atheist in place of atheist would you call john the baptist arrogant who boasted of one greater than he that s what many christians do today how is that in itself arrogant guilty as charged what i meant to say was the theists who are arrogant are this way because they say other than that i thought my meaning was clear enough any position that claims itself as superior to another with no supporting evidence is arrogant thanks for your apology btw i m not worthy only seriously misinformed with your sophisticated put down of they the theists your serious misinformation shines through explained above bake timmons iii there s nothing higher stronger more wholesome and more useful in life than some good memory alyosha in brothers karamazov dostoevsky\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_histvec(q_wd, doc):\n",
    "    randvec = np.random.randn(200)\n",
    "    qvec = word2vec.get(q_wd, randvec)\n",
    "    dvecs = np.vstack( [word2vec.get(wd, randvec) for wd in doc.split()] )\n",
    "    cossims = np.dot(dvecs, qvec) / norm(qvec) / norm(dvecs, axis=1)\n",
    "    hist, _ = np.histogram( cossims[cossims<1.0], bins=29, range=(-1,1) )\n",
    "    ones = len(cossims) - sum(hist)\n",
    "    ret = np.array( list(hist) + [ones] )\n",
    "    return ret # np.reshape(ret, (-1, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 30) (1, 3)\n"
     ]
    }
   ],
   "source": [
    "word2idf = {'football': 0.7, 'game': 0.5, 'article': 0.3, 'hello': 0.1}\n",
    "N = 3 # max query length\n",
    "\n",
    "# input examples\n",
    "hists = np.array([ get_histvec(wd, documents[0]) for wd in query.split()])\n",
    "hists2 = np.array([ get_histvec(wd, documents[1]) for wd in query.split()])\n",
    "idfs = np.array([word2idf.get(wd,0) for wd in query.split()])\n",
    "idfs = idfs.reshape((-1,N))\n",
    "print hists.shape, idfs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct the relevance IR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, InputLayer, Flatten, Input, Merge, merge, Reshape\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2 main components of the structure: feed forward network and gating\n",
    "feed_forward = Sequential(\n",
    "    [Dense(input_dim=30, output_dim=5, activation='tanh'),\n",
    "     Dense(output_dim=1, activation='tanh')], \n",
    "    name='feed_forward_nw')\n",
    "\n",
    "# inputs_hists = Input(shape=(N,30,))\n",
    "# inputs_hist = Input(shape=(30,))\n",
    "# hidden1 = Dense(5, activation='tanh')(inputs_hist)\n",
    "# hidden2 = Dense(1, activation='tanh')(hidden1)\n",
    "\n",
    "# model.compile()\n",
    "\n",
    "# zs = feed_forward.predict( hists )\n",
    "# print zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ***note: have to wrap ops into Lambda layers !!***\n",
    "# cf: https://groups.google.com/forum/#!topic/keras-users/fbRS-FkZw_Q\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "input_idf = Input(shape=(N,), name='input_idf')\n",
    "\n",
    "def scale(x): \n",
    "    w = K.variable(1e-10, name='w_g')\n",
    "    return K.mul(x,w)\n",
    "def scale_output_shape(input_shape): return input_shape\n",
    "\n",
    "scaled = Lambda(scale, scale_output_shape)(input_idf)\n",
    "gs_out = Activation('softmax')(scaled)\n",
    "gating = Model(input=input_idf, output=gs_out, name='gating')\n",
    "# gating = Sequential([InputLayer(input_shape=(N,)),\n",
    "#                      Activation('softmax')], name='gating')\n",
    "\n",
    "# print gating.predict(idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_hists = Input(shape=(N,30,), name='input_hists')\n",
    "\n",
    "def slicei(x, i=0): return x[:,i,:]\n",
    "def slicei_output_shape(input_shape): return (input_shape[0], input_shape[2])\n",
    "zs = [feed_forward( Lambda(lambda x:slicei(x,i), slicei_output_shape)(input_hists) ) for i in xrange(N) ]\n",
    "\n",
    "def concat(x): return K.reshape(K.concatenate(x), (-1,3))\n",
    "def concat_output_shape(input_shape): return (input_shape[0], N)\n",
    "zs = Lambda(concat, concat_output_shape)(zs)\n",
    "\n",
    "input_idf = Input(shape=(N,), name='input_idf')\n",
    "gs = gating(input_idf)\n",
    "\n",
    "# print zs.get_shape(), gs.get_shape()\n",
    "\n",
    "def innerprod(x): return K.sum( K.mul(x[0],x[1]), axis=1)\n",
    "def innerprod_output_shape(input_shape): return (input_shape[0])\n",
    "scores = Lambda(innerprod, innerprod_output_shape, name='elemul')([zs, gs])\n",
    "\n",
    "# print scores.get_shape()\n",
    "\n",
    "model = Model(input=[input_hists, input_idf], output=[scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75311828], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict( [hists.reshape((1,3,30)), idfs] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_hists_pos = Input(shape=(N,30,), name='input_hists_pos')\n",
    "input_hists_neg = Input(shape=(N,30,), name='input_hists_neg')\n",
    "\n",
    "s_pos = model([input_hists_pos,input_idf])\n",
    "s_neg = model([input_hists_neg, input_idf])\n",
    "\n",
    "def diff(x): return x[0]-x[1]\n",
    "def diff_output_shape(input_shape): return input_shape\n",
    "diff = Lambda(diff, diff_output_shape)([s_pos,s_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mmodel = Model(input=[input_idf, input_hists_pos,  input_hists_neg], output=[diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3206169]\n",
      "[ 0.3206169]\n"
     ]
    }
   ],
   "source": [
    "print mmodel.predict([idfs, hists.reshape((1,3,30)), hists2.reshape((1,3,30))])\n",
    "print mmodel.predict([idfs, hists2.reshape((1,3,30)), hists.reshape((1,3,30))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define my loss function: hinge of score_pos - score_neg\n",
    "def pairwise_hinge(y_true, y_pred): # y_pred = score_pos - score_neg, **y_true doesn't matter here**\n",
    "    return K.mean( K.maximum(1. - y_pred, y_true*0.0) )  \n",
    "\n",
    "mmodel.compile(optimizer='adagrad', loss=pairwise_hinge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s - loss: 1.3206\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s - loss: 0.9313\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s - loss: 0.5628\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s - loss: 0.5417\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s - loss: 0.5298\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s - loss: 0.4125\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s - loss: 0.3700\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s - loss: 0.3377\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s - loss: 0.3101\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s - loss: 0.2855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce6c03d4d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.fit( [idfs, hists.reshape(1,3,30), hists2.reshape(1,3,30)], np.array([1.0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputs_hists = Input(shape=(N,30,))\n",
    "# print inputs_hists.get_shape()\n",
    "# zs = [feed_forward(inputs_hists[:,i,:]) for i in xrange(N)]\n",
    "# zs = K.concatenate(zs, axis=1)\n",
    "# print zs.get_shape()\n",
    "\n",
    "# gs = gating(inputs_idf)\n",
    "# print gs.get_shape()\n",
    "# # print K.dot(gs, K.transpose(zs)).get_shape()\n",
    "# # scores = K.dot(gs, K.transpose(zs))\n",
    "\n",
    "# elemmul = K.mul(gs, zs)\n",
    "# print elemmul.get_shape()\n",
    "# # import tensorflow as tf \n",
    "\n",
    "# # tf.reduce_sum(elemmul, 1)\n",
    "# scores = K.sum(elemmul,axis=1)\n",
    "# scores = K.reshape(scores, [-1,1])\n",
    "# print scores.get_shape()\n",
    "\n",
    "\n",
    "# mmodel.fit( [idfs, hists.reshape(1,3,30), hists2.reshape(1,3,30)], np.array([1.0]) )\n",
    "# Epoch 1/10\n",
    "# 1/1 [==============================] - 0s - loss: 1.3206\n",
    "# Epoch 2/10\n",
    "# 1/1 [==============================] - 0s - loss: 0.9313\n",
    "# Epoch 3/10\n",
    "# 1/1 [==============================] - 0s - loss: 0.5628\n",
    "# Epoch 4/10\n",
    "# 1/1 [==============================] - 0s - loss: 0.5417\n",
    "# Epoch 5/10\n",
    "# 1/1 [==============================] - 0s - loss: 0.5298\n",
    "# Epoch 6/10\n",
    "# 1/1 [==============================] - 0s - loss: 0.4126\n",
    "# Epoch 7/10\n",
    "# 1/1 [==============================] - 0s - loss: 0.3700\n",
    "# Epoch 8/10\n",
    "# 1/1 [==============================] - 0s - loss: 0.3377\n",
    "# Epoch 9/10\n",
    "# 1/1 [==============================] - 0s - loss: 0.3101\n",
    "# Epoch 10/10\n",
    "# 1/1 [==============================] - 0s - loss: 0.2855\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis_nb]",
   "language": "python",
   "name": "conda-env-thesis_nb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
