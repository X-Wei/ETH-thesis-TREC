{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cPickle as pk\n",
    "np.random.seed(1) # to be reproductive\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "NOTE_DATA_DIR = '/local/XW/DATA/MIMIC/noteevents_by_sid/'\n",
    "ICD_FPATH = './subject_diag_icds.txt'\n",
    "PK_FPATH = './diag_processed_data.pk'\n",
    "# constants\n",
    "N_LABELS = 50 \n",
    "K_ICDS_TOKEEP = N_LABELS - 1 # predict only on top K frequent icd codes\n",
    "N_SUBJECTS = 46146\n",
    "# word2vec configurations\n",
    "GLOVE_DIR = '/local/XW/DATA/glove.6B/'\n",
    "MAX_SEQ_LEN = 1000 # max length of input sequence (pad/truncate to fix length)\n",
    "MAX_NB_WORDS = 20000 # top 20k most freq words\n",
    "EMBEDDING_DIM = 100\n",
    "# learning configurations\n",
    "VALIDATION_SPLIT = 0.2\n",
    "N_EPOCHS = 2\n",
    "SZ_BATCH = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If this step is done once, directly go to [step 2](#2.-Model-training) for training. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare label: k-hot encoding for (diagnose) icd code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46146/46146 [00:00<00:00, 83835.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# load the icd code into a dict\n",
    "from collections import Counter\n",
    "sid2icds = {} # map subject_id (int) ---> icd codes of this patient (str)\n",
    "icd_ctr = Counter()\n",
    "with open(ICD_FPATH) as f: \n",
    "    for line in tqdm(f, total=N_SUBJECTS): \n",
    "        sid, _icds = line.split(',')\n",
    "        sid = int(sid)\n",
    "        _icds = _icds.split()\n",
    "        icd_ctr.update(_icds)\n",
    "        sid2icds[sid] = set(_icds)\n",
    "#         print sid, icd[sid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('4019', 17510), ('41401', 10736), ('42731', 10193), ('4280', 9802), ('5849', 7634), ('2724', 7421), ('25000', 7332), ('51881', 6632), ('5990', 5746), ('V053', 5678), ('V290', 5440), ('2720', 5320), ('53081', 5246), ('2859', 4967), ('486', 4391), ('2851', 4231), ('2762', 4120), ('2449', 3789), ('496', 3572), ('99592', 3504), ('V3000', 3503), ('0389', 3387), ('5070', 3362), ('V5861', 3184), ('3051', 2982), ('311', 2907), ('41071', 2902), ('5859', 2889), ('40390', 2814), ('2761', 2789), ('2875', 2783), ('412', 2775), ('V3001', 2707), ('4240', 2643), ('5119', 2554), ('V1582', 2534), ('78552', 2376), ('V4581', 2318), ('4241', 2302), ('9971', 2299), ('42789', 2297), ('V4582', 2247), ('7742', 2241), ('5845', 2154), ('2760', 2077), ('5180', 2072), ('45829', 2055), ('V5867', 2009), ('V502', 1978)]\n"
     ]
    }
   ],
   "source": [
    "print icd_ctr.most_common(K_ICDS_TOKEEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('4019', '41401', '42731', '4280', '5849', '2724', '25000', '51881', '5990', 'V053', 'V290', '2720', '53081', '2859', '486', '2851', '2762', '2449', '496', '99592', 'V3000', '0389', '5070', 'V5861', '3051', '311', '41071', '5859', '40390', '2761', '2875', '412', 'V3001', '4240', '5119', 'V1582', '78552', 'V4581', '4241', '9971', '42789', 'V4582', '7742', '5845', '2760', '5180', '45829', 'V5867', 'V502', 'other')\n"
     ]
    }
   ],
   "source": [
    "icds = zip( *icd_ctr.most_common(K_ICDS_TOKEEP) )[0] + ('other',)\n",
    "print icds # these are icds to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# now turn each subject into a k-hot vector\n",
    "sid2khot = {} # map subject_id to k-hot vector\n",
    "\n",
    "for sid in sid2icds.keys():\n",
    "    _khot = np.zeros(N_LABELS)\n",
    "    for _icd in sid2icds[sid]:\n",
    "        if _icd in icds: \n",
    "            _khot[icds.index(_icd)] = 1\n",
    "        else: # label 'other icds'\n",
    "            _khot[-1] = 1\n",
    "    sid2khot[sid] = _khot\n",
    "\n",
    "print np.array( [sid2khot[i] for i in [2,3]] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare `X`: each note to be a (fixe-length) sequence of word ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal: generate a sid2seq dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46146/46146 [01:45<00:00, 437.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 46146 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sids = []\n",
    "# prepare text\n",
    "texts = [] # text bodies\n",
    "for fname in tqdm(os.listdir(NOTE_DATA_DIR)): # the data is 3.7G in size, can hold in memory...\n",
    "    sids.append( int(fname[:-4]) )\n",
    "    fpath = os.path.join(NOTE_DATA_DIR, fname)\n",
    "    df = pd.read_csv(fpath)\n",
    "    texts.append( '\\n=======\\n\\n\\n'.join(df['text']) )\n",
    "print('found %d texts' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now vectorize the notes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, # filter out numbers, otherwise lots of numbers\n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+'0123456789') \n",
    "tokenizer.fit_on_texts(texts) # this is required before using `texts_to_sequences`, might take time\n",
    "seqs = tokenizer.texts_to_sequences(texts) # turn article into list of ids\n",
    "word_index = tokenizer.word_index # dictionary mapping words (str) to their index (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 371491 unique tokens, use most frequent 20000 of them\n"
     ]
    }
   ],
   "source": [
    "print 'found %s unique tokens, use most frequent %d of them'%(len(word_index), MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 1), ('to', 2), ('the', 3), ('of', 4), ('with', 5), ('for', 6), ('in', 7), ('no', 8), ('is', 9), ('on', 10), ('a', 11), ('was', 12), ('pt', 13), ('at', 14), ('am', 15), ('name', 16), ('mg', 17), ('s', 18), ('o', 19), ('left', 20), ('this', 21), ('as', 22), ('right', 23), ('ml', 24), ('patient', 25), ('pm', 26), ('p', 27), ('l', 28), ('there', 29), ('or', 30), ('not', 31), ('are', 32), ('last', 33), ('from', 34), ('w', 35), ('c', 36), ('chest', 37), ('plan', 38), ('normal', 39), ('po', 40), ('t', 41), ('reason', 42), ('hr', 43), ('clip', 44), ('pain', 45), ('be', 46), ('dl', 47), ('ct', 48), ('has', 49), ('d', 50), ('he', 51), ('continue', 52), ('blood', 53), ('cc', 54), ('assessment', 55), ('x', 56), ('location', 57), ('status', 58), ('by', 59), ('stable', 60), ('day', 61), ('but', 62), ('she', 63), ('year', 64), ('history', 65), ('noted', 66), ('hospital', 67), ('well', 68), ('contrast', 69), ('tube', 70), ('given', 71), ('will', 72), ('old', 73), ('iv', 74), ('examination', 75), ('medical', 76), ('cont', 77), ('radiology', 78), ('respiratory', 79), ('care', 80), ('other', 81), ('r', 82), ('monitor', 83), ('q', 84), ('report', 85), ('acute', 86), ('clear', 87), ('fluid', 88), ('bp', 89), ('number', 90), ('seen', 91), ('were', 92), ('first', 93), ('final', 94), ('had', 95), ('small', 96), ('valve', 97), ('now', 98), ('his', 99), ('her', 100)]\n"
     ]
    }
   ],
   "source": [
    "print sorted(word_index.items(), key=lambda (k,v): v)[:100] # TODO: remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seqs_padded = pad_sequences(seqs, maxlen=MAX_SEQ_LEN)\n",
    "sid2seq = {}\n",
    "for sid, seq in zip(sids,seqs_padded):\n",
    "    sid2seq[sid] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, labels = [], []\n",
    "for sid in sid2seq.keys():\n",
    "    data.append(sid2seq[sid])\n",
    "    labels.append(sid2khot[sid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((46146, 1000), (46146, 50))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data, labels = data[indices], labels[indices]\n",
    "validset_sz = int(VALIDATION_SPLIT*data.shape[0])\n",
    "\n",
    "X_train, Y_train = data[:-validset_sz], labels[:-validset_sz]\n",
    "X_val, Y_val = data[-validset_sz:], labels[-validset_sz:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare embedding matrix(vector of each wd in dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:08<00:00, 48848.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# build index mapping: map word to its vector\n",
    "\n",
    "word2vec = {} # maps word ---> embedding vector\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in tqdm(f, total=400000):\n",
    "        vals = line.split()\n",
    "        word = vals[0]\n",
    "        word2vec[word] = np.asarray(vals[1:], dtype='float')\n",
    "print 'found %d word vectors.' % len(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 371491/371491 [00:00<00:00, 2127266.58it/s]\n"
     ]
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros( (nb_words+1, EMBEDDING_DIM) ) # +1 because ids in sequences starts from 1 ?\n",
    "for word,wd_id in tqdm(word_index.items()): \n",
    "    if wd_id > MAX_NB_WORDS or word not in word2vec: # there might be 0 rows in embedding matrix\n",
    "        continue # word_id>MAX_NB_WORDS, this id is not in the generated sequences, discard\n",
    "    embedding_matrix[wd_id,:] = word2vec[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump useful data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle things, for reuse, and reduce memory\n",
    "data_to_pickle = {\n",
    "    'embedding_matrix': embedding_matrix,\n",
    "    'X_train': X_train,\n",
    "    'Y_train': Y_train,\n",
    "    'X_val': X_val,\n",
    "    'Y_val': Y_val,\n",
    "    'sid2khot': sid2khot,\n",
    "    'sid2seq': sid2seq\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pk.dump(data_to_pickle, open(PK_FPATH,'wb'), pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# also dump a small version for quick check\n",
    "data_to_pickle = {\n",
    "    'embedding_matrix': embedding_matrix,\n",
    "    'X_train': X_train[:1024],\n",
    "    'Y_train': Y_train[:1024],\n",
    "    'X_val': X_val[:128],\n",
    "    'Y_val': Y_val[:128],\n",
    "}\n",
    "pk.dump(data_to_pickle, open('./diag_processed_data_small.pk','wb'), pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reset ipython env, to clear out useless objects in memory\n",
    "%reset -f "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cPickle as pk\n",
    "np.random.seed(1) # to be reproductive\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import Conv2D, MaxPooling2D, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "NOTE_DATA_DIR = '/local/XW/DATA/MIMIC/noteevents_by_sid/'\n",
    "ICD_FPATH = './subject_diag_icds.txt'\n",
    "PK_FPATH = './diag_processed_data.pk' # './processed_data_small.pk'\n",
    "MODEL_PATH = './models/'\n",
    "LOG_PATH = './logs/'\n",
    "# constants\n",
    "N_LABELS = 50\n",
    "K_ICDS_TOKEEP = N_LABELS - 1 # predict only on top K frequent icd codes\n",
    "N_SUBJECTS = 41886\n",
    "# word2vec configurations\n",
    "GLOVE_DIR = '/local/XW/DATA/glove.6B/'\n",
    "MAX_SEQ_LEN = 1000 # max length of input sequence (pad/truncate to fix length)\n",
    "MAX_NB_WORDS = 20000 # top 20k most freq words\n",
    "EMBEDDING_DIM = 100\n",
    "# learning configurations\n",
    "VALIDATION_SPLIT = 0.2\n",
    "N_EPOCHS = 6\n",
    "SZ_BATCH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load pickled data\n",
    "pk_data = pk.load(open(PK_FPATH, 'rb'))\n",
    "embedding_matrix = pk_data['embedding_matrix']\n",
    "X_train, Y_train = pk_data['X_train'], pk_data['Y_train']\n",
    "X_val, Y_val = pk_data['X_val'], pk_data['Y_val']\n",
    "nb_words = MAX_NB_WORDS # forgot to pickle this number..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** this metrics is the continus relaxation of what we really want, so the acc output during training is not precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def relax_acc(y_true, y_pred): # shape: (None,N_LABELS)\n",
    "    '''relaxed accuracy for the case when y_true is K-hot \n",
    "    if the predicted icd code is in the patient's icds, then it's good\n",
    "    \n",
    "    **note:**\n",
    "    the y_pred is the softmax output, we need to make it into 1-hot encoding \n",
    "    * via K.round() -- doesn't work well , lots of 0s\n",
    "    * by hand -- doesn't work either: \n",
    "    >InvalidArgumentError: You must feed a value for placeholder tensor 'embedding_input_4' with dtype int32\n",
    "    \n",
    "    ==> so the output is not the accuracy as we defined, but a *continus relaxation* version...\n",
    "    '''\n",
    "#     y_pred =K.round(y_pred) # doesn't work well, lots of 0s\n",
    "\n",
    "#     onehot = np.zeros_like(y_pred)\n",
    "#     onehot[:, K.eval(K.argmax(y_pred, axis=1))] = 1.0\n",
    "#     import tensorflow as tf\n",
    "#     y_pred = tf.constant(onehot)\n",
    "    \n",
    "#     idx = range(K.int_shape(y_pred)[0])\n",
    "#     K.equal( y_true[idx,K.argmax(y_pred, axis=1)],\n",
    "#             K.ones_like(idx) )\n",
    "    y_int = y_pred * y_true # element-wise mul, intersection\n",
    "\n",
    "    return K.mean( K.sum(y_int, axis=-1) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: custom loss function suitable for multi-label senarios ??\n",
    "# example: element-wise mul of softmax and y_true ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the output metrics during training is not precise, define an `evaluate` function to calculate the metric on training and validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_manual(model,X,Y):\n",
    "    n_correct = 0\n",
    "    for i,y_pred in enumerate(model.predict_classes(X)):\n",
    "        y_true = Y[i,:]\n",
    "        y_true = np.where(y_true==1)[0]\n",
    "    #     print y_pred, y_true, ('correct' if y_pred in y_true else 'wrong')\n",
    "        if y_pred in y_true: n_correct +=1 \n",
    "    return '%d cases checked, %d correct, accuracy=%.9f' % (X.shape[0], n_correct, float(n_correct)/X.shape[0] )\n",
    "\n",
    "def eval_using_relax_acc(model, X, Y):\n",
    "    return '%.9f' % K.eval( relax_acc( K.variable(Y), \n",
    "                               K.variable((to_categorical(model.predict_classes(X)))) )  )\n",
    "\n",
    "def evaluate_model(model):\n",
    "    perf_train = evaluate_manual(model, X_train, Y_train)\n",
    "    print 'relaxed accuracy on training set: %s' % perf_train\n",
    "    \n",
    "    perf_val = evaluate_manual(model, X_val, Y_val)\n",
    "    print 'relaxed accuracy on validation set: %s' % perf_val\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "class RelaxAccHistory(Callback): \n",
    "    'self-defined loss function, at the end of each epoch, run `evaluate_model` to get the true performance'\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        print ''\n",
    "        evaluate_model(self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wraps up operations on models\n",
    "def compile_fit_evaluate(model, quick_test=False, print_summary=True,\n",
    "                         save_log=True, save_model=True, del_model=False):\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=[relax_acc])\n",
    "    if print_summary:\n",
    "        print model.summary()\n",
    "        \n",
    "    if quick_test: # use tiny data for quick test\n",
    "        print '(quick test mode)'\n",
    "        model.fit(X_train[:100], Y_train[:100], nb_epoch=1)\n",
    "        return  \n",
    "    \n",
    "    _callbacks = [EarlyStopping(monitor='val_loss', patience=2)] #[RelaxAccHistory()]\n",
    "    if save_log:\n",
    "        logdir = os.path.join( LOG_PATH, str(model.name) )\n",
    "        if not os.path.exists(logdir):\n",
    "            os.makedirs(logdir)\n",
    "        _callbacks.append(TensorBoard(log_dir=logdir))\n",
    "        print 'run \"tensorboard --logdir=%s\" to launch tensorboard'%logdir\n",
    "    \n",
    "    model.fit( X_train, Y_train, \n",
    "              validation_data=(X_val, Y_val),\n",
    "              nb_epoch=N_EPOCHS, batch_size=SZ_BATCH,\n",
    "              callbacks=_callbacks )\n",
    "    \n",
    "    print 'evaluating model...'\n",
    "    evaluate_model(model)\n",
    "    \n",
    "    if save_model: \n",
    "        model_fpath = os.path.join( MODEL_PATH, '%s.h5'% str(model.name) )\n",
    "        model.save(model_fpath)\n",
    "    \n",
    "    if del_model:\n",
    "        del model # delete the model to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ***NOTE***\\nTo load models from file, we have to modify metrics.py at: \\n`/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras` \\nto add the `relax_acc` function, otherwise throws exception ! \\n\\ncf issue: https://github.com/fchollet/keras/issues/3911\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' ***NOTE***\n",
    "To load models from file, we have to modify metrics.py at: \n",
    "`/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras` \n",
    "to add the `relax_acc` function, otherwise throws exception ! \n",
    "\n",
    "cf issue: https://github.com/fchollet/keras/issues/3911\n",
    "'''\n",
    "# m = load_model(os.path.sep.join([MODEL_PATH, 'model_1conv1d.h5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flag_quick_test = 0 # set to False/0 to run on whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 1000, 100)     0           embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 996, 128)      64128       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 199, 128)      0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25472)         0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 50)            1273650     flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1337778\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/model_1conv1d\" to launch tensorboard\n",
      "Train on 36917 samples, validate on 9229 samples\n",
      "Epoch 1/6\n",
      "36917/36917 [==============================] - 588s - loss: 18.5530 - relax_acc: 0.3563 - val_loss: 17.5602 - val_relax_acc: 0.3615\n",
      "Epoch 2/6\n",
      "36917/36917 [==============================] - 593s - loss: 16.9793 - relax_acc: 0.4291 - val_loss: 17.6914 - val_relax_acc: 0.4693\n",
      "Epoch 3/6\n",
      "36917/36917 [==============================] - 582s - loss: 16.2541 - relax_acc: 0.4817 - val_loss: 18.3412 - val_relax_acc: 0.5339\n",
      "Epoch 4/6\n",
      "36917/36917 [==============================] - 588s - loss: 15.6040 - relax_acc: 0.5315 - val_loss: 18.1193 - val_relax_acc: 0.4204\n",
      "evaluating model...\n",
      "36917/36917 [==============================] - 246s   \n",
      "relaxed accuracy on training set: 36917 cases checked, 35733 correct, accuracy=0.967928055\n",
      "9229/9229 [==============================] - 69s    \n",
      "relaxed accuracy on validation set: 9229 cases checked, 7824 correct, accuracy=0.847762488\n"
     ]
    }
   ],
   "source": [
    "# with only 1 conv1d layer\n",
    "model_1conv1d = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQ_LEN, trainable=False # keep the embeddings fixed\n",
    "             ),# embedding layer\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dense(N_LABELS, activation='softmax') # candidate: sigmoid/tanh?\n",
    "        ], \n",
    "        name='model_1conv1d')\n",
    "compile_fit_evaluate(model_1conv1d, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 1000, 100)     0           embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 996, 128)      64128       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_2 (MaxPooling1D)    (None, 199, 128)      0           convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_3 (Convolution1D)  (None, 195, 128)      82048       maxpooling1d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_3 (MaxPooling1D)    (None, 39, 128)       0           convolution1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 4992)          0           maxpooling1d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 50)            249650      flatten_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 395826\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/model_2conv1d\" to launch tensorboard\n",
      "Train on 36917 samples, validate on 9229 samples\n",
      "Epoch 1/6\n",
      "36917/36917 [==============================] - 721s - loss: 18.0791 - relax_acc: 0.3568 - val_loss: 17.5338 - val_relax_acc: 0.4285\n",
      "Epoch 2/6\n",
      "36917/36917 [==============================] - 680s - loss: 17.2821 - relax_acc: 0.4069 - val_loss: 17.4390 - val_relax_acc: 0.4039\n",
      "Epoch 3/6\n",
      "36917/36917 [==============================] - 686s - loss: 16.9711 - relax_acc: 0.4292 - val_loss: 17.3040 - val_relax_acc: 0.4167\n",
      "Epoch 4/6\n",
      "36917/36917 [==============================] - 732s - loss: 16.6840 - relax_acc: 0.4487 - val_loss: 17.3857 - val_relax_acc: 0.4494\n",
      "Epoch 5/6\n",
      "36917/36917 [==============================] - 818s - loss: 16.3687 - relax_acc: 0.4693 - val_loss: 17.4446 - val_relax_acc: 0.4687\n",
      "Epoch 6/6\n",
      "36917/36917 [==============================] - 716s - loss: 16.0429 - relax_acc: 0.4914 - val_loss: 17.6012 - val_relax_acc: 0.4234\n",
      "evaluating model...\n",
      "36917/36917 [==============================] - 1203s  \n",
      "relaxed accuracy on training set: 36917 cases checked, 35949 correct, accuracy=0.973779018\n",
      "9229/9229 [==============================] - 839s   \n",
      "relaxed accuracy on validation set: 9229 cases checked, 8516 correct, accuracy=0.922743526\n"
     ]
    }
   ],
   "source": [
    "# 2 conv1d layers\n",
    "model_2conv1d = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dense(N_LABELS, activation='softmax') ],\n",
    "        name = 'model_2conv1d')\n",
    "compile_fit_evaluate(model_2conv1d, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 1000, 100)     0           embedding_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_4 (Convolution1D)  (None, 996, 128)      64128       embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_4 (MaxPooling1D)    (None, 199, 128)      0           convolution1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_5 (Convolution1D)  (None, 195, 128)      82048       maxpooling1d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_5 (MaxPooling1D)    (None, 39, 128)       0           convolution1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_6 (Convolution1D)  (None, 35, 128)       82048       maxpooling1d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_6 (MaxPooling1D)    (None, 7, 128)        0           convolution1d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 896)           0           maxpooling1d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 50)            44850       flatten_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 273074\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/model_3conv1d\" to launch tensorboard\n",
      "Train on 36917 samples, validate on 9229 samples\n",
      "Epoch 1/6\n",
      "36917/36917 [==============================] - 6218s - loss: 18.4204 - relax_acc: 0.3423 - val_loss: 17.9878 - val_relax_acc: 0.3046\n",
      "Epoch 2/6\n",
      "36917/36917 [==============================] - 5950s - loss: 17.4836 - relax_acc: 0.3891 - val_loss: 17.3452 - val_relax_acc: 0.4061\n",
      "Epoch 3/6\n",
      "36917/36917 [==============================] - 3547s - loss: 17.2324 - relax_acc: 0.4092 - val_loss: 17.4444 - val_relax_acc: 0.3866\n",
      "Epoch 4/6\n",
      "36917/36917 [==============================] - 3124s - loss: 17.0517 - relax_acc: 0.4235 - val_loss: 17.1485 - val_relax_acc: 0.4345\n",
      "Epoch 5/6\n",
      "36917/36917 [==============================] - 2188s - loss: 16.9029 - relax_acc: 0.4357 - val_loss: 17.2045 - val_relax_acc: 0.3879\n",
      "Epoch 6/6\n",
      "36917/36917 [==============================] - 2077s - loss: 16.7629 - relax_acc: 0.4461 - val_loss: 17.1097 - val_relax_acc: 0.4866\n",
      "evaluating model...\n",
      "36917/36917 [==============================] - 1259s  \n",
      "relaxed accuracy on training set: 36917 cases checked, 36286 correct, accuracy=0.982907604\n",
      "9229/9229 [==============================] - 311s   \n",
      "relaxed accuracy on validation set: 9229 cases checked, 9009 correct, accuracy=0.976162098\n"
     ]
    }
   ],
   "source": [
    "# 3 conv1d layers \n",
    "model_3conv1d =Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dense(N_LABELS, activation='softmax') ],\n",
    "        name = 'model_3conv1d')\n",
    "\n",
    "compile_fit_evaluate(model_3conv1d, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 1000, 100)     0           embedding_input_4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 1000, 100, 1)  0           embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 996, 96, 8)    208         reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 99, 9, 8)      0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 7128)          0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 50)            356450      flatten_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 356658\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/model_1conv2d\" to launch tensorboard\n",
      "Train on 36917 samples, validate on 9229 samples\n",
      "Epoch 1/6\n",
      "36917/36917 [==============================] - 4880s - loss: 64.9977 - relax_acc: 0.7671 - val_loss: 19.2519 - val_relax_acc: 0.2840\n",
      "Epoch 2/6\n",
      "36917/36917 [==============================] - 4865s - loss: 18.5322 - relax_acc: 0.3248 - val_loss: 18.4397 - val_relax_acc: 0.3108\n",
      "Epoch 3/6\n",
      "36917/36917 [==============================] - 2717s - loss: 17.9579 - relax_acc: 0.3581 - val_loss: 18.1104 - val_relax_acc: 0.3973\n",
      "Epoch 4/6\n",
      "36917/36917 [==============================] - 801s - loss: 17.6601 - relax_acc: 0.3745 - val_loss: 18.1461 - val_relax_acc: 0.3158\n",
      "Epoch 5/6\n",
      "36917/36917 [==============================] - 799s - loss: 17.4065 - relax_acc: 0.3875 - val_loss: 18.0564 - val_relax_acc: 0.3882\n",
      "Epoch 6/6\n",
      "36917/36917 [==============================] - 802s - loss: 17.1842 - relax_acc: 0.4004 - val_loss: 18.2653 - val_relax_acc: 0.3413\n",
      "evaluating model...\n",
      "36917/36917 [==============================] - 310s   \n",
      "relaxed accuracy on training set: 36917 cases checked, 31536 correct, accuracy=0.854240594\n",
      "9229/9229 [==============================] - 77s    \n",
      "relaxed accuracy on validation set: 9229 cases checked, 7501 correct, accuracy=0.812764113\n"
     ]
    }
   ],
   "source": [
    "# 2d conv models\n",
    "'''for 2d conv, the nb_filters cann't be too big: \n",
    "   128*MAX_SEQ_LEN*EMBEDDING_DIM is too much memory\n",
    "   nb_filter = 64 is fine for 1 conv2d layer\n",
    "'''\n",
    "model_1conv2d = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQ_LEN, trainable=False),\n",
    "            Reshape( (MAX_SEQ_LEN, EMBEDDING_DIM, 1) ), # **need to manually reshape and add a channel**\n",
    "            Conv2D(8, 5, 5, activation='relu' ), # , input_shape=(MAX_SEQ_LEN, EMBEDDING_DIM, 1)\n",
    "            MaxPooling2D((10,10)),# need to downsample heavily to reduce parameters... \n",
    "            Flatten(),\n",
    "            Dense(N_LABELS, activation='softmax') ],\n",
    "        name = 'model_1conv2d')\n",
    "# model_1conv2d.summary()\n",
    "compile_fit_evaluate(model_1conv2d, flag_quick_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_5 (Embedding)          (None, 1000, 100)     0           embedding_input_5[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 1000, 100, 1)  0           embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 996, 96, 32)   832         reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 199, 19, 32)   0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 195, 15, 8)    6408        maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 97, 7, 8)      0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 5432)          0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 50)            271650      flatten_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 278890\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/model_2conv2d\" to launch tensorboard\n",
      "Train on 36917 samples, validate on 9229 samples\n",
      "Epoch 1/6\n",
      "36917/36917 [==============================] - 1839s - loss: 19.6267 - relax_acc: 0.2947 - val_loss: 18.9179 - val_relax_acc: 0.3388\n",
      "Epoch 2/6\n",
      "36917/36917 [==============================] - 1840s - loss: 18.7508 - relax_acc: 0.3358 - val_loss: 18.3779 - val_relax_acc: 0.3662\n",
      "Epoch 3/6\n",
      "36917/36917 [==============================] - 1836s - loss: 18.0788 - relax_acc: 0.3536 - val_loss: 18.0549 - val_relax_acc: 0.3283\n",
      "Epoch 4/6\n",
      "36917/36917 [==============================] - 1839s - loss: 18.1148 - relax_acc: 0.3637 - val_loss: 17.9530 - val_relax_acc: 0.3569\n",
      "Epoch 5/6\n",
      "36917/36917 [==============================] - 1835s - loss: 17.9822 - relax_acc: 0.3683 - val_loss: 18.1052 - val_relax_acc: 0.3267\n",
      "Epoch 6/6\n",
      "36917/36917 [==============================] - 1966s - loss: 17.8265 - relax_acc: 0.3726 - val_loss: 17.9830 - val_relax_acc: 0.3133\n",
      "evaluating model...\n",
      "36917/36917 [==============================] - 636s   \n",
      "relaxed accuracy on training set: 36917 cases checked, 33843 correct, accuracy=0.916732129\n",
      "9229/9229 [==============================] - 148s   \n",
      "relaxed accuracy on validation set: 9229 cases checked, 8381 correct, accuracy=0.908115722\n"
     ]
    }
   ],
   "source": [
    "model_2conv2d = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQ_LEN, trainable=False),\n",
    "            Reshape( (MAX_SEQ_LEN, EMBEDDING_DIM, 1) ), # **need to manually reshape and add a channel**\n",
    "            Conv2D(32, 5, 5, activation='relu' ), # , input_shape=(MAX_SEQ_LEN, EMBEDDING_DIM, 1)\n",
    "            MaxPooling2D((5,5)),\n",
    "            Conv2D(8, 5, 5, activation='relu' ), \n",
    "            MaxPooling2D((2,2)),\n",
    "            Flatten(),\n",
    "            Dense(N_LABELS, activation='softmax') ],\n",
    "        name = 'model_2conv2d')\n",
    "compile_fit_evaluate(model_2conv2d, flag_quick_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue training the model of 2 conv2d layers.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_2conv2d = load_model('./models/model_2conv2d.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1312/9229 [===>..........................] - ETA: 135s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-50f86fde0d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_manual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_2conv2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-6345f0835242>\u001b[0m in \u001b[0;36mevaluate_manual\u001b[0;34m(model, X, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_manual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mn_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         '''\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1179\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m    876\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_manual(model_2conv2d, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36917 samples, validate on 9229 samples\n",
      "Epoch 1/6\n",
      "  384/36917 [..............................] - ETA: 1729s - loss: 17.1296 - relax_acc: 0.3771"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d55858043a83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSZ_BATCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m               callbacks=[TensorBoard(log_dir='logs/model_2conv2d/')] )\n\u001b[0m",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    822\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_2conv2d.fit( X_train, Y_train, \n",
    "              validation_data=(X_val, Y_val),\n",
    "              nb_epoch=N_EPOCHS, batch_size=SZ_BATCH,\n",
    "              callbacks=[TensorBoard(log_dir='logs/model_2conv2d/')] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_6 (Embedding)          (None, 1000, 100)     0           embedding_input_7[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)              (None, 1000, 100, 1)  0           embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 996, 96, 64)   1664        reshape_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 199, 19, 64)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 195, 15, 32)   51232       maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 97, 7, 32)     0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 93, 3, 8)      6408        maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_6 (MaxPooling2D)    (None, 46, 1, 8)      0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 368)           0           maxpooling2d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 50)            18450       flatten_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 77754\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_3conv2d = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQ_LEN, trainable=False),\n",
    "            Reshape( (MAX_SEQ_LEN, EMBEDDING_DIM, 1) ), # **need to manually reshape and add a channel**\n",
    "            Conv2D(64, 5, 5, activation='relu' ), # , input_shape=(MAX_SEQ_LEN, EMBEDDING_DIM, 1)\n",
    "            MaxPooling2D((5,5)),\n",
    "            Conv2D(32, 5, 5, activation='relu' ), \n",
    "            MaxPooling2D((2,2)),\n",
    "            Conv2D(8, 5, 5, activation='relu' ), \n",
    "            MaxPooling2D((2,2)),\n",
    "            Flatten(),\n",
    "            Dense(N_LABELS, activation='softmax') ],\n",
    "        name='model_3conv2d')\n",
    "print model_3conv2d.summary()\n",
    "# maybe this is too slow to compute? estimated time: 100 * 30 * (N_EPOCH+1) ~= 9hours ...\n",
    "# compile_fit_evaluate(model_3conv2d, flag_quick_test) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
