{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, time, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cPickle as pk\n",
    "np.random.seed(1) # to be reproductive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "PK_FPATH = 'data/processed_data_sidhid.pk'\n",
    "MODEL_FPATH = './models/1124_model_2embed_2conv1d_2FC.h5' # path of best trained model \n",
    "NOTES_DIR = '/local/XW/DATA/MIMIC/noteevents_by_sid_hid/'\n",
    "# constants\n",
    "MAX_NB_WORDS = 20000 # top 20k most freq words\n",
    "MAX_SEQ_LEN = 1000\n",
    "N_LABELS = 50\n",
    "N_SIDHID = 58328"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load pickled data\n",
    "pk_data = pk.load(open(PK_FPATH, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pk_data['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file contains the prepared data for note2vec training, \n",
      "* sidhids:     list of the 58361 unique (sid,hid) pairs\n",
      "* sidhid2icds: mapping from (sid,hid) pair --> set of icd codes\n",
      "* sidhid2khot: mapping from (sid,hid) pair --> khot-encoding correponding to this sidhid pair\n",
      "* sidhid2seq:  mapping from (sid,hid) pair --> fix-length sequences (len=1000) of word ids\n",
      "* word2idx:    mapping from a word to its id used in the sequence\n",
      "* embedding_w2v／embedding_glove: matrices for the embedding layer (used as the weights parameter)\n",
      "* train_sidhids/val_sidhids: list of (sid,hid) pairs used as training/validation set\n",
      "* X_train/Y_train/X_val/Y_val: ndarray generated for training/validation\n",
      "\n",
      "And here are 2 useful functions' source code: \n",
      "\n",
      "def to_khot(sidhid2icds, K=N_LABELS): # generate khot encoding (useful if want to change the K)\n",
      "    icds = zip( *icd_ctr.most_common(N_LABELS-1) )[0] + ('other',)\n",
      "    sidhid2khot = {} # map subject_id to k-hot vector\n",
      "    for sid,hid in sidhid2icds.keys():\n",
      "        _khot = np.zeros(N_LABELS)\n",
      "        for _icd in sidhid2icds[(sid,hid)]:\n",
      "            if _icd in icds: \n",
      "                _khot[icds.index(_icd)] = 1\n",
      "            else: # label 'other icds'\n",
      "                _khot[-1] = 1\n",
      "        sidhid2khot[(sid,hid)] = _khot\n",
      "    return sidhid2khot\n",
      "\n",
      "def getXY(sidhid_lst): # give a list of (sid, hid) pairs, generate the X and Y\n",
      "    data, labels = [], []\n",
      "    for sidhid in sidhid_lst:\n",
      "        data.append(sidhid2seq[sidhid])\n",
      "        labels.append(sidhid2khot[sidhid])\n",
      "    X = np.array(data)\n",
      "    Y = np.array(labels)\n",
      "    return X,Y\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print pk_data['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ***NOTE***\n",
    "# To load models from file, we have to modify metrics.py at: \n",
    "# `/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras/` \n",
    "# to add the custom metric function, otherwise `load_model` throws exception ! \n",
    "# cf issue: https://github.com/fchollet/keras/issues/3911\n",
    "from keras.models import load_model\n",
    "model = load_model(MODEL_FPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 1000, 200)     0           main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 1000, 200)     0           main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_4 (Convolution1D)  (None, 996, 128)      128128      embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_5 (Convolution1D)  (None, 996, 128)      128128      embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 996, 128)      0           convolution1d_4[0][0]            \n",
      "                                                                   convolution1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_3 (MaxPooling1D)    (None, 199, 128)      0           merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_6 (Convolution1D)  (None, 195, 128)      82048       maxpooling1d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_4 (MaxPooling1D)    (None, 39, 128)       0           convolution1d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 4992)          0           maxpooling1d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 4992)          0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 500)           2496500     dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 500)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 50)            25050       dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2859854\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"main_input:0\", shape=(?, 1000), dtype=int32)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 500), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print model.layers[0].input\n",
    "print model.layers[11].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use K.function to construct a model that outputs embedding vector\n",
    "from keras import backend as K\n",
    "get_embedvec = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                  [model.layers[11].output])\n",
    "embedvec = lambda X: get_embedvec([X,0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 500)\n"
     ]
    }
   ],
   "source": [
    "# output in test mode = 0\n",
    "layer_output = embedvec(X_train[:10])\n",
    "print layer_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn a paragraph into 500-dimensional input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sidhids = []\n",
    "# texts = [] # text bodies\n",
    "# for fname in tqdm(os.listdir(NOTES_DIR)): # the data is 3.7G in size, can hold in memory...\n",
    "#     sid,hid = map( int, fname[:-4].split('_') )\n",
    "#     sidhids.append( (sid,hid) )\n",
    "#     fpath = os.path.join(NOTES_DIR, fname)\n",
    "#     df = pd.read_csv(fpath)\n",
    "#     texts.append( '\\n=======\\n\\n\\n'.join(df['text']) )\n",
    "# print('found %d texts' % len(texts))\n",
    "\n",
    "# tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, # filter out numbers, otherwise lots of numbers\n",
    "#                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+'0123456789') \n",
    "# print 'fitting on whole text corpus...',\n",
    "# tokenizer.fit_on_texts(texts) # this might take some time\n",
    "# print 'done. '\n",
    "\n",
    "# pk.dump(tokenizer, open('data/tokenizer.pk', 'wb'), pk.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('data/tokenizer.pk', 'rb') as f:\n",
    "    tokenizer = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paragraph2vec(paragraph):\n",
    "    seqs = tokenizer.texts_to_sequences([paragraph])\n",
    "    seqs_padded = pad_sequences(seqs, maxlen=MAX_SEQ_LEN)\n",
    "    return embedvec(seqs_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_sample = ''' The imaged portions of the abdomen show a few [**Last Name (un) 36399**]-filled loops of bowel\n",
    "   within the left abdomen.  No abnormal soft tissue mass or calcifications.  No\n",
    "   free interperitoneal air.  The imaged bony structures are unremarkable.'''\n",
    "paragraph2vec(paragraph_sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide_paragraphs(text):\n",
    "    pat = re.compile('\\W*\\n\\W*\\n')\n",
    "    return pat.split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to extract histvec from query/article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PMC_PATH = '/local/XW/DATA/TREC/PMCs/'\n",
    "pmcid2fpath = {}\n",
    "\n",
    "for subdir1 in os.listdir(PMC_PATH):\n",
    "    for subdir2 in os.listdir(os.path.join(PMC_PATH, subdir1)):\n",
    "        diry = os.path.join(PMC_PATH, subdir1, subdir2)\n",
    "        for fn in os.listdir(diry):\n",
    "            pmcid = fn[:-5]\n",
    "            fpath = os.path.join(diry, fn)\n",
    "            pmcid2fpath[pmcid] = fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_tree = etree.parse('data/topics2016.xml')\n",
    "\n",
    "def get_query_paragraphs(i):# returns the summary string of the ith topic\n",
    "    note = topic_tree.xpath('//topic[@number=\"%d\"]/note/text()'%i)[0]\n",
    "    return divide_paragraphs( str(note).lower() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( get_query_paragraphs(1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_paragraphs(pmcid):\n",
    "    'returns a list of texts, each as a paragraph'\n",
    "    fpath = pmcid2fpath[pmcid]\n",
    "    tree = etree.parse(fpath)\n",
    "    ret = []\n",
    "    body = tree.xpath('//body')[0]\n",
    "    for p in body.xpath('.//p'):\n",
    "        ret.append( p.xpath('string(.)') )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_article_paragraphs('107838')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARA_PLACEHOLDER = '</s>'\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_histvec(query_para, pmcid):\n",
    "    if query_para == PARA_PLACEHOLDER: \n",
    "        return np.zeros(30)\n",
    "    qvec = paragraph2vec(query_para)\n",
    "    dvecs = np.vstack( [ paragraph2vec(p.encode('ascii','ignore')) for p in get_article_paragraphs(pmcid)] )\n",
    "    cossims = np.dot(dvecs, qvec.T) / norm(qvec) / norm(dvecs, axis=1)\n",
    "    hist, _ = np.histogram( cossims, bins=30, range=(0,1) )\n",
    "    ret = np.log(hist+1)\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_histvec(get_query_paragraphs(1)[1], '107838')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_para = get_query_paragraphs(1)[1] \n",
    "pmcid = '107838'\n",
    "qvec = paragraph2vec(query_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_query_doc_feature(query, pmcid): # query: list of paragraphs\n",
    "    return np.array([ get_histvec(p, pmcid) for p in query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_query_doc_feature(get_query_paragraphs(1), '107838')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data: padded queries, positive and negative histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERIES = [get_query_paragraphs(i) for i in xrange(1,31)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 10, 1, 2, 2, 2, 5, 4, 9, 3, 2, 1, 5, 2, 1, 2, 3, 7, 5, 3, 1, 4, 6, 2, 2, 3, 6, 4, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "print map(len, QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "N = max(map(len, QUERIES)) # = max query length\n",
    "print N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_query(q, SZ=N):\n",
    "    return q + [PARA_PLACEHOLDER]*(SZ-len(q))\n",
    "QUERIES = map(pad_query, QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37707/37707 [11:43<00:00, 53.61it/s]\n"
     ]
    }
   ],
   "source": [
    "pmcid_2relevance = [{} for i in xrange(31)] # list of dict mapping pmcid to relevance\n",
    "with open('data/qrels.txt') as f:\n",
    "    for line in tqdm(f, total=37707): \n",
    "        topicid, _, pmcid, relevance = line.split()\n",
    "        try:\n",
    "            if len( get_article_paragraphs(pmcid) )==0: continue\n",
    "            topicid = int(topicid)\n",
    "            pmcid_2relevance[topicid][pmcid] = int(relevance)\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model: no gating this time ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, InputLayer, Flatten, Input, Merge, merge, Reshape\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_forward = Sequential(\n",
    "    [Dense(input_dim=30, output_dim=10, activation='relu', bias=False),\n",
    "     Dense(output_dim=5, activation='relu', bias=False),\n",
    "     Dense(output_dim=1, activation='tanh', bias=False)], \n",
    "    name='feed_forward_nw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ***note: have to wrap ops into Lambda layers !!***\n",
    "# cf: https://groups.google.com/forum/#!topic/keras-users/fbRS-FkZw_Q\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "# input: hist vectors\n",
    "input_hists = Input(shape=(N,30), name='input_hists')\n",
    "\n",
    "def slicei(x, i): return x[:,i,:]\n",
    "def slicei_output_shape(input_shape): return (input_shape[0], input_shape[2])\n",
    "zs = [ feed_forward( Lambda(lambda x:slicei(x,i), slicei_output_shape, name='slice%d'%i)(input_hists) )\\\n",
    "          for i in xrange(N) ]\n",
    "\n",
    "def concat(x): return K.concatenate(x) \n",
    "def concat_output_shape(input_shape): return (input_shape[0][0], N)\n",
    "zs = Lambda(concat, concat_output_shape, name='concat_zs')(zs)\n",
    "\n",
    "\n",
    "def mean(x): return K.mean(x)\n",
    "def mean_output_shape(input_shape): return (input_shape[0],1)\n",
    "scores = Lambda(mean, mean_output_shape, name='mean')(zs)\n",
    "\n",
    "scoring_model = Model(input=input_hists, output=scores, name='scoring_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# third input -- the negative hists vector \n",
    "input_hists_neg = Input(shape=(N,30), name='input_hists_neg')\n",
    "\n",
    "zs_neg = [ feed_forward( Lambda(lambda x:slicei(x,i), slicei_output_shape, name='slice%d_neg'%i)(input_hists_neg) )\\\n",
    "          for i in xrange(N) ]\n",
    "\n",
    "zs_neg = Lambda(concat, concat_output_shape, name='concat_zs_neg')(zs_neg)\n",
    "\n",
    "scores_neg =  Lambda(mean, mean_output_shape, name='mean_neg')(zs_neg)\n",
    "\n",
    "two_score_model = Model(input=[input_hists, input_hists_neg], \n",
    "                        output=[scores, scores_neg], \n",
    "                        name='two_score_model')\n",
    "\n",
    "def diff(x): return tf.sub(x[0], x[1]) #x[0]-x[1]\n",
    "def diff_output_shape(input_shape): return input_shape[0]\n",
    "posneg_score_diff = Lambda(diff, diff_output_shape, name='posneg_score_diff')([scores, scores_neg])\n",
    "ranking_model = Model(input=[input_hists,  input_hists_neg]\n",
    "                      , output=[posneg_score_diff]\n",
    "                      , name='ranking_model')\n",
    "\n",
    "# define my loss function: hinge of score_pos - score_neg\n",
    "def pairwise_hinge(y_true, y_pred): # y_pred = score_pos - score_neg, **y_true doesn't matter here**\n",
    "    return K.mean( K.maximum(1. - y_pred, y_true*0.0) )  \n",
    "\n",
    "# self-defined metrics\n",
    "def ranking_acc(y_true, y_pred):\n",
    "    y_pred = y_pred > 0 \n",
    "    return K.mean(y_pred)\n",
    "\n",
    "ranking_model.compile(optimizer='adagrad', loss=pairwise_hinge, metrics=[ranking_acc])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
