{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** In this notebook: run the model with the last label (\"others\") removed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cPickle as pk\n",
    "np.random.seed(1) # to be reproductive\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import Conv2D, MaxPooling2D, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "NOTE_DATA_DIR = '/local/XW/DATA/MIMIC/noteevents_by_sid/'\n",
    "ICD_FPATH = 'data/subject_diag_icds.txt'\n",
    "PK_FPATH = 'data/diag_processed_data.pk' # './processed_data_small.pk'\n",
    "MODEL_PATH = './models/'\n",
    "LOG_PATH = './logs/'\n",
    "# constants\n",
    "N_LABELS = 49 # *** <-- remove last \"others\" label ***\n",
    "K_ICDS_TOKEEP = N_LABELS - 1 # predict only on top K frequent icd codes\n",
    "N_SUBJECTS = 41886\n",
    "# word2vec configurations\n",
    "GLOVE_DIR = '/local/XW/DATA/glove.6B/'\n",
    "MAX_SEQ_LEN = 1000 # max length of input sequence (pad/truncate to fix length)\n",
    "MAX_NB_WORDS = 20000 # top 20k most freq words\n",
    "EMBEDDING_DIM = 100\n",
    "# learning configurations\n",
    "VALIDATION_SPLIT = 0.2\n",
    "N_EPOCHS = 20\n",
    "SZ_BATCH = 512 # large batch size ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load pickled data\n",
    "pk_data = pk.load(open(PK_FPATH, 'rb'))\n",
    "embedding_matrix = pk_data['embedding_matrix']\n",
    "X_train, Y_train = pk_data['X_train'], pk_data['Y_train']\n",
    "X_val, Y_val = pk_data['X_val'], pk_data['Y_val']\n",
    "nb_words = MAX_NB_WORDS # forgot to pickle this number..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n",
      "53\n",
      "(36916, 1000) (36916, 49)\n"
     ]
    }
   ],
   "source": [
    "# found one row that is ALL 0) (strange?)\n",
    "print np.min( np.sum(Y_train, axis=1) ), np.min( np.sum(Y_val, axis=1) )\n",
    "print np.argmin( np.sum(Y_train, axis=1) )\n",
    "Y_train[11730]\n",
    "Y_train = np.delete(Y_train, 11730, axis=0)\n",
    "X_train = np.delete(X_train, 11730, axis=0)\n",
    "print X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36917, 49) (9229, 49)\n"
     ]
    }
   ],
   "source": [
    "# *** remove last column of Y_train and Y_val ***\n",
    "Y_train = Y_train[:,:-1]\n",
    "Y_val = Y_val[:, :-1]\n",
    "print Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** this metrics is the continus relaxation of what we really want, so the acc output during training is not precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multlabel_prec(y_true, y_pred):\n",
    "    y_pred = K.round(K.clip(y_pred, 0, 1)) # turn to 0/1 \n",
    "    tp = K.sum(y_true * y_pred, axis =-1)\n",
    "    sum_true = K.sum(y_true, axis=-1)\n",
    "    sum_pred = K.sum(y_pred, axis=-1)\n",
    "    return K.mean(tp/(sum_pred+1e-10)) # to avoid NaN precision\n",
    "    \n",
    "def multlabel_recall(y_true, y_pred):\n",
    "    y_pred = K.round(K.clip(y_pred, 0, 1)) # turn to 0/1 \n",
    "    tp = K.sum(y_true * y_pred, axis =-1)\n",
    "    sum_true = K.sum(y_true, axis=-1)\n",
    "    sum_pred = K.sum(y_pred, axis=-1)\n",
    "    return K.mean(tp/(sum_true+1e-10)) \n",
    "\n",
    "def multlabel_F1(y_true, y_pred):\n",
    "    y_pred = K.round(K.clip(y_pred, 0, 1)) # turn to 0/1 \n",
    "    tp = K.sum(y_true * y_pred, axis =-1)\n",
    "    sum_true = K.sum(y_true, axis=-1)\n",
    "    sum_pred = K.sum(y_pred, axis=-1)\n",
    "    return 2*K.mean(tp/(sum_true+sum_pred+1e-10))\n",
    "\n",
    "def multlabel_acc(y_true, y_pred):\n",
    "    y_pred = K.round(K.clip(y_pred, 0, 1)) # turn to 0/1 \n",
    "    intersect = y_true * y_pred\n",
    "    intersect = K.sum(intersect, axis=-1)\n",
    "    union = K.clip(y_true+y_pred, 0, 1)\n",
    "    union = K.sum(union, axis=-1)\n",
    "    return K.mean(intersect/(union+1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    print 'evaluation on training set:'\n",
    "    print model.evaluate(X_train, Y_train, batch_size=128)\n",
    "    print 'evaluation on validation set:'\n",
    "    print model.evaluate(X_val, Y_val, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wraps up operations on models\n",
    "def compile_fit_evaluate(model, quick_test=False, print_summary=True,\n",
    "                         save_log=True, save_model=True, del_model=False):\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=[multlabel_prec, multlabel_recall, multlabel_F1, multlabel_acc])\n",
    "    if print_summary:\n",
    "        print model.summary()\n",
    "        \n",
    "    if quick_test: # use tiny data for quick test\n",
    "        print '(quick test mode)'\n",
    "        model.fit(X_train[:100], Y_train[:100], nb_epoch=1)\n",
    "        return  \n",
    "    \n",
    "    _callbacks = [EarlyStopping(monitor='val_loss', patience=2)] #[RelaxAccHistory()]\n",
    "    if save_log:\n",
    "        logdir = os.path.join( LOG_PATH, time.strftime('%m%d')+'_'+str(model.name) )\n",
    "        if not os.path.exists(logdir):\n",
    "            os.makedirs(logdir)\n",
    "        _callbacks.append(TensorBoard(log_dir=logdir))\n",
    "        print 'run \"tensorboard --logdir=%s\" to launch tensorboard'%logdir\n",
    "    \n",
    "    model.fit( X_train, Y_train, \n",
    "              validation_data=(X_val, Y_val),\n",
    "              nb_epoch=N_EPOCHS, batch_size=SZ_BATCH,\n",
    "              callbacks=_callbacks )\n",
    "    \n",
    "    print 'evaluating model...'\n",
    "    evaluate_model(model)\n",
    "    \n",
    "    if save_model: \n",
    "        model_fpath = os.path.join( MODEL_PATH, '%s.h5'% str(model.name) )\n",
    "        model.save(model_fpath)\n",
    "    \n",
    "    if del_model:\n",
    "        del model # delete the model to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ''' ***NOTE***\n",
    "# To load models from file, we have to modify metrics.py at: \n",
    "# `/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras` \n",
    "# to add the `multlabel_XXX` function, otherwise throws exception ! \n",
    "\n",
    "# cf issue: https://github.com/fchollet/keras/issues/3911\n",
    "# '''\n",
    "# m = load_model(os.path.sep.join([MODEL_PATH, 'model_1conv1d.h5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flag_quick_test = 0 # set to False/0 to run on whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_15 (Embedding)         (None, 1000, 100)     0           embedding_input_15[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_24 (Convolution1D) (None, 996, 128)      64128       embedding_15[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_24 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_24[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)             (None, 25472)         0           maxpooling1d_24[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 25472)         0           flatten_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 49)            1248177     dropout_18[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1312305\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1101_model_1conv1d_dropout\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 438s - loss: 0.2801 - multlabel_prec: 0.2339 - multlabel_recall: 0.1340 - multlabel_F1: 0.1484 - multlabel_acc: 0.1135 - val_loss: 0.2511 - val_multlabel_prec: 0.3154 - val_multlabel_recall: 0.1678 - val_multlabel_F1: 0.1964 - val_multlabel_acc: 0.1476\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 447s - loss: 0.2335 - multlabel_prec: 0.3105 - multlabel_recall: 0.1825 - multlabel_F1: 0.2048 - multlabel_acc: 0.1570 - val_loss: 0.2424 - val_multlabel_prec: 0.2455 - val_multlabel_recall: 0.1574 - val_multlabel_F1: 0.1703 - val_multlabel_acc: 0.1325\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 450s - loss: 0.2225 - multlabel_prec: 0.3549 - multlabel_recall: 0.2059 - multlabel_F1: 0.2331 - multlabel_acc: 0.1790 - val_loss: 0.2393 - val_multlabel_prec: 0.3405 - val_multlabel_recall: 0.1861 - val_multlabel_F1: 0.2159 - val_multlabel_acc: 0.1626\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 490s - loss: 0.2154 - multlabel_prec: 0.3911 - multlabel_recall: 0.2244 - multlabel_F1: 0.2556 - multlabel_acc: 0.1963 - val_loss: 0.2454 - val_multlabel_prec: 0.3402 - val_multlabel_recall: 0.1870 - val_multlabel_F1: 0.2163 - val_multlabel_acc: 0.1642\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 458s - loss: 0.2105 - multlabel_prec: 0.4173 - multlabel_recall: 0.2418 - multlabel_F1: 0.2752 - multlabel_acc: 0.2120 - val_loss: 0.2416 - val_multlabel_prec: 0.3372 - val_multlabel_recall: 0.2072 - val_multlabel_F1: 0.2292 - val_multlabel_acc: 0.1733\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 432s - loss: 0.2069 - multlabel_prec: 0.4391 - multlabel_recall: 0.2572 - multlabel_F1: 0.2922 - multlabel_acc: 0.2260 - val_loss: 0.2459 - val_multlabel_prec: 0.3694 - val_multlabel_recall: 0.2287 - val_multlabel_F1: 0.2542 - val_multlabel_acc: 0.1898\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 203s   \n",
      "[0.19823942737998368, 0.49777800945254497, 0.29465588894008182, 0.33788584380524944, 0.26231076261025621]\n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 49s    \n",
      "[0.24592598417957706, 0.36941693674279313, 0.22872575368824649, 0.25419823673183112, 0.18984565090298045]\n"
     ]
    }
   ],
   "source": [
    "model_1conv1d_dropout = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQ_LEN, trainable=False # keep the embeddings fixed\n",
    "             ),# embedding layer\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dropout(p=0.2),\n",
    "            Dense(N_LABELS, activation='sigmoid') \n",
    "        ], \n",
    "        name='model_1conv1d_dropout')\n",
    "model_1conv1d_dropout.fit()\n",
    "compile_fit_evaluate(model_1conv1d_dropout, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_16 (Embedding)         (None, 1000, 100)     0           embedding_input_16[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_25 (Convolution1D) (None, 996, 128)      64128       embedding_16[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_25 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_25[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_26 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_25[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_26 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)             (None, 4992)          0           maxpooling1d_26[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)             (None, 4992)          0           flatten_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 49)            244657      dropout_19[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 390833\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1101_model_2conv1d_dropout\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 547s - loss: 0.2649 - multlabel_prec: 0.2214 - multlabel_recall: 0.1291 - multlabel_F1: 0.1416 - multlabel_acc: 0.1087 - val_loss: 0.2418 - val_multlabel_prec: 0.2698 - val_multlabel_recall: 0.1534 - val_multlabel_F1: 0.1750 - val_multlabel_acc: 0.1342\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 542s - loss: 0.2352 - multlabel_prec: 0.2935 - multlabel_recall: 0.1790 - multlabel_F1: 0.1997 - multlabel_acc: 0.1525 - val_loss: 0.2356 - val_multlabel_prec: 0.2939 - val_multlabel_recall: 0.1805 - val_multlabel_F1: 0.2021 - val_multlabel_acc: 0.1547\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 573s - loss: 0.2280 - multlabel_prec: 0.3332 - multlabel_recall: 0.1963 - multlabel_F1: 0.2224 - multlabel_acc: 0.1689 - val_loss: 0.2304 - val_multlabel_prec: 0.3767 - val_multlabel_recall: 0.2187 - val_multlabel_F1: 0.2495 - val_multlabel_acc: 0.1884\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 582s - loss: 0.2234 - multlabel_prec: 0.3599 - multlabel_recall: 0.2087 - multlabel_F1: 0.2387 - multlabel_acc: 0.1811 - val_loss: 0.2284 - val_multlabel_prec: 0.3208 - val_multlabel_recall: 0.1763 - val_multlabel_F1: 0.2060 - val_multlabel_acc: 0.1563\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 581s - loss: 0.2197 - multlabel_prec: 0.3797 - multlabel_recall: 0.2168 - multlabel_F1: 0.2494 - multlabel_acc: 0.1893 - val_loss: 0.2275 - val_multlabel_prec: 0.3927 - val_multlabel_recall: 0.2214 - val_multlabel_F1: 0.2584 - val_multlabel_acc: 0.1940\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 569s - loss: 0.2168 - multlabel_prec: 0.3964 - multlabel_recall: 0.2240 - multlabel_F1: 0.2590 - multlabel_acc: 0.1967 - val_loss: 0.2270 - val_multlabel_prec: 0.3326 - val_multlabel_recall: 0.1896 - val_multlabel_F1: 0.2165 - val_multlabel_acc: 0.1652\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 644s - loss: 0.2141 - multlabel_prec: 0.4087 - multlabel_recall: 0.2301 - multlabel_F1: 0.2667 - multlabel_acc: 0.2025 - val_loss: 0.2273 - val_multlabel_prec: 0.3477 - val_multlabel_recall: 0.1981 - val_multlabel_F1: 0.2266 - val_multlabel_acc: 0.1724\n",
      "Epoch 8/20\n",
      "36916/36916 [==============================] - 674s - loss: 0.2118 - multlabel_prec: 0.4160 - multlabel_recall: 0.2336 - multlabel_F1: 0.2708 - multlabel_acc: 0.2058 - val_loss: 0.2267 - val_multlabel_prec: 0.3837 - val_multlabel_recall: 0.2219 - val_multlabel_F1: 0.2524 - val_multlabel_acc: 0.1903\n",
      "Epoch 9/20\n",
      "36916/36916 [==============================] - 687s - loss: 0.2099 - multlabel_prec: 0.4239 - multlabel_recall: 0.2389 - multlabel_F1: 0.2768 - multlabel_acc: 0.2106 - val_loss: 0.2268 - val_multlabel_prec: 0.3917 - val_multlabel_recall: 0.2314 - val_multlabel_F1: 0.2629 - val_multlabel_acc: 0.1983\n",
      "Epoch 10/20\n",
      "36916/36916 [==============================] - 683s - loss: 0.2079 - multlabel_prec: 0.4293 - multlabel_recall: 0.2445 - multlabel_F1: 0.2826 - multlabel_acc: 0.2154 - val_loss: 0.2264 - val_multlabel_prec: 0.4127 - val_multlabel_recall: 0.2373 - val_multlabel_F1: 0.2739 - val_multlabel_acc: 0.2053\n",
      "Epoch 11/20\n",
      "36916/36916 [==============================] - 698s - loss: 0.2064 - multlabel_prec: 0.4361 - multlabel_recall: 0.2486 - multlabel_F1: 0.2874 - multlabel_acc: 0.2196 - val_loss: 0.2287 - val_multlabel_prec: 0.4030 - val_multlabel_recall: 0.2368 - val_multlabel_F1: 0.2711 - val_multlabel_acc: 0.2032\n",
      "Epoch 12/20\n",
      "36916/36916 [==============================] - 695s - loss: 0.2051 - multlabel_prec: 0.4421 - multlabel_recall: 0.2512 - multlabel_F1: 0.2910 - multlabel_acc: 0.2220 - val_loss: 0.2299 - val_multlabel_prec: 0.3733 - val_multlabel_recall: 0.2128 - val_multlabel_F1: 0.2418 - val_multlabel_acc: 0.1826\n",
      "Epoch 13/20\n",
      "36916/36916 [==============================] - 709s - loss: 0.2038 - multlabel_prec: 0.4487 - multlabel_recall: 0.2561 - multlabel_F1: 0.2963 - multlabel_acc: 0.2267 - val_loss: 0.2330 - val_multlabel_prec: 0.3891 - val_multlabel_recall: 0.2059 - val_multlabel_F1: 0.2427 - val_multlabel_acc: 0.1835\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 293s   \n",
      "[0.19762305993986795, 0.45047947127597665, 0.23055556264416033, 0.27751910893484888, 0.21254996484738284]\n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 73s    \n",
      "[0.23300173949873684, 0.389126790221831, 0.20589570104203953, 0.24265298864438198, 0.18351473699076901]\n"
     ]
    }
   ],
   "source": [
    "# 2 conv1d layers\n",
    "model_2conv1d_dropout = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dropout(p=0.5),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_2conv1d_dropout')\n",
    "compile_fit_evaluate(model_2conv1d_dropout, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_17 (Embedding)         (None, 1000, 100)     0           embedding_input_17[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_27 (Convolution1D) (None, 996, 128)      64128       embedding_17[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_27 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_27[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_28 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_27[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_28 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_28[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_29 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_28[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_29 (MaxPooling1D)   (None, 7, 128)        0           convolution1d_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)             (None, 896)           0           maxpooling1d_29[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)             (None, 896)           0           flatten_17[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 49)            43953       dropout_20[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 272177\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1101_model_3conv1d_dropout\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 735s - loss: 0.2728 - multlabel_prec: 0.2119 - multlabel_recall: 0.1129 - multlabel_F1: 0.1254 - multlabel_acc: 0.0948 - val_loss: 0.2492 - val_multlabel_prec: 0.1432 - val_multlabel_recall: 0.1236 - val_multlabel_F1: 0.1220 - val_multlabel_acc: 0.0986\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 738s - loss: 0.2421 - multlabel_prec: 0.2729 - multlabel_recall: 0.1659 - multlabel_F1: 0.1846 - multlabel_acc: 0.1411 - val_loss: 0.2387 - val_multlabel_prec: 0.2646 - val_multlabel_recall: 0.1681 - val_multlabel_F1: 0.1846 - val_multlabel_acc: 0.1429\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 736s - loss: 0.2334 - multlabel_prec: 0.3121 - multlabel_recall: 0.1881 - multlabel_F1: 0.2118 - multlabel_acc: 0.1609 - val_loss: 0.2388 - val_multlabel_prec: 0.2511 - val_multlabel_recall: 0.1596 - val_multlabel_F1: 0.1758 - val_multlabel_acc: 0.1371\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 739s - loss: 0.2284 - multlabel_prec: 0.3460 - multlabel_recall: 0.2023 - multlabel_F1: 0.2309 - multlabel_acc: 0.1747 - val_loss: 0.2283 - val_multlabel_prec: 0.3456 - val_multlabel_recall: 0.2050 - val_multlabel_F1: 0.2332 - val_multlabel_acc: 0.1762\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 743s - loss: 0.2247 - multlabel_prec: 0.3671 - multlabel_recall: 0.2113 - multlabel_F1: 0.2426 - multlabel_acc: 0.1830 - val_loss: 0.2328 - val_multlabel_prec: 0.2836 - val_multlabel_recall: 0.1650 - val_multlabel_F1: 0.1864 - val_multlabel_acc: 0.1436\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 729s - loss: 0.2213 - multlabel_prec: 0.3876 - multlabel_recall: 0.2194 - multlabel_F1: 0.2539 - multlabel_acc: 0.1914 - val_loss: 0.2329 - val_multlabel_prec: 0.3440 - val_multlabel_recall: 0.1855 - val_multlabel_F1: 0.2178 - val_multlabel_acc: 0.1638\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 737s - loss: 0.2186 - multlabel_prec: 0.3999 - multlabel_recall: 0.2248 - multlabel_F1: 0.2611 - multlabel_acc: 0.1967 - val_loss: 0.2275 - val_multlabel_prec: 0.3460 - val_multlabel_recall: 0.2056 - val_multlabel_F1: 0.2319 - val_multlabel_acc: 0.1764\n",
      "Epoch 8/20\n",
      "36916/36916 [==============================] - 737s - loss: 0.2158 - multlabel_prec: 0.4131 - multlabel_recall: 0.2306 - multlabel_F1: 0.2686 - multlabel_acc: 0.2023 - val_loss: 0.2278 - val_multlabel_prec: 0.3914 - val_multlabel_recall: 0.2181 - val_multlabel_F1: 0.2532 - val_multlabel_acc: 0.1897\n",
      "Epoch 9/20\n",
      "36916/36916 [==============================] - 733s - loss: 0.2130 - multlabel_prec: 0.4241 - multlabel_recall: 0.2348 - multlabel_F1: 0.2741 - multlabel_acc: 0.2063 - val_loss: 0.2260 - val_multlabel_prec: 0.3763 - val_multlabel_recall: 0.2041 - val_multlabel_F1: 0.2394 - val_multlabel_acc: 0.1807\n",
      "Epoch 10/20\n",
      "36916/36916 [==============================] - 740s - loss: 0.2106 - multlabel_prec: 0.4311 - multlabel_recall: 0.2380 - multlabel_F1: 0.2788 - multlabel_acc: 0.2099 - val_loss: 0.2307 - val_multlabel_prec: 0.4120 - val_multlabel_recall: 0.2343 - val_multlabel_F1: 0.2717 - val_multlabel_acc: 0.2040\n",
      "Epoch 11/20\n",
      "36916/36916 [==============================] - 736s - loss: 0.2082 - multlabel_prec: 0.4406 - multlabel_recall: 0.2435 - multlabel_F1: 0.2852 - multlabel_acc: 0.2149 - val_loss: 0.2257 - val_multlabel_prec: 0.4097 - val_multlabel_recall: 0.2306 - val_multlabel_F1: 0.2661 - val_multlabel_acc: 0.2004\n",
      "Epoch 12/20\n",
      "36916/36916 [==============================] - 732s - loss: 0.2058 - multlabel_prec: 0.4491 - multlabel_recall: 0.2484 - multlabel_F1: 0.2912 - multlabel_acc: 0.2195 - val_loss: 0.2281 - val_multlabel_prec: 0.3918 - val_multlabel_recall: 0.2324 - val_multlabel_F1: 0.2633 - val_multlabel_acc: 0.2001\n",
      "Epoch 13/20\n",
      "36916/36916 [==============================] - 733s - loss: 0.2037 - multlabel_prec: 0.4572 - multlabel_recall: 0.2533 - multlabel_F1: 0.2969 - multlabel_acc: 0.2238 - val_loss: 0.2307 - val_multlabel_prec: 0.3950 - val_multlabel_recall: 0.2330 - val_multlabel_F1: 0.2650 - val_multlabel_acc: 0.1992\n",
      "Epoch 14/20\n",
      "36916/36916 [==============================] - 736s - loss: 0.2015 - multlabel_prec: 0.4618 - multlabel_recall: 0.2587 - multlabel_F1: 0.3027 - multlabel_acc: 0.2286 - val_loss: 0.2372 - val_multlabel_prec: 0.3908 - val_multlabel_recall: 0.2109 - val_multlabel_F1: 0.2442 - val_multlabel_acc: 0.1858\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 285s   \n",
      "[0.19707995986567764, 0.45254173829000349, 0.23257847140147686, 0.27522241291329175, 0.20974838281261635]\n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 66s    \n",
      "[0.23721943600475626, 0.39082215483539451, 0.21087668908370227, 0.24423596829414909, 0.18581785994095176]\n"
     ]
    }
   ],
   "source": [
    "model_3conv1d_dropout =Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dropout(p=0.5),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_3conv1d_dropout')\n",
    "\n",
    "compile_fit_evaluate(model_3conv1d_dropout, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_18 (Embedding)         (None, 1000, 100)     0           embedding_input_18[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_30 (Convolution1D) (None, 996, 128)      64128       embedding_18[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_30 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_31 (Convolution1D) (None, 195, 64)       41024       maxpooling1d_30[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_31 (MaxPooling1D)   (None, 65, 64)        0           convolution1d_31[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_32 (Convolution1D) (None, 61, 32)        10272       maxpooling1d_31[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_32 (MaxPooling1D)   (None, 30, 32)        0           convolution1d_32[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)             (None, 960)           0           maxpooling1d_32[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)             (None, 960)           0           flatten_18[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 500)           480500      dropout_21[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)             (None, 500)           0           dense_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 49)            24549       dropout_22[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 620473\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1101_model_3conv1d_2FC\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 590s - loss: 0.2809 - multlabel_prec: 0.2073 - multlabel_recall: 0.1104 - multlabel_F1: 0.1230 - multlabel_acc: 0.0928 - val_loss: 0.2519 - val_multlabel_prec: 0.2227 - val_multlabel_recall: 0.1537 - val_multlabel_F1: 0.1570 - val_multlabel_acc: 0.1227\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 591s - loss: 0.2446 - multlabel_prec: 0.2761 - multlabel_recall: 0.1608 - multlabel_F1: 0.1803 - multlabel_acc: 0.1376 - val_loss: 0.2531 - val_multlabel_prec: 0.2012 - val_multlabel_recall: 0.1426 - val_multlabel_F1: 0.1517 - val_multlabel_acc: 0.1208\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 587s - loss: 0.2371 - multlabel_prec: 0.3007 - multlabel_recall: 0.1768 - multlabel_F1: 0.2001 - multlabel_acc: 0.1523 - val_loss: 0.2412 - val_multlabel_prec: 0.3236 - val_multlabel_recall: 0.2037 - val_multlabel_F1: 0.2264 - val_multlabel_acc: 0.1700\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 587s - loss: 0.2322 - multlabel_prec: 0.3334 - multlabel_recall: 0.1930 - multlabel_F1: 0.2210 - multlabel_acc: 0.1668 - val_loss: 0.2358 - val_multlabel_prec: 0.2811 - val_multlabel_recall: 0.1707 - val_multlabel_F1: 0.1895 - val_multlabel_acc: 0.1459\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 587s - loss: 0.2292 - multlabel_prec: 0.3506 - multlabel_recall: 0.2010 - multlabel_F1: 0.2314 - multlabel_acc: 0.1741 - val_loss: 0.2294 - val_multlabel_prec: 0.3874 - val_multlabel_recall: 0.2163 - val_multlabel_F1: 0.2530 - val_multlabel_acc: 0.1884\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 592s - loss: 0.2266 - multlabel_prec: 0.3666 - multlabel_recall: 0.2077 - multlabel_F1: 0.2399 - multlabel_acc: 0.1800 - val_loss: 0.2314 - val_multlabel_prec: 0.3969 - val_multlabel_recall: 0.2357 - val_multlabel_F1: 0.2689 - val_multlabel_acc: 0.2003\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 587s - loss: 0.2239 - multlabel_prec: 0.3805 - multlabel_recall: 0.2143 - multlabel_F1: 0.2486 - multlabel_acc: 0.1866 - val_loss: 0.2274 - val_multlabel_prec: 0.3664 - val_multlabel_recall: 0.2192 - val_multlabel_F1: 0.2467 - val_multlabel_acc: 0.1868\n",
      "Epoch 8/20\n",
      "36916/36916 [==============================] - 586s - loss: 0.2221 - multlabel_prec: 0.3953 - multlabel_recall: 0.2203 - multlabel_F1: 0.2563 - multlabel_acc: 0.1919 - val_loss: 0.2252 - val_multlabel_prec: 0.4128 - val_multlabel_recall: 0.2419 - val_multlabel_F1: 0.2758 - val_multlabel_acc: 0.2064\n",
      "Epoch 9/20\n",
      "36916/36916 [==============================] - 586s - loss: 0.2206 - multlabel_prec: 0.4007 - multlabel_recall: 0.2237 - multlabel_F1: 0.2608 - multlabel_acc: 0.1956 - val_loss: 0.2265 - val_multlabel_prec: 0.3992 - val_multlabel_recall: 0.2410 - val_multlabel_F1: 0.2731 - val_multlabel_acc: 0.2047\n",
      "Epoch 10/20\n",
      "36916/36916 [==============================] - 587s - loss: 0.2188 - multlabel_prec: 0.4100 - multlabel_recall: 0.2283 - multlabel_F1: 0.2668 - multlabel_acc: 0.1996 - val_loss: 0.2345 - val_multlabel_prec: 0.3438 - val_multlabel_recall: 0.2088 - val_multlabel_F1: 0.2351 - val_multlabel_acc: 0.1786\n",
      "Epoch 11/20\n",
      "36916/36916 [==============================] - 594s - loss: 0.2174 - multlabel_prec: 0.4161 - multlabel_recall: 0.2316 - multlabel_F1: 0.2706 - multlabel_acc: 0.2027 - val_loss: 0.2279 - val_multlabel_prec: 0.4025 - val_multlabel_recall: 0.2301 - val_multlabel_F1: 0.2660 - val_multlabel_acc: 0.1994\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 248s   \n",
      "[0.21056985385899218, 0.43851502142343635, 0.24424333404275342, 0.28610821702663991, 0.21473332454535901]\n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 61s    \n",
      "[0.2279126420108504, 0.40252749849042624, 0.23011055672035688, 0.26603338040346081, 0.19944122431805622]\n"
     ]
    }
   ],
   "source": [
    "model_3conv1d_2FC =Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(64, 5, activation='relu'),\n",
    "            MaxPooling1D(3),\n",
    "            Conv1D(32, 5, activation='relu'),\n",
    "            MaxPooling1D(2),\n",
    "            Flatten(),\n",
    "            Dropout(p=0.5),\n",
    "            Dense(500, activation='relu'),\n",
    "            Dropout(p=0.5),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_3conv1d_2FC')\n",
    "\n",
    "compile_fit_evaluate(model_3conv1d_2FC, flag_quick_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:thesis_nb]",
   "language": "python",
   "name": "conda-env-thesis_nb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
