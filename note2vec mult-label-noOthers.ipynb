{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** In this notebook: run the model with the last label (\"others\") removed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cPickle as pk\n",
    "np.random.seed(1) # to be reproductive\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import Conv2D, MaxPooling2D, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "NOTE_DATA_DIR = '/local/XW/DATA/MIMIC/noteevents_by_sid/'\n",
    "ICD_FPATH = 'data/subject_diag_icds.txt'\n",
    "PK_FPATH = 'data/diag_processed_data.pk' # './processed_data_small.pk'\n",
    "MODEL_PATH = './models/'\n",
    "LOG_PATH = './logs/'\n",
    "# constants\n",
    "N_LABELS = 49 # *** <-- remove last \"others\" label ***\n",
    "K_ICDS_TOKEEP = N_LABELS - 1 # predict only on top K frequent icd codes\n",
    "N_SUBJECTS = 41886\n",
    "# word2vec configurations\n",
    "GLOVE_DIR = '/local/XW/DATA/glove.6B/'\n",
    "MAX_SEQ_LEN = 1000 # max length of input sequence (pad/truncate to fix length)\n",
    "MAX_NB_WORDS = 20000 # top 20k most freq words\n",
    "EMBEDDING_DIM = 100\n",
    "# learning configurations\n",
    "VALIDATION_SPLIT = 0.2\n",
    "N_EPOCHS = 20\n",
    "SZ_BATCH = 512 # large batch size ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load pickled data\n",
    "pk_data = pk.load(open(PK_FPATH, 'rb'))\n",
    "embedding_matrix = pk_data['embedding_matrix']\n",
    "X_train, Y_train = pk_data['X_train'], pk_data['Y_train']\n",
    "X_val, Y_val = pk_data['X_val'], pk_data['Y_val']\n",
    "nb_words = MAX_NB_WORDS # forgot to pickle this number..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n",
      "53\n",
      "(36916, 1000) (36916, 49)\n"
     ]
    }
   ],
   "source": [
    "# found one row that is ALL 0) (strange?)\n",
    "print np.min( np.sum(Y_train, axis=1) ), np.min( np.sum(Y_val, axis=1) )\n",
    "print np.argmin( np.sum(Y_train, axis=1) )\n",
    "Y_train[11730]\n",
    "Y_train = np.delete(Y_train, 11730, axis=0)\n",
    "X_train = np.delete(X_train, 11730, axis=0)\n",
    "print X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36917, 49) (9229, 49)\n"
     ]
    }
   ],
   "source": [
    "# *** remove last column of Y_train and Y_val ***\n",
    "Y_train = Y_train[:,:-1]\n",
    "Y_val = Y_val[:, :-1]\n",
    "print Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_n_poslabels = Y_train.sum(axis=1) # this can be used as sample weights: more sample the ones with more 1s..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** this metrics is the continus relaxation of what we really want, so the acc output during training is not precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multlabel_prec(y_true, y_pred):\n",
    "    y_pred = K.round(K.clip(y_pred, 0, 1)) # turn to 0/1 \n",
    "    tp = K.sum(y_true * y_pred, axis =-1)\n",
    "    sum_true = K.sum(y_true, axis=-1)\n",
    "    sum_pred = K.sum(y_pred, axis=-1)\n",
    "    return K.mean(tp/(sum_pred+1e-10)) # to avoid NaN precision\n",
    "    \n",
    "def multlabel_recall(y_true, y_pred):\n",
    "    y_pred = K.round(K.clip(y_pred, 0, 1)) # turn to 0/1 \n",
    "    tp = K.sum(y_true * y_pred, axis =-1)\n",
    "    sum_true = K.sum(y_true, axis=-1)\n",
    "    sum_pred = K.sum(y_pred, axis=-1)\n",
    "    return K.mean(tp/(sum_true+1e-10)) \n",
    "\n",
    "def multlabel_F1(y_true, y_pred):\n",
    "    y_pred = K.round(K.clip(y_pred, 0, 1)) # turn to 0/1 \n",
    "    tp = K.sum(y_true * y_pred, axis =-1)\n",
    "    sum_true = K.sum(y_true, axis=-1)\n",
    "    sum_pred = K.sum(y_pred, axis=-1)\n",
    "    return 2*K.mean(tp/(sum_true+sum_pred+1e-10))\n",
    "\n",
    "def multlabel_acc(y_true, y_pred):\n",
    "    y_pred = K.round(K.clip(y_pred, 0, 1)) # turn to 0/1 \n",
    "    intersect = y_true * y_pred\n",
    "    intersect = K.sum(intersect, axis=-1)\n",
    "    union = K.clip(y_true+y_pred, 0, 1)\n",
    "    union = K.sum(union, axis=-1)\n",
    "    return K.mean(intersect/(union+1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    print 'evaluation on training set:'\n",
    "    print model.evaluate(X_train, Y_train, batch_size=128)\n",
    "    print 'evaluation on validation set:'\n",
    "    print model.evaluate(X_val, Y_val, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wraps up operations on models\n",
    "def compile_fit_evaluate(model, quick_test=False, print_summary=True,\n",
    "                         save_log=True, save_model=True, del_model=False):\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=[multlabel_prec, multlabel_recall, multlabel_F1, multlabel_acc])\n",
    "    if print_summary:\n",
    "        print model.summary()\n",
    "        \n",
    "    if quick_test: # use tiny data for quick test\n",
    "        print '(quick test mode)'\n",
    "        model.fit(X_train[:100], Y_train[:100], nb_epoch=1)\n",
    "        return  \n",
    "    \n",
    "    _callbacks = [EarlyStopping(monitor='val_loss', patience=2)] #[RelaxAccHistory()]\n",
    "    if save_log:\n",
    "        logdir = os.path.join( LOG_PATH, time.strftime('%m%d')+'_'+str(model.name) )\n",
    "        if not os.path.exists(logdir):\n",
    "            os.makedirs(logdir)\n",
    "        _callbacks.append(TensorBoard(log_dir=logdir))\n",
    "        print 'run \"tensorboard --logdir=%s\" to launch tensorboard'%logdir\n",
    "    \n",
    "    model.fit( X_train, Y_train, \n",
    "              validation_data=(X_val, Y_val),\n",
    "              nb_epoch=N_EPOCHS, batch_size=SZ_BATCH,\n",
    "              sample_weight = y_n_poslabels,\n",
    "              callbacks=_callbacks )\n",
    "    \n",
    "    print 'evaluating model...'\n",
    "    evaluate_model(model)\n",
    "    \n",
    "    if save_model: \n",
    "        model_fpath = os.path.join( MODEL_PATH, '%s.h5'% str(model.name) )\n",
    "        model.save(model_fpath)\n",
    "    \n",
    "    if del_model:\n",
    "        del model # delete the model to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ''' ***NOTE***\n",
    "# To load models from file, we have to modify metrics.py at: \n",
    "# `/local/XW/SOFT/anaconda2/envs/thesis_nb/lib/python2.7/site-packages/keras` \n",
    "# to add the `multlabel_XXX` function, otherwise throws exception ! \n",
    "\n",
    "# cf issue: https://github.com/fchollet/keras/issues/3911\n",
    "# '''\n",
    "# m = load_model(os.path.sep.join([MODEL_PATH, 'model_1conv1d.h5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flag_quick_test = 0 # set to False/0 to run on whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_19 (Embedding)         (None, 1000, 100)     0           embedding_input_19[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_33 (Convolution1D) (None, 996, 128)      64128       embedding_19[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_33 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_33[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)             (None, 25472)         0           maxpooling1d_33[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)             (None, 25472)         0           flatten_19[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (None, 49)            1248177     dropout_23[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1312305\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1103_model_1conv1d_dropout\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 511s - loss: 1.8186 - multlabel_prec: 0.3595 - multlabel_recall: 0.1850 - multlabel_F1: 0.2124 - multlabel_acc: 0.1554 - val_loss: 0.2579 - val_multlabel_prec: 0.3930 - val_multlabel_recall: 0.2378 - val_multlabel_F1: 0.2610 - val_multlabel_acc: 0.1930\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 461s - loss: 1.5662 - multlabel_prec: 0.4451 - multlabel_recall: 0.2446 - multlabel_F1: 0.2791 - multlabel_acc: 0.2076 - val_loss: 0.2550 - val_multlabel_prec: 0.3872 - val_multlabel_recall: 0.2204 - val_multlabel_F1: 0.2514 - val_multlabel_acc: 0.1868\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 476s - loss: 1.5019 - multlabel_prec: 0.4652 - multlabel_recall: 0.2625 - multlabel_F1: 0.2985 - multlabel_acc: 0.2235 - val_loss: 0.2482 - val_multlabel_prec: 0.4039 - val_multlabel_recall: 0.2162 - val_multlabel_F1: 0.2484 - val_multlabel_acc: 0.1855\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 478s - loss: 1.4606 - multlabel_prec: 0.4802 - multlabel_recall: 0.2768 - multlabel_F1: 0.3132 - multlabel_acc: 0.2354 - val_loss: 0.2490 - val_multlabel_prec: 0.4248 - val_multlabel_recall: 0.2427 - val_multlabel_F1: 0.2749 - val_multlabel_acc: 0.2048\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 495s - loss: 1.4270 - multlabel_prec: 0.4873 - multlabel_recall: 0.2895 - multlabel_F1: 0.3246 - multlabel_acc: 0.2451 - val_loss: 0.2602 - val_multlabel_prec: 0.4168 - val_multlabel_recall: 0.2698 - val_multlabel_F1: 0.2957 - val_multlabel_acc: 0.2166\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 505s - loss: 1.4024 - multlabel_prec: 0.4991 - multlabel_recall: 0.3037 - multlabel_F1: 0.3376 - multlabel_acc: 0.2560 - val_loss: 0.2528 - val_multlabel_prec: 0.4118 - val_multlabel_recall: 0.2517 - val_multlabel_F1: 0.2744 - val_multlabel_acc: 0.2033\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 216s   \n",
      "[0.20539869192407634, 0.52220584757473476, 0.30881171119485651, 0.34625956053490475, 0.26479378785352325]\n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 53s    \n",
      "[0.2527882983640235, 0.41184643369665747, 0.25169138162149512, 0.27439961816135888, 0.20329877484889727]\n"
     ]
    }
   ],
   "source": [
    "model_1conv1d_dropout = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQ_LEN, trainable=False # keep the embeddings fixed\n",
    "             ),# embedding layer\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dropout(p=0.2),\n",
    "            Dense(N_LABELS, activation='sigmoid') \n",
    "        ], \n",
    "        name='model_1conv1d_dropout')\n",
    "compile_fit_evaluate(model_1conv1d_dropout, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_20 (Embedding)         (None, 1000, 100)     0           embedding_input_20[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_34 (Convolution1D) (None, 996, 128)      64128       embedding_20[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_34 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_34[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_35 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_34[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_35 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_35[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)             (None, 4992)          0           maxpooling1d_35[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)             (None, 4992)          0           flatten_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 49)            244657      dropout_24[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 390833\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1103_model_2conv1d_dropout\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 603s - loss: 1.7353 - multlabel_prec: 0.3518 - multlabel_recall: 0.1809 - multlabel_F1: 0.2061 - multlabel_acc: 0.1514 - val_loss: 0.2438 - val_multlabel_prec: 0.3640 - val_multlabel_recall: 0.1993 - val_multlabel_F1: 0.2240 - val_multlabel_acc: 0.1697\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 602s - loss: 1.6005 - multlabel_prec: 0.4201 - multlabel_recall: 0.2377 - multlabel_F1: 0.2680 - multlabel_acc: 0.1993 - val_loss: 0.2407 - val_multlabel_prec: 0.4520 - val_multlabel_recall: 0.2299 - val_multlabel_F1: 0.2657 - val_multlabel_acc: 0.1989\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 548s - loss: 1.5621 - multlabel_prec: 0.4543 - multlabel_recall: 0.2544 - multlabel_F1: 0.2898 - multlabel_acc: 0.2156 - val_loss: 0.2437 - val_multlabel_prec: 0.4377 - val_multlabel_recall: 0.2500 - val_multlabel_F1: 0.2829 - val_multlabel_acc: 0.2097\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 538s - loss: 1.5354 - multlabel_prec: 0.4697 - multlabel_recall: 0.2615 - multlabel_F1: 0.2994 - multlabel_acc: 0.2230 - val_loss: 0.2405 - val_multlabel_prec: 0.4563 - val_multlabel_recall: 0.2705 - val_multlabel_F1: 0.3065 - val_multlabel_acc: 0.2274\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 521s - loss: 1.5188 - multlabel_prec: 0.4746 - multlabel_recall: 0.2676 - multlabel_F1: 0.3059 - multlabel_acc: 0.2282 - val_loss: 0.2387 - val_multlabel_prec: 0.4577 - val_multlabel_recall: 0.2421 - val_multlabel_F1: 0.2844 - val_multlabel_acc: 0.2114\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 524s - loss: 1.5018 - multlabel_prec: 0.4864 - multlabel_recall: 0.2735 - multlabel_F1: 0.3134 - multlabel_acc: 0.2336 - val_loss: 0.2453 - val_multlabel_prec: 0.4422 - val_multlabel_recall: 0.2665 - val_multlabel_F1: 0.3013 - val_multlabel_acc: 0.2227\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 517s - loss: 1.4891 - multlabel_prec: 0.4876 - multlabel_recall: 0.2764 - multlabel_F1: 0.3159 - multlabel_acc: 0.2354 - val_loss: 0.2477 - val_multlabel_prec: 0.4373 - val_multlabel_recall: 0.2673 - val_multlabel_F1: 0.3004 - val_multlabel_acc: 0.2228\n",
      "Epoch 8/20\n",
      "36916/36916 [==============================] - 516s - loss: 1.4771 - multlabel_prec: 0.4899 - multlabel_recall: 0.2775 - multlabel_F1: 0.3181 - multlabel_acc: 0.2378 - val_loss: 0.2354 - val_multlabel_prec: 0.4754 - val_multlabel_recall: 0.2652 - val_multlabel_F1: 0.3056 - val_multlabel_acc: 0.2284\n",
      "Epoch 9/20\n",
      "36916/36916 [==============================] - 518s - loss: 1.4663 - multlabel_prec: 0.4939 - multlabel_recall: 0.2799 - multlabel_F1: 0.3208 - multlabel_acc: 0.2398 - val_loss: 0.2377 - val_multlabel_prec: 0.4552 - val_multlabel_recall: 0.2636 - val_multlabel_F1: 0.3026 - val_multlabel_acc: 0.2252\n",
      "Epoch 10/20\n",
      "36916/36916 [==============================] - 532s - loss: 1.4567 - multlabel_prec: 0.4968 - multlabel_recall: 0.2848 - multlabel_F1: 0.3250 - multlabel_acc: 0.2430 - val_loss: 0.2365 - val_multlabel_prec: 0.5008 - val_multlabel_recall: 0.2755 - val_multlabel_F1: 0.3217 - val_multlabel_acc: 0.2390\n",
      "Epoch 11/20\n",
      "36916/36916 [==============================] - 548s - loss: 1.4505 - multlabel_prec: 0.4996 - multlabel_recall: 0.2861 - multlabel_F1: 0.3271 - multlabel_acc: 0.2449 - val_loss: 0.2359 - val_multlabel_prec: 0.4779 - val_multlabel_recall: 0.2543 - val_multlabel_F1: 0.2963 - val_multlabel_acc: 0.2218\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 230s   \n",
      "[0.21163908898933748, 0.51721071714517397, 0.26996807328592154, 0.31868042255070528, 0.23965164222067828]\n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 58s    \n",
      "[0.23589472177357002, 0.47787502690260064, 0.25432662652119847, 0.29629603270164795, 0.22177617986032513]\n"
     ]
    }
   ],
   "source": [
    "# 2 conv1d layers\n",
    "model_2conv1d_dropout = Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dropout(p=0.5),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_2conv1d_dropout')\n",
    "compile_fit_evaluate(model_2conv1d_dropout, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_21 (Embedding)         (None, 1000, 100)     0           embedding_input_21[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_36 (Convolution1D) (None, 996, 128)      64128       embedding_21[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_36 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_36[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_37 (Convolution1D) (None, 195, 128)      82048       maxpooling1d_36[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_37 (MaxPooling1D)   (None, 39, 128)       0           convolution1d_37[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_38 (Convolution1D) (None, 35, 128)       82048       maxpooling1d_37[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_38 (MaxPooling1D)   (None, 7, 128)        0           convolution1d_38[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)             (None, 896)           0           maxpooling1d_38[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)             (None, 896)           0           flatten_21[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 49)            43953       dropout_25[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 272177\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1103_model_3conv1d_dropout\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 587s - loss: 1.7743 - multlabel_prec: 0.3433 - multlabel_recall: 0.1629 - multlabel_F1: 0.1884 - multlabel_acc: 0.1360 - val_loss: 0.2540 - val_multlabel_prec: 0.4657 - val_multlabel_recall: 0.2413 - val_multlabel_F1: 0.2778 - val_multlabel_acc: 0.2051\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 605s - loss: 1.6322 - multlabel_prec: 0.4039 - multlabel_recall: 0.2231 - multlabel_F1: 0.2511 - multlabel_acc: 0.1865 - val_loss: 0.2636 - val_multlabel_prec: 0.4110 - val_multlabel_recall: 0.2643 - val_multlabel_F1: 0.2902 - val_multlabel_acc: 0.2120\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 603s - loss: 1.5938 - multlabel_prec: 0.4407 - multlabel_recall: 0.2491 - multlabel_F1: 0.2820 - multlabel_acc: 0.2093 - val_loss: 0.2543 - val_multlabel_prec: 0.3969 - val_multlabel_recall: 0.2225 - val_multlabel_F1: 0.2523 - val_multlabel_acc: 0.1897\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 605s - loss: 1.5691 - multlabel_prec: 0.4578 - multlabel_recall: 0.2598 - multlabel_F1: 0.2952 - multlabel_acc: 0.2198 - val_loss: 0.2468 - val_multlabel_prec: 0.4655 - val_multlabel_recall: 0.2833 - val_multlabel_F1: 0.3169 - val_multlabel_acc: 0.2354\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 608s - loss: 1.5490 - multlabel_prec: 0.4720 - multlabel_recall: 0.2672 - multlabel_F1: 0.3047 - multlabel_acc: 0.2268 - val_loss: 0.2348 - val_multlabel_prec: 0.4298 - val_multlabel_recall: 0.2296 - val_multlabel_F1: 0.2665 - val_multlabel_acc: 0.2003\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 612s - loss: 1.5300 - multlabel_prec: 0.4777 - multlabel_recall: 0.2719 - multlabel_F1: 0.3105 - multlabel_acc: 0.2310 - val_loss: 0.2366 - val_multlabel_prec: 0.4541 - val_multlabel_recall: 0.2542 - val_multlabel_F1: 0.2906 - val_multlabel_acc: 0.2167\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 621s - loss: 1.5141 - multlabel_prec: 0.4807 - multlabel_recall: 0.2764 - multlabel_F1: 0.3152 - multlabel_acc: 0.2348 - val_loss: 0.2308 - val_multlabel_prec: 0.4655 - val_multlabel_recall: 0.2507 - val_multlabel_F1: 0.2925 - val_multlabel_acc: 0.2185\n",
      "Epoch 8/20\n",
      "36916/36916 [==============================] - 602s - loss: 1.5000 - multlabel_prec: 0.4843 - multlabel_recall: 0.2797 - multlabel_F1: 0.3191 - multlabel_acc: 0.2379 - val_loss: 0.2340 - val_multlabel_prec: 0.4579 - val_multlabel_recall: 0.2607 - val_multlabel_F1: 0.2985 - val_multlabel_acc: 0.2232\n",
      "Epoch 9/20\n",
      "36916/36916 [==============================] - 596s - loss: 1.4868 - multlabel_prec: 0.4881 - multlabel_recall: 0.2833 - multlabel_F1: 0.3228 - multlabel_acc: 0.2408 - val_loss: 0.2397 - val_multlabel_prec: 0.4441 - val_multlabel_recall: 0.2556 - val_multlabel_F1: 0.2919 - val_multlabel_acc: 0.2171\n",
      "Epoch 10/20\n",
      "36916/36916 [==============================] - 599s - loss: 1.4736 - multlabel_prec: 0.4920 - multlabel_recall: 0.2869 - multlabel_F1: 0.3269 - multlabel_acc: 0.2440 - val_loss: 0.2317 - val_multlabel_prec: 0.4838 - val_multlabel_recall: 0.2510 - val_multlabel_F1: 0.2929 - val_multlabel_acc: 0.2189\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 253s   \n",
      "[0.21553797376951528, 0.51267996335781363, 0.26451322145058515, 0.31101176605114556, 0.23304352204310622]\n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 63s    \n",
      "[0.23173021297082055, 0.48376856152860714, 0.25097404483587488, 0.29294995086844611, 0.21891873106416945]\n"
     ]
    }
   ],
   "source": [
    "model_3conv1d_dropout =Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Flatten(),\n",
    "            Dropout(p=0.5),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_3conv1d_dropout')\n",
    "\n",
    "compile_fit_evaluate(model_3conv1d_dropout, flag_quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_22 (Embedding)         (None, 1000, 100)     0           embedding_input_22[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_39 (Convolution1D) (None, 996, 128)      64128       embedding_22[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_39 (MaxPooling1D)   (None, 199, 128)      0           convolution1d_39[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_40 (Convolution1D) (None, 195, 64)       41024       maxpooling1d_39[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_40 (MaxPooling1D)   (None, 65, 64)        0           convolution1d_40[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_41 (Convolution1D) (None, 61, 32)        10272       maxpooling1d_40[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_41 (MaxPooling1D)   (None, 30, 32)        0           convolution1d_41[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)             (None, 960)           0           maxpooling1d_41[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)             (None, 960)           0           flatten_22[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 500)           480500      dropout_26[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)             (None, 500)           0           dense_26[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_27 (Dense)                 (None, 49)            24549       dropout_27[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 620473\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "run \"tensorboard --logdir=./logs/1103_model_3conv1d_2FC\" to launch tensorboard\n",
      "Train on 36916 samples, validate on 9229 samples\n",
      "Epoch 1/20\n",
      "36916/36916 [==============================] - 565s - loss: 1.8340 - multlabel_prec: 0.3290 - multlabel_recall: 0.1431 - multlabel_F1: 0.1698 - multlabel_acc: 0.1213 - val_loss: 0.2602 - val_multlabel_prec: 0.4535 - val_multlabel_recall: 0.2096 - val_multlabel_F1: 0.2362 - val_multlabel_acc: 0.1749\n",
      "Epoch 2/20\n",
      "36916/36916 [==============================] - 566s - loss: 1.6668 - multlabel_prec: 0.4133 - multlabel_recall: 0.2016 - multlabel_F1: 0.2320 - multlabel_acc: 0.1715 - val_loss: 0.2804 - val_multlabel_prec: 0.4128 - val_multlabel_recall: 0.2447 - val_multlabel_F1: 0.2653 - val_multlabel_acc: 0.1977\n",
      "Epoch 3/20\n",
      "36916/36916 [==============================] - 574s - loss: 1.6248 - multlabel_prec: 0.4049 - multlabel_recall: 0.2276 - multlabel_F1: 0.2566 - multlabel_acc: 0.1907 - val_loss: 0.2442 - val_multlabel_prec: 0.3832 - val_multlabel_recall: 0.2214 - val_multlabel_F1: 0.2411 - val_multlabel_acc: 0.1822\n",
      "Epoch 4/20\n",
      "36916/36916 [==============================] - 572s - loss: 1.5995 - multlabel_prec: 0.4264 - multlabel_recall: 0.2431 - multlabel_F1: 0.2746 - multlabel_acc: 0.2041 - val_loss: 0.2480 - val_multlabel_prec: 0.4284 - val_multlabel_recall: 0.2122 - val_multlabel_F1: 0.2494 - val_multlabel_acc: 0.1877\n",
      "Epoch 5/20\n",
      "36916/36916 [==============================] - 572s - loss: 1.5835 - multlabel_prec: 0.4471 - multlabel_recall: 0.2554 - multlabel_F1: 0.2893 - multlabel_acc: 0.2144 - val_loss: 0.2706 - val_multlabel_prec: 0.3965 - val_multlabel_recall: 0.2744 - val_multlabel_F1: 0.2852 - val_multlabel_acc: 0.2082\n",
      "Epoch 6/20\n",
      "36916/36916 [==============================] - 573s - loss: 1.5734 - multlabel_prec: 0.4567 - multlabel_recall: 0.2609 - multlabel_F1: 0.2964 - multlabel_acc: 0.2194 - val_loss: 0.2439 - val_multlabel_prec: 0.4328 - val_multlabel_recall: 0.2317 - val_multlabel_F1: 0.2657 - val_multlabel_acc: 0.1995\n",
      "Epoch 7/20\n",
      "36916/36916 [==============================] - 579s - loss: 1.5655 - multlabel_prec: 0.4621 - multlabel_recall: 0.2627 - multlabel_F1: 0.2991 - multlabel_acc: 0.2213 - val_loss: 0.2640 - val_multlabel_prec: 0.4556 - val_multlabel_recall: 0.3115 - val_multlabel_F1: 0.3329 - val_multlabel_acc: 0.2440\n",
      "Epoch 8/20\n",
      "36916/36916 [==============================] - 572s - loss: 1.5589 - multlabel_prec: 0.4650 - multlabel_recall: 0.2666 - multlabel_F1: 0.3034 - multlabel_acc: 0.2244 - val_loss: 0.2367 - val_multlabel_prec: 0.4804 - val_multlabel_recall: 0.2437 - val_multlabel_F1: 0.2856 - val_multlabel_acc: 0.2142\n",
      "Epoch 9/20\n",
      "36916/36916 [==============================] - 576s - loss: 1.5518 - multlabel_prec: 0.4625 - multlabel_recall: 0.2664 - multlabel_F1: 0.3028 - multlabel_acc: 0.2243 - val_loss: 0.2441 - val_multlabel_prec: 0.4417 - val_multlabel_recall: 0.2752 - val_multlabel_F1: 0.3048 - val_multlabel_acc: 0.2256\n",
      "Epoch 10/20\n",
      "36916/36916 [==============================] - 581s - loss: 1.5475 - multlabel_prec: 0.4699 - multlabel_recall: 0.2696 - multlabel_F1: 0.3070 - multlabel_acc: 0.2269 - val_loss: 0.2385 - val_multlabel_prec: 0.4863 - val_multlabel_recall: 0.2769 - val_multlabel_F1: 0.3157 - val_multlabel_acc: 0.2353\n",
      "Epoch 11/20\n",
      "36916/36916 [==============================] - 577s - loss: 1.5415 - multlabel_prec: 0.4695 - multlabel_recall: 0.2711 - multlabel_F1: 0.3086 - multlabel_acc: 0.2285 - val_loss: 0.2373 - val_multlabel_prec: 0.4474 - val_multlabel_recall: 0.2675 - val_multlabel_F1: 0.2948 - val_multlabel_acc: 0.2204\n",
      "evaluating model...\n",
      "evaluation on training set:\n",
      "36916/36916 [==============================] - 250s   \n",
      "[0.22731832667308505, 0.46696200407213995, 0.27703572891556516, 0.30665332538126505, 0.22986256059064214]\n",
      "evaluation on validation set:\n",
      "9229/9229 [==============================] - 62s    \n",
      "[0.23733951218054061, 0.44741008440972097, 0.2674784229531269, 0.29482632398650555, 0.22037777112996104]\n"
     ]
    }
   ],
   "source": [
    "model_3conv1d_2FC =Sequential(\n",
    "        [ Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM, \n",
    "                  weights=[embedding_matrix],input_length=MAX_SEQ_LEN, trainable=False ),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(64, 5, activation='relu'),\n",
    "            MaxPooling1D(3),\n",
    "            Conv1D(32, 5, activation='relu'),\n",
    "            MaxPooling1D(2),\n",
    "            Flatten(),\n",
    "            Dropout(p=0.5),\n",
    "            Dense(500, activation='relu'),\n",
    "            Dropout(p=0.5),\n",
    "            Dense(N_LABELS, activation='sigmoid') ],\n",
    "        name = 'model_3conv1d_2FC')\n",
    "\n",
    "compile_fit_evaluate(model_3conv1d_2FC, flag_quick_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:thesis_nb]",
   "language": "python",
   "name": "conda-env-thesis_nb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
