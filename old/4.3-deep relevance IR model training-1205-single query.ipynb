{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "import os, nltk\n",
    "from keras.callbacks import EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_tree = etree.parse('data/topics2016.xml')\n",
    "\n",
    "def get_topic(i):# returns the summary string of the ith topic\n",
    "    summary = topic_tree.xpath('//topic[@number=\"%d\"]/summary/text()'%i)\n",
    "    return str(summary).lower()\n",
    "\n",
    "# build a mapping of article name (PMCID) to its file path\n",
    "\n",
    "PMC_PATH = '/local/XW/DATA/TREC/PMCs/'\n",
    "pmcid2fpath = {}\n",
    "\n",
    "for subdir1 in os.listdir(PMC_PATH):\n",
    "    for subdir2 in os.listdir(os.path.join(PMC_PATH, subdir1)):\n",
    "        diry = os.path.join(PMC_PATH, subdir1, subdir2)\n",
    "#         print diry, len(os.listdir(diry))\n",
    "        for fn in os.listdir(diry):\n",
    "            pmcid = fn[:-5]\n",
    "            fpath = os.path.join(diry, fn)\n",
    "            pmcid2fpath[pmcid] = fpath\n",
    "\n",
    "def get_article_abstract(pmcid):\n",
    "    fpath = pmcid2fpath[pmcid]\n",
    "    tree = etree.parse(fpath)\n",
    "    abstract = tree.xpath('//abstract')[0]\n",
    "    ret = u''+abstract.xpath('string(.)')\n",
    "    return ret.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = get_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4176/37707 [00:04<00:39, 840.53it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "pmcid_2relevance = [{} for i in xrange(31)] # list of dict mapping pmcid to relevance\n",
    "with open('data/qrels.txt') as f:\n",
    "    for line in tqdm(f, total=37707): \n",
    "        topicid, _, pmcid, relevance = line.split()\n",
    "        topicid = int(topicid)\n",
    "        if topicid>3: break\n",
    "        try:\n",
    "            corpus.append(get_article_abstract(pmcid))\n",
    "            pmcid_2relevance[topicid][pmcid] = int(relevance)\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3871"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(corpus)\n",
    "\n",
    "def get_idf(wd):\n",
    "    if wd not in vectorizer.vocabulary_: return 1.0\n",
    "    return vectorizer.idf_[ vectorizer.vocabulary_[wd] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cPickle as pk\n",
    "np.random.seed(1) # to be reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2V_FPATH = '/local/XW/DATA/WORD_EMBEDDINGS/biomed-w2v-200.txt'\n",
    "GLOVE_FPATH = '/local/XW/DATA/WORD_EMBEDDINGS/glove.6B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5443657/5443657 [01:02<00:00, 87451.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 22981 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec = {} # maps word ---> embedding vector\n",
    "with open(W2V_FPATH) as f:\n",
    "    for line in tqdm(f, total=5443657): #5443657 400000\n",
    "        vals = line.split()\n",
    "        word = vals[0]\n",
    "        if word in vocab:\n",
    "            vec = np.asarray(vals[1:], dtype='float')\n",
    "            word2vec[word] = vec\n",
    "print 'found %d word vectors.' % len(word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Define the deep relevance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function for visualization of model\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "def viz_model(model):\n",
    "    return SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct the relevance IR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, InputLayer, Flatten, Input, Merge, merge, Reshape\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "N = len(query.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2 main components of the structure: feed forward network and gating\n",
    "feed_forward = Sequential(\n",
    "    [Dense(input_dim=30, output_dim=5, activation='tanh'),\n",
    "     Dense(output_dim=1, activation='tanh')], \n",
    "    name='feed_forward_nw')\n",
    "\n",
    "# ***note: have to wrap ops into Lambda layers !!***\n",
    "# cf: https://groups.google.com/forum/#!topic/keras-users/fbRS-FkZw_Q\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "input_idf = Input(shape=(N,), name='input_idf')\n",
    "def scale(x): \n",
    "    w = K.variable(1, name='w_g')\n",
    "    return K.mul(x,w)\n",
    "def scale_output_shape(input_shape): return input_shape\n",
    "\n",
    "scaled = Lambda(scale, scale_output_shape, name='softmax_scale')(input_idf)\n",
    "gs_out = Activation('softmax', name='softmax')(scaled)\n",
    "gating = Model(input=input_idf, output=gs_out, name='gating')\n",
    "\n",
    "# first input: hist vectors\n",
    "input_hists = Input(shape=(N,30), name='input_hists')\n",
    "\n",
    "def slicei(x, i): return x[:,i,:]\n",
    "def slicei_output_shape(input_shape): return (input_shape[0], input_shape[2])\n",
    "zs = [ feed_forward( Lambda(lambda x:slicei(x,i), slicei_output_shape, name='slice%d'%i)(input_hists) )\\\n",
    "          for i in xrange(N) ]\n",
    "\n",
    "def concat(x): return K.concatenate(x) \n",
    "def concat_output_shape(input_shape): return (input_shape[0][0], N)\n",
    "zs = Lambda(concat, concat_output_shape, name='concat_zs')(zs)\n",
    "\n",
    "# second input: idf scores of each query term \n",
    "input_idf = Input(shape=(N,), name='input_idf')\n",
    "gs = gating(input_idf)\n",
    "\n",
    "def innerprod(x): return K.sum( K.mul(x[0],x[1]), axis=1)\n",
    "def innerprod_output_shape(input_shape): return (input_shape[0][0],1)\n",
    "scores = Lambda(innerprod, innerprod_output_shape, name='innerprod_zs_gs')([zs, gs])\n",
    "\n",
    "scoring_model = Model(input=[input_idf, input_hists], output=[scores], name='scoring_model')\n",
    "\n",
    "# third input -- the negative hists vector \n",
    "input_hists_neg = Input(shape=(N,30), name='input_hists_neg')\n",
    "\n",
    "zs_neg = [ feed_forward( Lambda(lambda x:slicei(x,i), slicei_output_shape, name='slice%d_neg'%i)(input_hists_neg) )\\\n",
    "          for i in xrange(N) ]\n",
    "\n",
    "zs_neg = Lambda(concat, concat_output_shape, name='concat_zs_neg')(zs_neg)\n",
    "\n",
    "scores_neg = Lambda(innerprod, innerprod_output_shape, name='innerprod_zs_gs_neg')([zs_neg, gs])\n",
    "\n",
    "two_score_model = Model(input=[input_idf, input_hists, input_hists_neg], \n",
    "                        output=[scores, scores_neg], \n",
    "                        name='two_score_model')\n",
    "\n",
    "def diff(x): return tf.sub(x[0], x[1]) #x[0]-x[1]\n",
    "def diff_output_shape(input_shape): return input_shape[0]\n",
    "posneg_score_diff = Lambda(diff, diff_output_shape, name='posneg_score_diff')([scores, scores_neg])\n",
    "ranking_model = Model(input=[input_idf, input_hists,  input_hists_neg]\n",
    "                      , output=[posneg_score_diff]\n",
    "                      , name='ranking_model')\n",
    "\n",
    "# define my loss function: hinge of score_pos - score_neg\n",
    "def pairwise_hinge(y_true, y_pred): # y_pred = score_pos - score_neg, **y_true doesn't matter here**\n",
    "    return K.mean( K.maximum(1. - y_pred, y_true*0.0) )  \n",
    "\n",
    "ranking_model.compile(optimizer='adagrad', loss=pairwise_hinge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. train model (for topic-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "randvec = np.random.randn(200)\n",
    "def get_histvec(q_wd, doc):\n",
    "    qvec = word2vec.get(q_wd, randvec)\n",
    "#     dvecs = np.vstack( [word2vec.get(wd, randvec) for wd in nltk.word_tokenize(doc)] )\n",
    "    words_doc = filter(lambda wd:wd in word2vec, nltk.word_tokenize(doc))\n",
    "    dvecs = np.vstack( [ word2vec[wd] for wd in words_doc ] )\n",
    "    cossims = np.dot(dvecs, qvec) / norm(qvec) / norm(dvecs, axis=1)\n",
    "    hist, _ = np.histogram( cossims[cossims<1.0], bins=29, range=(-1,1) )\n",
    "    ones = len(cossims) - sum(hist)\n",
    "    ret = np.array( list(hist) + [ones] )\n",
    "    return ret # np.reshape(ret, (-1, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_query_doc_feature(query, pmcid):\n",
    "    doc = get_article_abstract(pmcid)\n",
    "    return np.array([ get_histvec(wd, doc) for wd in query.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare positive/negative pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevance = pmcid_2relevance[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 1209\n"
     ]
    }
   ],
   "source": [
    "pos_ids, neg_ids = [], []\n",
    "for pmcid in relevance.keys():\n",
    "    if relevance[pmcid]==0: neg_ids.append(pmcid)\n",
    "    else: pos_ids.append(pmcid)\n",
    "print len(pos_ids), len(neg_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:04<00:00, 25.59it/s]\n",
      "100%|██████████| 1209/1209 [00:46<00:00, 26.02it/s]\n"
     ]
    }
   ],
   "source": [
    "hists_pos = np.array( [get_query_doc_feature(query, posid) for posid in tqdm(pos_ids)] )\n",
    "hists_neg = np.array( [get_query_doc_feature(query, negid) for negid in tqdm(neg_ids)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model using `fit_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VALDATION_SPLIT = 0.2\n",
    "BATCH_SZ = 128\n",
    "NB_EPOCH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_pairs = []\n",
    "for pidx in xrange(len(hists_pos)):\n",
    "    for nidx in xrange(len(hists_neg)):\n",
    "        idx_pairs.append( (pidx, nidx) )\n",
    "idx_pairs = np.array(idx_pairs)\n",
    "val_sz = int(len(idx_pairs)*VALDATION_SPLIT)\n",
    "idx_pairs_train, idx_pairs_val = idx_pairs[val_sz:], idx_pairs[:val_sz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(idx_pairs, batch_size=BATCH_SZ): \n",
    "    np.random.shuffle(idx_pairs)\n",
    "    batches_pre_epoch = len(idx_pairs) //batch_size\n",
    "    samples_per_epoch = batches_pre_epoch * batch_size # make samples_per_epoch a multiple of batch size\n",
    "    counter = 0\n",
    "    _idf = np.array([get_idf(wd) for wd in query.split()])\n",
    "    idfs_batch = np.vstack([_idf]*batch_size)\n",
    "    y_true_batch_dummy = np.ones((batch_size))\n",
    "    while 1:\n",
    "        idx_batch = idx_pairs[batch_size*counter: min(samples_per_epoch, batch_size*(counter+1))]\n",
    "        pos_batch = hists_pos[idx_batch[:,0]]\n",
    "        neg_batch = hists_neg[idx_batch[:,1]]\n",
    "        counter += 1\n",
    "        if (counter >= batches_pre_epoch):\n",
    "            np.random.shuffle(idx_pairs)\n",
    "            counter=0\n",
    "        yield [idfs_batch, pos_batch, neg_batch], y_true_batch_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# self-defined metrics\n",
    "def ranking_acc(y_true, y_pred):\n",
    "    y_pred = y_pred > 0 \n",
    "    return K.mean(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranking_model.compile(optimizer='adagrad', loss=pairwise_hinge, metrics=[ranking_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.6695 - ranking_acc: 0.7771 - val_loss: 0.4714 - val_ranking_acc: 0.8727\n",
      "Epoch 2/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.4651 - ranking_acc: 0.8634 - val_loss: 0.4274 - val_ranking_acc: 0.8743\n",
      "Epoch 3/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.4348 - ranking_acc: 0.8717 - val_loss: 0.4125 - val_ranking_acc: 0.8775\n",
      "Epoch 4/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.4186 - ranking_acc: 0.8784 - val_loss: 0.4079 - val_ranking_acc: 0.8771\n",
      "Epoch 5/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.4047 - ranking_acc: 0.8844 - val_loss: 0.3988 - val_ranking_acc: 0.8815\n",
      "Epoch 6/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3957 - ranking_acc: 0.8885 - val_loss: 0.3909 - val_ranking_acc: 0.8834\n",
      "Epoch 7/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3885 - ranking_acc: 0.8915 - val_loss: 0.3877 - val_ranking_acc: 0.8839\n",
      "Epoch 8/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3828 - ranking_acc: 0.8938 - val_loss: 0.3801 - val_ranking_acc: 0.8884\n",
      "Epoch 9/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3775 - ranking_acc: 0.8956 - val_loss: 0.3816 - val_ranking_acc: 0.8873\n",
      "Epoch 10/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3732 - ranking_acc: 0.8972 - val_loss: 0.3799 - val_ranking_acc: 0.8867\n",
      "Epoch 11/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3697 - ranking_acc: 0.8986 - val_loss: 0.3786 - val_ranking_acc: 0.8876\n",
      "Epoch 12/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3666 - ranking_acc: 0.8995 - val_loss: 0.3789 - val_ranking_acc: 0.8879\n",
      "Epoch 13/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3638 - ranking_acc: 0.9008 - val_loss: 0.3781 - val_ranking_acc: 0.8872\n",
      "Epoch 14/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3613 - ranking_acc: 0.9017 - val_loss: 0.3774 - val_ranking_acc: 0.8866\n",
      "Epoch 15/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3591 - ranking_acc: 0.9020 - val_loss: 0.3774 - val_ranking_acc: 0.8863\n",
      "Epoch 16/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3570 - ranking_acc: 0.9027 - val_loss: 0.3739 - val_ranking_acc: 0.8887\n",
      "Epoch 17/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3550 - ranking_acc: 0.9036 - val_loss: 0.3741 - val_ranking_acc: 0.8883\n",
      "Epoch 18/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3533 - ranking_acc: 0.9039 - val_loss: 0.3754 - val_ranking_acc: 0.8866\n",
      "Epoch 19/20\n",
      "120832/120832 [==============================] - 6s - loss: 0.3517 - ranking_acc: 0.9043 - val_loss: 0.3764 - val_ranking_acc: 0.8853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f90f08c6d10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = './log/relevance_matching'\n",
    "_callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "              TensorBoard(log_dir=logdir, histogram_freq=0, write_graph=False) # \n",
    "             ]\n",
    "# 30-5-1 tanh\n",
    "ranking_model.fit_generator( batch_generator(idx_pairs_train), \n",
    "                    samples_per_epoch = len(idx_pairs_train)//BATCH_SZ*BATCH_SZ,\n",
    "                    nb_epoch=NB_EPOCH,\n",
    "                    validation_data=batch_generator(idx_pairs_val),\n",
    "                    nb_val_samples=len(idx_pairs_val)//BATCH_SZ*BATCH_SZ, \n",
    "                    callbacks = _callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## below are some testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3429740', '3921765'),\n",
       " ('2999735', '4085271'),\n",
       " ('3526517', '4532844'),\n",
       " ('1750992', '2769307'),\n",
       " ('3503351', '3809984'),\n",
       " ('4130891', '4685986'),\n",
       " ('4471306', '2996340'),\n",
       " ('3286730', '4395018'),\n",
       " ('4659952', '28994'),\n",
       " ('4724023', '4716452')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(pos_ids[:10], neg_ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11) (2, 11, 30)\n"
     ]
    }
   ],
   "source": [
    "pos_sample = get_query_doc_feature(query, '3429740')\n",
    "neg_sample = get_query_doc_feature(query, '3921765')\n",
    "pair_sample = np.array([pos_sample, neg_sample])\n",
    "\n",
    "_idf = np.array([get_idf(wd) for wd in query.split()])\n",
    "idf_sample = np.vstack([_idf]*2)\n",
    "\n",
    "print idf_sample.shape, pair_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test `scoring_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3721953 ,  0.44491634], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_model.predict([idf_sample,pair_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.43848625]\n",
      " [ 0.47157994]\n",
      " [-0.98957956]\n",
      " [-0.99888313]\n",
      " [ 0.41266179]\n",
      " [-0.82911295]\n",
      " [-0.9381572 ]\n",
      " [ 0.99194616]\n",
      " [ 0.98716438]\n",
      " [ 0.98718643]\n",
      " [-0.43848625]]\n"
     ]
    }
   ],
   "source": [
    "a = feed_forward.predict(pos_sample)\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00482637  0.20997439  0.0174815   0.02042374  0.0538551   0.20764136\n",
      "  0.00537621  0.12458481  0.34606892  0.00494123  0.00482637]\n"
     ]
    }
   ],
   "source": [
    "b = gating.predict(idf_sample)[0]\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.37219518], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.dot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.87186098]\n",
      " [ 0.7142241 ]\n",
      " [-0.96034747]\n",
      " [ 0.94236648]\n",
      " [-0.3230921 ]\n",
      " [ 0.99326599]\n",
      " [-0.93802834]\n",
      " [ 0.99049699]\n",
      " [-0.08088312]\n",
      " [ 0.98430783]\n",
      " [ 0.87186098]]\n",
      "[ 0.44491631]\n"
     ]
    }
   ],
   "source": [
    "c = feed_forward.predict(neg_sample)\n",
    "print c\n",
    "print b.dot(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> the scoring model works all right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test ranking_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07272103,  0.07272103], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_model.predict( [idf_sample, pair_sample, np.array([neg_sample, pos_sample]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.47823831999999994"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.57720977 - -0.09897145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_score(pmcid):\n",
    "    _idf = np.array([get_idf(wd) for wd in query.split()])\n",
    "    _idf = np.vstack([_idf])\n",
    "    _hist = get_query_doc_feature(query, pmcid).reshape(1,11,30)\n",
    "    return scoring_model.predict([_idf, _hist])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.37219524, 0.44491634),\n",
       " (-0.071498334, -0.15975329),\n",
       " (-0.032746524, -0.80821633),\n",
       " (0.81949872, 0.30661809),\n",
       " (0.1990094, -0.18697436),\n",
       " (0.92787206, -0.3595199),\n",
       " (0.50025171, -0.54916126),\n",
       " (0.48231193, 0.13923873),\n",
       " (0.7508564, -0.60553944),\n",
       " (0.79059291, -0.93590719)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip( map(predict_score, pos_ids[:10]), map(predict_score, neg_ids[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_score_diff( (pmcid_pos, pmcid_neg) ):\n",
    "    _idf = np.array([get_idf(wd) for wd in query.split()])\n",
    "    _idf = np.vstack([_idf])\n",
    "    hist_pos = get_query_doc_feature(query, pmcid_pos).reshape((1,11,30))\n",
    "    hist_neg = get_query_doc_feature(query, pmcid_neg).reshape((1,11,30))\n",
    "    return ranking_model.predict([_idf, hist_pos, hist_neg])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the scoring model (metrics=AP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AP(pos_scores, neg_scores):\n",
    "    Q = len(pos_scores)\n",
    "    pos_tags = [1] * len(pos_scores)\n",
    "    neg_tags = [0] * len(neg_scores)\n",
    "    all_tagged = zip(pos_scores, pos_tags) + zip(neg_scores, neg_tags)\n",
    "    ranked_list = sorted(all_tagged, reverse=True)\n",
    "    print ranked_list[:20]\n",
    "    ranked_tag = zip(*ranked_list)[1]\n",
    "    print ranked_tag[:20]\n",
    "    precision_at_i = []\n",
    "    corr, total = 0.0, 0\n",
    "    while corr<Q:\n",
    "        if ranked_tag[total]==1: \n",
    "            corr += 1\n",
    "            precision_at_i.append(corr*1.0 / (total+1) )\n",
    "        total += 1\n",
    "    print precision_at_i[:20]\n",
    "    return np.mean(precision_at_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_idfs = np.vstack([_idf]*len(hists_pos))\n",
    "pos_scores = scoring_model.predict( [ _idfs, hists_pos])\n",
    "_idfs = np.vstack([_idf]*len(hists_neg))\n",
    "neg_scores = scoring_model.predict( [ _idfs, hists_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.459192 -0.286091\n",
      "0.984027 0.916831\n",
      "-0.624351 -0.987414\n"
     ]
    }
   ],
   "source": [
    "print pos_scores.mean(), neg_scores.mean()\n",
    "print pos_scores.max(), neg_scores.max()\n",
    "print pos_scores.min(), neg_scores.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.98402655, 1), (0.9368118, 1), (0.92787206, 1), (0.92328721, 1), (0.91716737, 1), (0.91683054, 0), (0.91507411, 0), (0.90818655, 0), (0.89776915, 1), (0.89175165, 0), (0.86966276, 0), (0.86896461, 1), (0.86194515, 1), (0.86036122, 0), (0.85995638, 1), (0.85855299, 0), (0.85561204, 0), (0.85223681, 1), (0.84764636, 0), (0.8459121, 1)]\n",
      "(1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1)\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5833333333333334, 0.6153846153846154, 0.6, 0.5555555555555556, 0.55, 0.5714285714285714, 0.5909090909090909, 0.6086956521739131, 0.5769230769230769, 0.5714285714285714, 0.5666666666666667, 0.5806451612903226, 0.5277777777777778, 0.5405405405405406]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46100751843546001"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AP(pos_scores, neg_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
