{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cPickle as pk\n",
    "np.random.seed(1) # to be reproducible\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "NOTES_DIR = '/local/XW/DATA/MIMIC/noteevents_by_sid_hid/'\n",
    "ICD_FPATH = 'data/sid_hid_diagicds.txt'\n",
    "PK_FPATH = 'data/processed_data_sidhid.pk'\n",
    "# constants\n",
    "N_LABELS = 50 # predict only on top K frequent icd codes\n",
    "N_SIDHID = 58328  # number of unique (sid,hid) pairs\n",
    "MAX_SEQ_LEN = 1000 # max length of input sequence (pad/truncate to fix length)\n",
    "MAX_NB_WORDS = 20000 # top 20k most freq words\n",
    "EMBEDDING_DIM = 200\n",
    "VALIDATION_SPLIT = 0.2\n",
    "# word2vec configurations\n",
    "W2V_FPATH = '/local/XW/DATA/WORD_EMBEDDINGS/biomed-w2v-200.txt'\n",
    "GLOVE_FPATH = '/local/XW/DATA/WORD_EMBEDDINGS/glove.6B.200d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. prepare Y: khot encoding (`sidhid2khot`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the icd code into a dict\n",
    "from collections import Counter\n",
    "sidhid2icds = {} # map subject_id, hadm_id (int tuple) ---> icd codes of this patient (set)\n",
    "icd_ctr = Counter()\n",
    "with open(ICD_FPATH) as f: \n",
    "    for line in f: \n",
    "        sid, hid, _icds = line.split(',')\n",
    "        sid, hid = map( int, (sid,hid))\n",
    "        _icds = _icds.split()\n",
    "        icd_ctr.update(_icds)\n",
    "        sidhid2icds[(sid,hid)] = set(_icds)\n",
    "\n",
    "def to_khot(sidhid2icds, K=N_LABELS):\n",
    "    icds = zip( *icd_ctr.most_common(N_LABELS-1) )[0] + ('other',)\n",
    "    # print icds # these are icds to predict\n",
    "    # print icd_ctr.most_common(K_ICDS_TOKEEP)\n",
    "    # now turn each subject into a k-hot vector\n",
    "    sidhid2khot = {} # map subject_id to k-hot vector\n",
    "    for sid,hid in sidhid2icds.keys():\n",
    "        _khot = np.zeros(N_LABELS)\n",
    "        for _icd in sidhid2icds[(sid,hid)]:\n",
    "            if _icd in icds: \n",
    "                _khot[icds.index(_icd)] = 1\n",
    "            else: # label 'other icds'\n",
    "                _khot[-1] = 1\n",
    "        if sum(_khot) == 0: print 'strange case: ', (sid,hid)\n",
    "        sidhid2khot[(sid,hid)] = _khot\n",
    "    return sidhid2khot\n",
    "\n",
    "sidhid2khot = to_khot(sidhid2icds, K=N_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print np.array( [sidhid2khot[k] for k in sidhid2khot.keys()[:2]] )\n",
    "# print sidhid2khot.keys()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. prepare X: turn notes into fix-length of word ids (`sidhid2seq`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58328/58328 [01:34<00:00, 615.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 58328 texts\n",
      "found 356391 unique tokens, use most frequent 20000 of them\n"
     ]
    }
   ],
   "source": [
    "sidhids = []\n",
    "texts = [] # text bodies\n",
    "for fname in tqdm(os.listdir(NOTES_DIR)): # the data is 3.7G in size, can hold in memory...\n",
    "    sid,hid = map( int, fname[:-4].split('_') )\n",
    "    sidhids.append( (sid,hid) )\n",
    "    fpath = os.path.join(NOTES_DIR, fname)\n",
    "    df = pd.read_csv(fpath)\n",
    "    texts.append( '\\n=======\\n\\n\\n'.join(df['text']) )\n",
    "print('found %d texts' % len(texts))\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, # filter out numbers, otherwise lots of numbers\n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+'0123456789') \n",
    "print 'fitting on whole text corpus...',\n",
    "tokenizer.fit_on_texts(texts) # this might take some time\n",
    "print 'done. '\n",
    "\n",
    "seqs = tokenizer.texts_to_sequences(texts) # turn article into seq of ids\n",
    "word2idx = tokenizer.word_index # dictionary mapping words (str) ---> their index (int)\n",
    "\n",
    "print 'found %s unique tokens, use most frequent %d of them'%(len(word2idx), MAX_NB_WORDS)\n",
    "\n",
    "# print sorted(word2idx.items(), key=lambda (k,v): v)[:100] # TODO: remove stopwords\n",
    "\n",
    "seqs_padded = pad_sequences(seqs, maxlen=MAX_SEQ_LEN) # turn into fix-length sequences\n",
    "sidhid2seq = {}\n",
    "for (sid,hid), seq in zip(sidhids,seqs_padded):\n",
    "    sidhid2seq[(sid,hid)] = seq\n",
    "\n",
    "del texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# W2V_BIN_FPATH = '/local/XW/DATA/WORD_EMBEDDINGS/W2V_BIO/wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "# from gensim.models import word2vec\n",
    "# w2v = word2vec.Word2Vec.load_word2vec_format(W2V_BIN_FPATH, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5443657/5443657 [01:14<00:00, 72989.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 86588 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# build index mapping: map word to its vector\n",
    "word2vec = {} # maps word ---> embedding vector\n",
    "with open(W2V_FPATH) as f:\n",
    "    for line in tqdm(f, total=5443657):\n",
    "        vals = line.split()\n",
    "        word = vals[0]\n",
    "        if word in word2idx or word=='</s>':\n",
    "            word2vec[word] = np.asarray(vals[1:], dtype='float')\n",
    "print 'found %d word vectors.' % len(word2vec)\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word2idx))\n",
    "embedding_w2v = np.zeros( (nb_words+1, EMBEDDING_DIM) ) # +1 because ids in sequences starts from 1 ?\n",
    "for word,wd_id in word2idx.items(): \n",
    "    if wd_id > MAX_NB_WORDS or word not in word2vec: # there might be 0 rows in embedding matrix\n",
    "        continue # word_id>MAX_NB_WORDS, this id is not in the generated sequences, discard\n",
    "    embedding_w2v[wd_id,:] = word2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:05<00:00, 73839.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 50708 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove = {} # maps word ---> embedding vector\n",
    "with open(GLOVE_FPATH) as f:\n",
    "    for line in tqdm(f, total=400000):\n",
    "        vals = line.split()\n",
    "        word = vals[0]\n",
    "        if word in word2idx or word=='</s>':\n",
    "            glove[word] = np.asarray(vals[1:], dtype='float')\n",
    "print 'found %d word vectors.' % len(glove)\n",
    "\n",
    "embedding_glove = np.zeros( (nb_words+1, EMBEDDING_DIM) ) # +1 because ids in sequences starts from 1 ?\n",
    "for word,wd_id in word2idx.items(): \n",
    "    if wd_id > MAX_NB_WORDS or word not in glove: # there might be 0 rows in embedding matrix\n",
    "        continue # word_id>MAX_NB_WORDS, this id is not in the generated sequences, discard\n",
    "    embedding_glove[wd_id,:] = glove[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all (sid,hid) pairs in the list `sidhids`, for each (sid,hid) pair, can get the sequence vector by dict `sidhid2seq`, the khot encoding by dict `sidhid2khot`, and if we want all icds (instead of the khot representation), just use the dict `sidhid2icds`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46663, 1000) (46663, 50)\n",
      "(11665, 1000) (11665, 50)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "\n",
    "indices = np.arange(len(sidhids))\n",
    "np.random.shuffle(indices)\n",
    "validset_sz = int(VALIDATION_SPLIT*len(sidhids))\n",
    "train_sidhids, val_sidhids = sidhids[:-validset_sz], sidhids[-validset_sz:]\n",
    "\n",
    "def getXY(sidhid_lst, sidhid2seq=sidhid2seq, sidhid2khot=sidhid2khot): # give a list of (sid, hid) pairs, generate the X and Y\n",
    "    data, labels = [], []\n",
    "    for sidhid in sidhid_lst:\n",
    "        data.append(sidhid2seq[sidhid])\n",
    "        labels.append(sidhid2khot[sidhid])\n",
    "    X = np.array(data)\n",
    "    Y = np.array(labels)\n",
    "    return X,Y\n",
    "\n",
    "X_train, Y_train = getXY(train_sidhids)\n",
    "print X_train.shape, Y_train.shape\n",
    "X_val, Y_val = getXY(val_sidhids)\n",
    "print X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dump to pk file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed data is written into data/processed_data_sidhid.pk\n"
     ]
    }
   ],
   "source": [
    "description = '''This file contains the prepared data for note2vec training, \n",
    "* sidhids:     list of the 58361 unique (sid,hid) pairs\n",
    "* sidhid2icds: mapping from (sid,hid) pair --> set of icd codes\n",
    "* sidhid2khot: mapping from (sid,hid) pair --> khot-encoding correponding to this sidhid pair\n",
    "* sidhid2seq:  mapping from (sid,hid) pair --> fix-length sequences (len=1000) of word ids\n",
    "* word2idx:    mapping from a word to its id used in the sequence\n",
    "* embedding_w2v／embedding_glove: matrices for the embedding layer (used as the weights parameter)\n",
    "* train_sidhids/val_sidhids: list of (sid,hid) pairs used as training/validation set\n",
    "* X_train/Y_train/X_val/Y_val: ndarray generated for training/validation\n",
    "\n",
    "And here are 2 useful functions' source code: \n",
    "\n",
    "def to_khot(sidhid2icds, K=N_LABELS): # generate khot encoding (useful if want to change the K)\n",
    "    icds = zip( *icd_ctr.most_common(N_LABELS-1) )[0] + ('other',)\n",
    "    sidhid2khot = {} # map subject_id to k-hot vector\n",
    "    for sid,hid in sidhid2icds.keys():\n",
    "        _khot = np.zeros(N_LABELS)\n",
    "        for _icd in sidhid2icds[(sid,hid)]:\n",
    "            if _icd in icds: \n",
    "                _khot[icds.index(_icd)] = 1\n",
    "            else: # label 'other icds'\n",
    "                _khot[-1] = 1\n",
    "        sidhid2khot[(sid,hid)] = _khot\n",
    "    return sidhid2khot\n",
    "\n",
    "def getXY(sidhid_lst): # give a list of (sid, hid) pairs, generate the X and Y\n",
    "    data, labels = [], []\n",
    "    for sidhid in sidhid_lst:\n",
    "        data.append(sidhid2seq[sidhid])\n",
    "        labels.append(sidhid2khot[sidhid])\n",
    "    X = np.array(data)\n",
    "    Y = np.array(labels)\n",
    "    return X,Y\n",
    "'''\n",
    "\n",
    "data_to_pickle = {\n",
    "    'description'     : description,\n",
    "    'sidhids'         : sidhids,\n",
    "    'sidhid2icds'     : sidhid2icds,\n",
    "    'sidhid2khot'     : sidhid2khot,\n",
    "    'sidhid2seq'      : sidhid2seq,\n",
    "    'word2idx'        : word2idx,\n",
    "    'embedding_w2v'   : embedding_w2v,\n",
    "    'embedding_glove' : embedding_glove,\n",
    "    'train_sidhids'   : train_sidhids,\n",
    "    'val_sidhids'     : val_sidhids,\n",
    "    'X_train'         : X_train,\n",
    "    'Y_train'         : Y_train,\n",
    "    'X_val'           : X_val,\n",
    "    'Y_val'           : Y_val,\n",
    "}\n",
    "with open(PK_FPATH, 'wb') as fout:\n",
    "    pk.dump(data_to_pickle, fout, pk.HIGHEST_PROTOCOL)\n",
    "print 'processed data is written into %s' % PK_FPATH"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:thesis_nb]",
   "language": "python",
   "name": "conda-env-thesis_nb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
